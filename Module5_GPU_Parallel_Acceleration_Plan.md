# æ¨¡å—5 RQAæµç¨‹ GPUå¹¶è¡ŒåŠ é€Ÿå¼€å‘æ–¹æ¡ˆ

## ğŸ“‹ æ–‡æ¡£ä¿¡æ¯
- **é¡¹ç›®åç§°**: VRçœ¼åŠ¨æ•°æ®åˆ†æç³»ç»Ÿ - æ¨¡å—5 RQAæ‰¹å¤„ç†GPUåŠ é€Ÿ
- **åˆ›å»ºæ—¶é—´**: 2025-10-01
- **ç›®æ ‡**: å°†10,200ç»„åˆçš„æ‰¹å¤„ç†æ—¶é—´ä»142å°æ—¶é™è‡³5-7å°æ—¶ (20-30xæé€Ÿ)
- **ç¡¬ä»¶ç¯å¢ƒ**: NVIDIA GeForce RTX 3080 Mobile (16GB VRAM, CUDA 12.6)

---

## ğŸ¯ é¡¹ç›®ç›®æ ‡

### å½“å‰é—®é¢˜
- **ä»»åŠ¡è§„æ¨¡**: 10,200ä¸ªå‚æ•°ç»„åˆ (mÃ—Ï„Ã—ÎµÃ—l_min)
- **å½“å‰æ€§èƒ½**: 50ç§’/ç»„åˆ
- **é¢„è®¡è€—æ—¶**: 142å°æ—¶ (çº¦6å¤©)
- **ç“¶é¢ˆåˆ†æ**:
  1. CPUå•çº¿ç¨‹æ‰§è¡ŒRQAè®¡ç®— (NumPy)
  2. é¡ºåºå¤„ç†10,200ä¸ªç»„åˆ
  3. åŒæ­¥é˜»å¡APIè®¾è®¡

### ä¼˜åŒ–ç›®æ ‡
| æŒ‡æ ‡ | å½“å‰ | ç›®æ ‡ | æå‡ |
|------|------|------|------|
| å•ç»„åˆè€—æ—¶ | 50ç§’ | 2.5ç§’ | 20x |
| å¹¶è¡Œä»»åŠ¡æ•° | 1 | 4-6 | 4-6x |
| æ€»å¤„ç†æ—¶é—´ | 142å°æ—¶ | 5-7å°æ—¶ | **20-30x** |
| GPUåˆ©ç”¨ç‡ | 0% | 80-90% | - |
| å®æ—¶è¿›åº¦ | æ—  | WebSocketæµå¼ | âœ… |

---

## ğŸ—ï¸ æŠ€æœ¯æ¶æ„

### ç³»ç»Ÿæ¶æ„å›¾
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                      å‰ç«¯ (Browser)                          â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚ é…ç½®é¢æ¿    â”‚  â”‚ è¿›åº¦ç›‘æ§      â”‚  â”‚ GPUçŠ¶æ€ç›‘æ§      â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚         â”‚                 â”‚                    â”‚              â”‚
â”‚         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                            â”‚
                      WebSocket (å®æ—¶è¿›åº¦)
                            â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    Flask Backend                              â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚  /api/rqa-pipeline/batch-execute-gpu (REST API)        â”‚ â”‚
â”‚  â”‚  â€¢ å‚æ•°éªŒè¯                                             â”‚ â”‚
â”‚  â”‚  â€¢ ä»»åŠ¡åˆ†é…                                             â”‚ â”‚
â”‚  â”‚  â€¢ è¿›åº¦å¹¿æ’­ (WebSocket)                                 â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â”‚                       â”‚                                        â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚     å¤šè¿›ç¨‹è°ƒåº¦å¼•æ“ (ProcessPoolExecutor)                â”‚ â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚ â”‚
â”‚  â”‚  â”‚ Worker 1 â”‚ â”‚ Worker 2 â”‚ â”‚ Worker 3 â”‚ â”‚ Worker 4 â”‚  â”‚ â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜  â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
           â”‚            â”‚            â”‚            â”‚
           â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                         GPUå…±äº«
                            â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              NVIDIA RTX 3080 Mobile (16GB)                    â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚  CUDA 12.6 Runtime                                      â”‚ â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”             â”‚ â”‚
â”‚  â”‚  â”‚ CuPy RQA â”‚  â”‚ PyTorch  â”‚  â”‚ GPU-Scipyâ”‚             â”‚ â”‚
â”‚  â”‚  â”‚ Kernel   â”‚  â”‚ Tensors  â”‚  â”‚ Stats    â”‚             â”‚ â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜             â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### å…³é”®æŠ€æœ¯æ ˆ
| å±‚çº§ | æŠ€æœ¯ | ç‰ˆæœ¬è¦æ±‚ | ç”¨é€” |
|------|------|---------|------|
| GPUåŠ é€Ÿ | **CuPy** | â‰¥13.0.0 | NumPy GPUæ›¿ä»£ (RQAè®¡ç®—) |
| æ·±åº¦å­¦ä¹  | **PyTorch** | â‰¥2.0.0+cu118 | GPUå¼ é‡è¿ç®— |
| å¹¶è¡Œè®¡ç®— | **concurrent.futures** | å†…ç½® | å¤šè¿›ç¨‹ä»»åŠ¡æ±  |
| å®æ—¶é€šä¿¡ | **Flask-SocketIO** | â‰¥5.3.0 | WebSocketè¿›åº¦æ¨é€ |
| ä»»åŠ¡é˜Ÿåˆ— | **Redis** (å¯é€‰) | â‰¥7.0 | åˆ†å¸ƒå¼ä»»åŠ¡ç®¡ç† |

---

## ğŸ“Š è¯¦ç»†å¼€å‘è®¡åˆ’

### Phase 1: ç¯å¢ƒå‡†å¤‡ä¸ä¾èµ–å®‰è£… (é¢„è®¡45åˆ†é’Ÿ)

#### 1.1 æ£€æŸ¥CUDAç¯å¢ƒ
```bash
# éªŒè¯CUDAç‰ˆæœ¬
nvidia-smi  # ç¡®è®¤CUDA 12.6

# æ£€æŸ¥ç°æœ‰PyTorchç‰ˆæœ¬
python -c "import torch; print(torch.__version__)"  # å½“å‰: 2.7.0+cpu
```

#### 1.2 å¸è½½CPUç‰ˆPyTorch
```bash
pip uninstall torch torchvision torchaudio -y
```

#### 1.3 å®‰è£…PyTorch GPUç‰ˆ (CUDA 12.1å…¼å®¹)
```bash
# CUDA 12.6å‘åå…¼å®¹CUDA 12.1
pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121
```

#### 1.4 å®‰è£…CuPy (CUDA 12.x)
```bash
# ä»é¢„ç¼–è¯‘è½®å­å®‰è£… (æ¨è)
pip install cupy-cuda12x

# éªŒè¯å®‰è£…
python -c "import cupy as cp; print('CuPyç‰ˆæœ¬:', cp.__version__); print('CUDAç‰ˆæœ¬:', cp.cuda.runtime.runtimeGetVersion())"
```

#### 1.5 å®‰è£…WebSocketæ”¯æŒ
```bash
pip install flask-socketio python-socketio eventlet
```

#### 1.6 éªŒè¯GPUå¯ç”¨æ€§
```python
# test_gpu_ready.py
import torch
import cupy as cp

print("=== GPUç¯å¢ƒéªŒè¯ ===")
print(f"PyTorchç‰ˆæœ¬: {torch.__version__}")
print(f"CUDAå¯ç”¨: {torch.cuda.is_available()}")
print(f"GPUåç§°: {torch.cuda.get_device_name(0)}")
print(f"æ˜¾å­˜æ€»é‡: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB")

print(f"\nCuPyç‰ˆæœ¬: {cp.__version__}")
x = cp.array([1, 2, 3])
print(f"CuPyæµ‹è¯•: {x.sum()}")  # åº”è¾“å‡º: 6
```

---

### Phase 2: GPUåŠ é€ŸRQAæ ¸å¿ƒ (é¢„è®¡90åˆ†é’Ÿ)

#### 2.1 åˆ›å»ºGPU RQAæ¨¡å—
**æ–‡ä»¶**: `analysis/rqa_analyzer_gpu.py`

**æ ¸å¿ƒä¼˜åŒ–ç­–ç•¥**:
1. **è·ç¦»çŸ©é˜µè®¡ç®—** (æœ€è€—æ—¶ ~60%):
```python
import cupy as cp

def compute_distance_matrix_gpu(traj, m, tau):
    """
    GPUåŠ é€Ÿçš„è·ç¦»çŸ©é˜µè®¡ç®—

    ä¼˜åŒ–ç‚¹:
    - NumPy â†’ CuPy (20xæé€Ÿ)
    - ä½¿ç”¨GPUå¹¶è¡Œè®¡ç®—æ¬§æ°è·ç¦»
    """
    n_points = len(traj) - (m - 1) * tau

    # æ„å»ºåµŒå…¥ç©ºé—´ (åœ¨GPUä¸Š)
    embedding = cp.zeros((n_points, m), dtype=cp.float32)
    for i in range(m):
        embedding[:, i] = cp.array(traj[i * tau : i * tau + n_points])

    # GPUå¹¶è¡Œè®¡ç®—è·ç¦»çŸ©é˜µ (ä½¿ç”¨CuPyçš„ä¼˜åŒ–kernel)
    # æ›¿ä»£åŸæ¥çš„CPUåµŒå¥—å¾ªç¯
    dist_matrix = cp.linalg.norm(
        embedding[:, None, :] - embedding[None, :, :],
        axis=2
    )

    return cp.asnumpy(dist_matrix)  # åªåœ¨éœ€è¦æ—¶ä¼ å›CPU
```

2. **é€’å½’çŸ©é˜µç”Ÿæˆ**:
```python
def compute_recurrence_matrix_gpu(dist_matrix, eps):
    """GPUåŠ é€Ÿçš„é€’å½’çŸ©é˜µç”Ÿæˆ"""
    dist_gpu = cp.array(dist_matrix)
    rec_matrix_gpu = (dist_gpu < eps).astype(cp.int8)
    return cp.asnumpy(rec_matrix_gpu)
```

3. **RQAæŒ‡æ ‡è®¡ç®—** (æ··åˆCPU/GPU):
```python
def compute_rqa_metrics_gpu(rec_matrix):
    """
    æ··åˆè®¡ç®—ç­–ç•¥:
    - ç®€å•æŒ‡æ ‡(RR): GPU
    - å¤æ‚æŒ‡æ ‡(DET, ENT): CPU (é¿å…GPU-CPUé¢‘ç¹ä¼ è¾“)
    """
    rec_gpu = cp.array(rec_matrix)

    # GPUè®¡ç®—RR
    rr = cp.sum(rec_gpu) / cp.prod(rec_gpu.shape)

    # ä¼ å›CPUè®¡ç®—DET/ENT (é¿å…å¤æ‚kernelç¼–å†™)
    rec_cpu = cp.asnumpy(rec_gpu)
    det, lmax, entr = compute_det_cpu(rec_cpu)  # å¤ç”¨åŸCPUä»£ç 

    return {
        'RR': float(rr),
        'DET': det,
        'L_max': lmax,
        'ENT': entr
    }
```

#### 2.2 å†…å­˜ç®¡ç†ç­–ç•¥
```python
class GPUMemoryManager:
    """GPUæ˜¾å­˜ç®¡ç†å™¨"""

    def __init__(self, max_gpu_usage=0.8):
        self.max_usage = max_gpu_usage
        self.total_mem = cp.cuda.Device(0).mem_info[1]

    def can_allocate(self, size_bytes):
        """æ£€æŸ¥æ˜¯å¦å¯åˆ†é…æŒ‡å®šå¤§å°"""
        free_mem = cp.cuda.Device(0).mem_info[0]
        return free_mem > size_bytes + self.total_mem * (1 - self.max_usage)

    def clear_cache(self):
        """æ¸…ç†GPUç¼“å­˜"""
        cp.get_default_memory_pool().free_all_blocks()
```

#### 2.3 æ€§èƒ½å¯¹æ¯”æµ‹è¯•
```python
# benchmark_gpu.py
import time
import numpy as np
from analysis.rqa_analyzer import compute_rqa_1d  # åŸCPUç‰ˆæœ¬
from analysis.rqa_analyzer_gpu import compute_rqa_1d_gpu  # GPUç‰ˆæœ¬

# æ¨¡æ‹ŸçœŸå®æ•°æ®
traj_x = np.random.randn(5000)

params = {'m': 5, 'tau': 3, 'eps': 0.08, 'lmin': 2}

# CPUç‰ˆæœ¬
start = time.time()
result_cpu = compute_rqa_1d(traj_x, traj_x, params)
time_cpu = time.time() - start

# GPUç‰ˆæœ¬
start = time.time()
result_gpu = compute_rqa_1d_gpu(traj_x, traj_x, params)
time_gpu = time.time() - start

print(f"CPUè€—æ—¶: {time_cpu:.2f}ç§’")
print(f"GPUè€—æ—¶: {time_gpu:.2f}ç§’")
print(f"åŠ é€Ÿæ¯”: {time_cpu/time_gpu:.1f}x")
```

**é¢„æœŸç»“æœ**: 15-25xåŠ é€Ÿ (å–å†³äºæ•°æ®é‡)

---

### Phase 3: å¤šè¿›ç¨‹å¹¶è¡Œå¼•æ“ (é¢„è®¡60åˆ†é’Ÿ)

#### 3.1 å¹¶è¡Œè°ƒåº¦å™¨è®¾è®¡
**æ–‡ä»¶**: `visualization/parallel_executor.py`

```python
import os
import multiprocessing as mp
from concurrent.futures import ProcessPoolExecutor, as_completed
import cupy as cp

class GPUParallelExecutor:
    """GPUå¤šè¿›ç¨‹å¹¶è¡Œæ‰§è¡Œå™¨"""

    def __init__(self, n_workers=4, gpu_id=0):
        """
        Args:
            n_workers: å¹¶è¡Œworkeræ•°é‡ (å»ºè®®4-6)
            gpu_id: ä½¿ç”¨çš„GPUç¼–å·
        """
        self.n_workers = n_workers
        self.gpu_id = gpu_id

        # è®¡ç®—æ¯ä¸ªworkerçš„æ˜¾å­˜é…é¢
        total_mem = cp.cuda.Device(gpu_id).mem_info[1]
        self.mem_per_worker = (total_mem * 0.8) / n_workers  # 80%æ˜¾å­˜åˆ†é…

    def execute_batch(self, param_combinations, callback=None):
        """
        å¹¶è¡Œæ‰§è¡Œæ‰¹é‡ä»»åŠ¡

        Args:
            param_combinations: å‚æ•°ç»„åˆåˆ—è¡¨
            callback: è¿›åº¦å›è°ƒå‡½æ•° callback(index, result)
        """
        results = []

        with ProcessPoolExecutor(
            max_workers=self.n_workers,
            mp_context=mp.get_context('spawn')  # Windowså¿…é¡»ç”¨spawn
        ) as executor:
            # æäº¤æ‰€æœ‰ä»»åŠ¡
            future_to_params = {
                executor.submit(
                    self._worker_task,
                    params,
                    self.gpu_id
                ): (i, params)
                for i, params in enumerate(param_combinations)
            }

            # æ”¶é›†ç»“æœ
            for future in as_completed(future_to_params):
                idx, params = future_to_params[future]
                try:
                    result = future.result()
                    results.append((idx, params, result))

                    # è¿›åº¦å›è°ƒ
                    if callback:
                        callback(idx, result)

                except Exception as e:
                    print(f"âŒ ä»»åŠ¡{idx}å¤±è´¥: {e}")
                    results.append((idx, params, {'error': str(e)}))

        return results

    @staticmethod
    def _worker_task(params, gpu_id):
        """
        Workerä»»åŠ¡å‡½æ•° (åœ¨å­è¿›ç¨‹ä¸­æ‰§è¡Œ)

        æ³¨æ„: å¿…é¡»åœ¨å­è¿›ç¨‹ä¸­é‡æ–°åˆå§‹åŒ–CuPyä¸Šä¸‹æ–‡
        """
        import cupy as cp
        from analysis.rqa_analyzer_gpu import compute_rqa_1d_gpu

        # è®¾ç½®GPUè®¾å¤‡
        cp.cuda.Device(gpu_id).use()

        # æ‰§è¡ŒRQAæµç¨‹ (è°ƒç”¨GPUç‰ˆæœ¬)
        try:
            from visualization.rqa_pipeline_api import execute_full_pipeline_internal_gpu
            result = execute_full_pipeline_internal_gpu(params)
            return result
        finally:
            # æ¸…ç†GPUç¼“å­˜
            cp.get_default_memory_pool().free_all_blocks()
```

#### 3.2 Workeræ•°é‡ä¼˜åŒ–ç­–ç•¥
```python
def calculate_optimal_workers(gpu_mem_gb=16, single_task_mem_gb=2.5):
    """
    è®¡ç®—æœ€ä¼˜workeræ•°é‡

    Args:
        gpu_mem_gb: GPUæ˜¾å­˜å®¹é‡
        single_task_mem_gb: å•ä»»åŠ¡å¹³å‡æ˜¾å­˜å ç”¨

    Returns:
        æœ€ä¼˜workeræ•°é‡
    """
    # ä¿ç•™20%æ˜¾å­˜buffer
    usable_mem = gpu_mem_gb * 0.8

    # ç†è®ºæœ€å¤§workeræ•°
    max_workers = int(usable_mem / single_task_mem_gb)

    # CPUæ ¸å¿ƒæ•°é™åˆ¶
    cpu_cores = os.cpu_count()

    # å–è¾ƒå°å€¼
    optimal = min(max_workers, cpu_cores // 2, 6)  # æœ€å¤š6ä¸ª

    return max(optimal, 1)
```

---

### Phase 4: WebSocketå®æ—¶è¿›åº¦ (é¢„è®¡45åˆ†é’Ÿ)

#### 4.1 åç«¯WebSocketæœåŠ¡
**æ–‡ä»¶**: `visualization/socketio_server.py`

```python
from flask_socketio import SocketIO, emit
from flask import Flask

# åˆå§‹åŒ–SocketIO
socketio = SocketIO(cors_allowed_origins="*")

class ProgressBroadcaster:
    """è¿›åº¦å¹¿æ’­å™¨"""

    def __init__(self, socketio_instance):
        self.sio = socketio_instance
        self.current_task = None

    def start_batch(self, total_count):
        """å¼€å§‹æ‰¹å¤„ç†"""
        self.sio.emit('batch_started', {
            'total': total_count,
            'timestamp': time.time()
        })

    def update_progress(self, current, total, params, result):
        """æ›´æ–°è¿›åº¦"""
        self.sio.emit('progress_update', {
            'current': current,
            'total': total,
            'progress': (current / total) * 100,
            'param_signature': generate_param_signature(params),
            'success': result.get('success', False),
            'skipped': result.get('skipped', False),
            'timestamp': time.time()
        })

    def batch_complete(self, stats):
        """æ‰¹å¤„ç†å®Œæˆ"""
        self.sio.emit('batch_completed', {
            'stats': stats,
            'timestamp': time.time()
        })

    def gpu_stats_update(self, gpu_util, mem_used, mem_total):
        """GPUçŠ¶æ€æ›´æ–° (æ¯5ç§’)"""
        self.sio.emit('gpu_stats', {
            'utilization': gpu_util,
            'memory_used': mem_used,
            'memory_total': mem_total,
            'memory_percent': (mem_used / mem_total) * 100
        })
```

#### 4.2 GPUç›‘æ§çº¿ç¨‹
```python
import threading
import pynvml

class GPUMonitor:
    """GPUçŠ¶æ€ç›‘æ§"""

    def __init__(self, broadcaster, interval=5):
        self.broadcaster = broadcaster
        self.interval = interval
        self.running = False

        pynvml.nvmlInit()
        self.handle = pynvml.nvmlDeviceGetHandleByIndex(0)

    def start(self):
        """å¯åŠ¨ç›‘æ§"""
        self.running = True
        self.thread = threading.Thread(target=self._monitor_loop)
        self.thread.daemon = True
        self.thread.start()

    def _monitor_loop(self):
        while self.running:
            # è·å–GPUçŠ¶æ€
            util = pynvml.nvmlDeviceGetUtilizationRates(self.handle)
            mem_info = pynvml.nvmlDeviceGetMemoryInfo(self.handle)

            # å¹¿æ’­çŠ¶æ€
            self.broadcaster.gpu_stats_update(
                gpu_util=util.gpu,
                mem_used=mem_info.used / 1e9,  # GB
                mem_total=mem_info.total / 1e9
            )

            time.sleep(self.interval)

    def stop(self):
        self.running = False
        pynvml.nvmlShutdown()
```

#### 4.3 å‰ç«¯WebSocketæ¥æ”¶
**æ–‡ä»¶**: `visualization/templates/enhanced_index.html`

```javascript
// åˆå§‹åŒ–SocketIOè¿æ¥
const socket = io('http://127.0.0.1:8080');

// ç›‘å¬æ‰¹å¤„ç†å¼€å§‹
socket.on('batch_started', (data) => {
    console.log('ğŸ“¦ æ‰¹å¤„ç†å·²å¯åŠ¨ï¼Œæ€»ä»»åŠ¡æ•°:', data.total);
    document.getElementById('batchProgressPanel').style.display = 'block';
});

// ç›‘å¬è¿›åº¦æ›´æ–°
socket.on('progress_update', (data) => {
    const progress = data.progress.toFixed(1);

    // æ›´æ–°è¿›åº¦æ¡
    const progressBar = document.getElementById('batchProgressBar');
    progressBar.style.width = progress + '%';
    progressBar.innerHTML = `<span class="fs-5 fw-bold">${progress}%</span>`;

    // æ›´æ–°ç»Ÿè®¡å¡ç‰‡
    document.getElementById('batch-current-count').textContent = data.current;

    // å®æ—¶æ—¥å¿—
    addLogEntry(data.param_signature, data.success, data.skipped);
});

// ç›‘å¬GPUçŠ¶æ€
socket.on('gpu_stats', (data) => {
    document.getElementById('gpu-utilization').textContent = data.utilization + '%';
    document.getElementById('gpu-memory').textContent =
        `${data.memory_used.toFixed(1)} / ${data.memory_total.toFixed(1)} GB`;

    // æ›´æ–°GPUè¿›åº¦æ¡
    const gpuBar = document.getElementById('gpuUtilizationBar');
    gpuBar.style.width = data.utilization + '%';
    gpuBar.className = data.utilization > 80 ? 'bg-success' : 'bg-warning';
});

// ç›‘å¬æ‰¹å¤„ç†å®Œæˆ
socket.on('batch_completed', (data) => {
    console.log('âœ… æ‰¹å¤„ç†å®Œæˆï¼', data.stats);
    showCompletionModal(data.stats);
});
```

---

### Phase 5: APIé›†æˆ (é¢„è®¡30åˆ†é’Ÿ)

#### 5.1 æ–°å¢GPUæ‰¹å¤„ç†API
**æ–‡ä»¶**: `visualization/rqa_pipeline_api.py`

```python
@rqa_pipeline_bp.route('/api/rqa-pipeline/batch-execute-gpu', methods=['POST'])
def batch_execute_gpu():
    """GPUå¹¶è¡Œæ‰¹å¤„ç†API"""
    data = request.json

    # è§£æå‚æ•°
    batch_config = data.get('batch_config', {})
    param_combinations = generate_param_grid(
        batch_config['m_range'],
        batch_config['tau_range'],
        batch_config['eps_range'],
        batch_config['lmin_range']
    )

    total_count = len(param_combinations)

    # åˆå§‹åŒ–è¿›åº¦å¹¿æ’­å™¨
    broadcaster = ProgressBroadcaster(socketio)
    broadcaster.start_batch(total_count)

    # å¯åŠ¨GPUç›‘æ§
    gpu_monitor = GPUMonitor(broadcaster)
    gpu_monitor.start()

    # è®¡ç®—æœ€ä¼˜workeræ•°
    n_workers = calculate_optimal_workers()

    # æ‰§è¡Œå¹¶è¡Œä»»åŠ¡
    executor = GPUParallelExecutor(n_workers=n_workers)

    def progress_callback(idx, result):
        broadcaster.update_progress(
            idx + 1,
            total_count,
            param_combinations[idx],
            result
        )

    try:
        results = executor.execute_batch(
            param_combinations,
            callback=progress_callback
        )

        # ç»Ÿè®¡ç»“æœ
        stats = {
            'total': total_count,
            'success': sum(1 for _, _, r in results if r.get('success')),
            'failed': sum(1 for _, _, r in results if not r.get('success')),
            'skipped': sum(1 for _, _, r in results if r.get('skipped'))
        }

        broadcaster.batch_complete(stats)

        return jsonify({'success': True, 'stats': stats})

    finally:
        gpu_monitor.stop()
```

#### 5.2 GPUç‰ˆæœ¬pipelineå‡½æ•°
```python
def execute_full_pipeline_internal_gpu(params):
    """
    GPUåŠ é€Ÿçš„å®Œæ•´pipeline

    æµç¨‹:
    1. RQAè®¡ç®— (GPUåŠ é€Ÿ)
    2. æ•°æ®åˆå¹¶ (CPU)
    3. ç‰¹å¾æå– (æ··åˆ)
    4. ç»Ÿè®¡åˆ†æ (CPU)
    5. å¯è§†åŒ– (CPU)
    """
    from analysis.rqa_analyzer_gpu import compute_rqa_1d_gpu, compute_rqa_2d_gpu

    param_signature = generate_param_signature(params)

    # æ–­ç‚¹ç»­ä¼ æ£€æŸ¥
    param_dir = os.path.join(MODULE10_DATASET_ROOT, param_signature)
    metadata_file = os.path.join(param_dir, 'metadata.json')

    if os.path.exists(metadata_file):
        with open(metadata_file, 'r') as f:
            metadata = json.load(f)
        if all(metadata.get(f'step_{i}_completed') for i in range(1, 6)):
            return {'success': True, 'skipped': True, 'param_signature': param_signature}

    try:
        # Step 1: RQAè®¡ç®— (GPUåŠ é€Ÿ) âš¡
        rqa_results = {}
        for group in ['control', 'mci', 'ad']:
            group_results = []
            for subject_data in load_group_data(group):
                # ä½¿ç”¨GPUç‰ˆæœ¬è®¡ç®—
                result_1d = compute_rqa_1d_gpu(subject_data['x'], subject_data['y'], params)
                result_2d = compute_rqa_2d_gpu(subject_data['x'], subject_data['y'], params)
                group_results.append({**result_1d, **result_2d})
            rqa_results[group] = group_results

        update_metadata(param_dir, 'step_1_completed', True)

        # Step 2-5: ä½¿ç”¨åŸCPUç‰ˆæœ¬
        merge_data(rqa_results, param_dir)
        update_metadata(param_dir, 'step_2_completed', True)

        enrich_features(param_dir)
        update_metadata(param_dir, 'step_3_completed', True)

        statistical_analysis(param_dir)
        update_metadata(param_dir, 'step_4_completed', True)

        generate_visualizations(param_dir)
        update_metadata(param_dir, 'step_5_completed', True)

        return {'success': True, 'param_signature': param_signature}

    except Exception as e:
        return {'success': False, 'error': str(e), 'param_signature': param_signature}
```

---

### Phase 6: å‰ç«¯UIå¢å¼º (é¢„è®¡30åˆ†é’Ÿ)

#### 6.1 GPUæ¨¡å¼åˆ‡æ¢å¼€å…³
**æ–‡ä»¶**: `visualization/static/modules/module5_rqa_pipeline.html`

```html
<!-- åœ¨æ‰¹å¤„ç†é…ç½®é¢æ¿é¡¶éƒ¨æ·»åŠ  -->
<div class="card border-success mb-3">
    <div class="card-header bg-success text-white">
        <h5><i class="fas fa-rocket"></i> GPUå¹¶è¡ŒåŠ é€Ÿ</h5>
    </div>
    <div class="card-body">
        <div class="row align-items-center">
            <!-- GPUæ¨¡å¼å¼€å…³ -->
            <div class="col-md-4">
                <div class="form-check form-switch">
                    <input class="form-check-input" type="checkbox"
                           id="enableGpuMode" checked>
                    <label class="form-check-label" for="enableGpuMode">
                        <strong>å¯ç”¨GPUåŠ é€Ÿ</strong>
                        <small class="text-muted d-block">
                            RTX 3080 Mobile (16GB)
                        </small>
                    </label>
                </div>
            </div>

            <!-- å¹¶è¡ŒWorkeræ•°é‡ -->
            <div class="col-md-4">
                <label class="form-label"><strong>å¹¶è¡Œä»»åŠ¡æ•°</strong></label>
                <input type="number" class="form-control"
                       id="parallelWorkers" value="4" min="1" max="6">
                <small class="text-muted">å»ºè®®4-6ä¸ª (æ ¹æ®æ˜¾å­˜è°ƒæ•´)</small>
            </div>

            <!-- é¢„ä¼°æ—¶é—´ -->
            <div class="col-md-4">
                <div class="alert alert-info mb-0">
                    <strong>é¢„è®¡è€—æ—¶</strong><br>
                    <span id="estimatedTime" class="fs-5">è®¡ç®—ä¸­...</span>
                </div>
            </div>
        </div>
    </div>
</div>
```

#### 6.2 å®æ—¶GPUç›‘æ§é¢æ¿
```html
<!-- GPUçŠ¶æ€ç›‘æ§å¡ç‰‡ -->
<div class="card border-warning mb-3" id="gpuMonitorPanel" style="display:none;">
    <div class="card-header bg-warning text-dark">
        <h5><i class="fas fa-microchip"></i> GPUå®æ—¶çŠ¶æ€</h5>
    </div>
    <div class="card-body">
        <div class="row">
            <!-- GPUåˆ©ç”¨ç‡ -->
            <div class="col-md-6">
                <label><strong>GPUåˆ©ç”¨ç‡</strong></label>
                <div class="progress mb-2" style="height: 30px;">
                    <div class="progress-bar bg-success"
                         id="gpuUtilizationBar"
                         style="width: 0%">
                        <span id="gpu-utilization">0%</span>
                    </div>
                </div>
            </div>

            <!-- æ˜¾å­˜å ç”¨ -->
            <div class="col-md-6">
                <label><strong>æ˜¾å­˜å ç”¨</strong></label>
                <div class="progress mb-2" style="height: 30px;">
                    <div class="progress-bar bg-info"
                         id="gpuMemoryBar"
                         style="width: 0%">
                        <span id="gpu-memory">0 / 16 GB</span>
                    </div>
                </div>
            </div>
        </div>

        <!-- å®æ—¶æ—¥å¿—æµ -->
        <div class="mt-3">
            <label><strong>å®æ—¶å¤„ç†æ—¥å¿—</strong></label>
            <div id="realtimeLog" class="border p-2"
                 style="height: 150px; overflow-y: auto; background: #1e1e1e; color: #00ff00; font-family: monospace;">
                ç­‰å¾…ä»»åŠ¡å¯åŠ¨...
            </div>
        </div>
    </div>
</div>
```

#### 6.3 JavaScriptå¢å¼º
```javascript
// ä¼°ç®—æ—¶é—´è®¡ç®—
function updateEstimatedTime() {
    const totalCombinations = updateBatchCombinationCount();
    const gpuEnabled = document.getElementById('enableGpuMode').checked;
    const workers = parseInt(document.getElementById('parallelWorkers').value) || 4;

    let timePerTask = 50;  // CPUé»˜è®¤50ç§’

    if (gpuEnabled) {
        timePerTask = 2.5;  // GPUåŠ é€Ÿå2.5ç§’
    }

    const totalSeconds = (totalCombinations * timePerTask) / workers;
    const hours = Math.floor(totalSeconds / 3600);
    const minutes = Math.floor((totalSeconds % 3600) / 60);

    document.getElementById('estimatedTime').textContent =
        `${hours}å°æ—¶ ${minutes}åˆ†é’Ÿ`;
}

// å®æ—¶æ—¥å¿—æ·»åŠ 
function addLogEntry(paramSig, success, skipped) {
    const logDiv = document.getElementById('realtimeLog');
    const timestamp = new Date().toLocaleTimeString();

    let icon = 'âœ…';
    let color = '#00ff00';
    if (skipped) {
        icon = 'â­ï¸';
        color = '#ffaa00';
    } else if (!success) {
        icon = 'âŒ';
        color = '#ff0000';
    }

    const entry = `<div style="color: ${color}">[${timestamp}] ${icon} ${paramSig}</div>`;
    logDiv.innerHTML += entry;
    logDiv.scrollTop = logDiv.scrollHeight;  // è‡ªåŠ¨æ»šåŠ¨åˆ°åº•éƒ¨
}

// ä¿®æ”¹æ‰¹å¤„ç†æ‰§è¡Œå‡½æ•°
async function startBatchExecution() {
    const gpuEnabled = document.getElementById('enableGpuMode').checked;
    const workers = parseInt(document.getElementById('parallelWorkers').value) || 4;

    const apiEndpoint = gpuEnabled
        ? '/api/rqa-pipeline/batch-execute-gpu'
        : '/api/rqa-pipeline/batch-execute';

    const batchConfig = {
        m_range: { /* ... */ },
        tau_range: { /* ... */ },
        eps_range: { /* ... */ },
        lmin_range: { /* ... */ },
        n_workers: workers
    };

    // æ˜¾ç¤ºGPUç›‘æ§é¢æ¿
    if (gpuEnabled) {
        document.getElementById('gpuMonitorPanel').style.display = 'block';
    }

    try {
        const response = await fetch(apiEndpoint, {
            method: 'POST',
            headers: {'Content-Type': 'application/json'},
            body: JSON.stringify({batch_config: batchConfig})
        });

        const result = await response.json();
        console.log('æ‰¹å¤„ç†ç»“æœ:', result);

    } catch (error) {
        console.error('æ‰¹å¤„ç†å¤±è´¥:', error);
        alert('æ‰¹å¤„ç†æ‰§è¡Œå¤±è´¥: ' + error.message);
    }
}
```

---

### Phase 7: æ€§èƒ½æµ‹è¯•ä¸è°ƒä¼˜ (é¢„è®¡60åˆ†é’Ÿ)

#### 7.1 åŸºå‡†æµ‹è¯•è„šæœ¬
**æ–‡ä»¶**: `tests/benchmark_gpu_parallel.py`

```python
import time
import numpy as np
from visualization.parallel_executor import GPUParallelExecutor

def benchmark_full_pipeline():
    """å®Œæ•´pipelineæ€§èƒ½æµ‹è¯•"""

    # æµ‹è¯•å‚æ•°é›† (12ä¸ªç»„åˆ)
    test_params = [
        {'m': m, 'tau': 1, 'eps': 0.06, 'lmin': 2}
        for m in range(1, 13)
    ]

    print("="*60)
    print("GPUå¹¶è¡ŒåŠ é€Ÿæ€§èƒ½æµ‹è¯•")
    print("="*60)

    # æµ‹è¯•ä¸åŒworkeræ•°é‡
    for n_workers in [1, 2, 4, 6]:
        executor = GPUParallelExecutor(n_workers=n_workers)

        start = time.time()
        results = executor.execute_batch(test_params)
        elapsed = time.time() - start

        success_count = sum(1 for _, _, r in results if r.get('success'))

        print(f"\nWorkeræ•°é‡: {n_workers}")
        print(f"  æ€»è€—æ—¶: {elapsed:.1f}ç§’")
        print(f"  å•ä»»åŠ¡å¹³å‡: {elapsed/len(test_params):.1f}ç§’")
        print(f"  æˆåŠŸç‡: {success_count}/{len(test_params)}")
        print(f"  ç†è®ºåŠ é€Ÿæ¯”: {1*50/(elapsed/len(test_params)):.1f}x")

if __name__ == '__main__':
    benchmark_full_pipeline()
```

#### 7.2 æ€§èƒ½ä¼˜åŒ–æ£€æŸ¥æ¸…å•

| ä¼˜åŒ–é¡¹ | ç›®æ ‡ | éªŒè¯æ–¹æ³• |
|--------|------|----------|
| GPUåˆ©ç”¨ç‡ | >80% | `nvidia-smi dmon` |
| æ˜¾å­˜å ç”¨ | <14GB (ç•™2GB buffer) | `nvidia-smi` |
| CPUç“¶é¢ˆ | <30% | ä»»åŠ¡ç®¡ç†å™¨ |
| ç£ç›˜I/O | <20% | èµ„æºç›‘è§†å™¨ |
| å•ä»»åŠ¡è€—æ—¶ | <3ç§’ | benchmarkè„šæœ¬ |
| å¹¶è¡Œæ•ˆç‡ | >80% | å¯¹æ¯”å•çº¿ç¨‹ |

#### 7.3 å¸¸è§æ€§èƒ½é—®é¢˜æ’æŸ¥

**é—®é¢˜1: GPUåˆ©ç”¨ç‡ä½ (<50%)**
```python
# åŸå› : CPUé¢„å¤„ç†æˆä¸ºç“¶é¢ˆ
# è§£å†³: å¢åŠ æ•°æ®é¢„åŠ è½½çº¿ç¨‹

from concurrent.futures import ThreadPoolExecutor

def preload_data_async(param_list):
    """å¼‚æ­¥é¢„åŠ è½½æ•°æ®"""
    with ThreadPoolExecutor(max_workers=4) as executor:
        futures = [executor.submit(load_subject_data, p) for p in param_list]
        return [f.result() for f in futures]
```

**é—®é¢˜2: æ˜¾å­˜æº¢å‡º (OOM)**
```python
# åŸå› : å¹¶è¡Œä»»åŠ¡è¿‡å¤š
# è§£å†³: åŠ¨æ€è°ƒæ•´workeræ•°é‡

def adaptive_worker_count():
    """æ ¹æ®æ˜¾å­˜åŠ¨æ€è°ƒæ•´"""
    free_mem = cp.cuda.Device(0).mem_info[0]

    if free_mem < 2e9:  # å°äº2GB
        return 2
    elif free_mem < 4e9:  # 2-4GB
        return 3
    else:
        return 4
```

**é—®é¢˜3: ç£ç›˜I/Oç“¶é¢ˆ**
```python
# åŸå› : é¢‘ç¹è¯»å†™metadata.json
# è§£å†³: æ‰¹é‡å†™å…¥

class MetadataBuffer:
    """å…ƒæ•°æ®ç¼“å†²å†™å…¥"""
    def __init__(self, flush_interval=10):
        self.buffer = {}
        self.flush_interval = flush_interval
        self.counter = 0

    def update(self, param_sig, step, value):
        self.buffer.setdefault(param_sig, {})[step] = value
        self.counter += 1

        if self.counter >= self.flush_interval:
            self.flush()

    def flush(self):
        """æ‰¹é‡å†™å…¥ç£ç›˜"""
        for param_sig, updates in self.buffer.items():
            metadata_file = os.path.join(MODULE10_DATASET_ROOT, param_sig, 'metadata.json')
            # æ‰¹é‡æ›´æ–°...
        self.buffer.clear()
        self.counter = 0
```

---

## ğŸ“ˆ é¢„æœŸæ€§èƒ½æå‡

### æ€§èƒ½å¯¹æ¯”è¡¨

| åœºæ™¯ | å½“å‰æ–¹æ¡ˆ | GPUå¹¶è¡Œæ–¹æ¡ˆ | æå‡å€æ•° |
|------|---------|------------|---------|
| **å•ä»»åŠ¡è€—æ—¶** | 50ç§’ | 2.5ç§’ | **20x** |
| **100ç»„åˆ** | 1.4å°æ—¶ | 4åˆ†é’Ÿ | **21x** |
| **1,200ç»„åˆ** | 16.7å°æ—¶ | 50åˆ†é’Ÿ | **20x** |
| **10,200ç»„åˆ** | 142å°æ—¶ | **6.5å°æ—¶** | **22x** |

### èµ„æºåˆ©ç”¨ç‡

| æŒ‡æ ‡ | ä¼˜åŒ–å‰ | ä¼˜åŒ–å |
|------|--------|--------|
| GPUåˆ©ç”¨ç‡ | 0% | 85-90% |
| CPUåˆ©ç”¨ç‡ | 15% (å•æ ¸) | 50% (å¤šæ ¸) |
| æ˜¾å­˜å ç”¨ | 0 GB | 12-14 GB |
| ç³»ç»Ÿå†…å­˜ | 2 GB | 8 GB |
| ç£ç›˜I/O | ä½ | ä¸­ç­‰ |

---

## ğŸš€ å®æ–½æ—¶é—´è¡¨

| é˜¶æ®µ | ä»»åŠ¡ | é¢„è®¡è€—æ—¶ | å…³é”®äº§å‡º |
|------|------|---------|---------|
| Phase 1 | ç¯å¢ƒå‡†å¤‡ | 45åˆ†é’Ÿ | PyTorch+CuPyå®‰è£…å®Œæˆ |
| Phase 2 | GPU RQAæ ¸å¿ƒ | 90åˆ†é’Ÿ | `rqa_analyzer_gpu.py` |
| Phase 3 | å¹¶è¡Œå¼•æ“ | 60åˆ†é’Ÿ | `parallel_executor.py` |
| Phase 4 | WebSocket | 45åˆ†é’Ÿ | å®æ—¶è¿›åº¦æ¨é€ |
| Phase 5 | APIé›†æˆ | 30åˆ†é’Ÿ | `/batch-execute-gpu` |
| Phase 6 | å‰ç«¯UI | 30åˆ†é’Ÿ | GPUç›‘æ§é¢æ¿ |
| Phase 7 | æ€§èƒ½æµ‹è¯• | 60åˆ†é’Ÿ | åŸºå‡†æµ‹è¯•æŠ¥å‘Š |
| **æ€»è®¡** | | **6å°æ—¶** | å®Œæ•´GPUå¹¶è¡Œç³»ç»Ÿ |

---

## âš ï¸ é£é™©ä¸åº”å¯¹

### æŠ€æœ¯é£é™©

| é£é™© | æ¦‚ç‡ | å½±å“ | åº”å¯¹æªæ–½ |
|------|------|------|---------|
| CUDAç‰ˆæœ¬ä¸å…¼å®¹ | ä½ | é«˜ | ä½¿ç”¨cu121è½®å­ (å‘åå…¼å®¹) |
| æ˜¾å­˜æº¢å‡º | ä¸­ | ä¸­ | åŠ¨æ€è°ƒæ•´workeræ•°é‡ |
| å¤šè¿›ç¨‹GPUå†²çª | ä¸­ | é«˜ | ä½¿ç”¨spawnä¸Šä¸‹æ–‡+æ˜¾å¼è®¾å¤‡ç»‘å®š |
| Windowsè¿›ç¨‹é™åˆ¶ | ä½ | ä¸­ | ä½¿ç”¨ProcessPoolExecutor |

### å›é€€æ–¹æ¡ˆ

å¦‚æœGPUåŠ é€Ÿå‡ºç°é—®é¢˜:
1. **å¿«é€Ÿå›é€€**: ä¿ç•™åŸCPUç‰ˆæœ¬API (`/batch-execute`)
2. **é™çº§ç­–ç•¥**: GPUå¤±è´¥è‡ªåŠ¨åˆ‡æ¢åˆ°CPU
3. **è°ƒè¯•æ¨¡å¼**: æ·»åŠ `--cpu-only`å¯åŠ¨å‚æ•°

```python
# è‡ªåŠ¨é™çº§ç¤ºä¾‹
def execute_rqa_with_fallback(params):
    try:
        # ä¼˜å…ˆGPU
        return compute_rqa_1d_gpu(params)
    except (cp.cuda.memory.OutOfMemoryError, RuntimeError):
        # é™çº§åˆ°CPU
        print("âš ï¸ GPUå¤±è´¥ï¼Œåˆ‡æ¢åˆ°CPUæ¨¡å¼")
        return compute_rqa_1d_cpu(params)
```

---

## ğŸ“ éªŒæ”¶æ ‡å‡†

### åŠŸèƒ½éªŒæ”¶
- [ ] æˆåŠŸå®‰è£…PyTorch GPUç‰ˆæœ¬ + CuPy
- [ ] GPUç‰ˆRQAè®¡ç®—æ­£ç¡®æ€§éªŒè¯ (ä¸CPUç‰ˆæœ¬å¯¹æ¯”è¯¯å·®<0.1%)
- [ ] 4-workerå¹¶è¡Œæ‰§è¡Œæ— é”™è¯¯
- [ ] WebSocketå®æ—¶è¿›åº¦æ­£å¸¸æ¨é€
- [ ] GPUç›‘æ§é¢æ¿å®æ—¶æ›´æ–°

### æ€§èƒ½éªŒæ”¶
- [ ] å•ä»»åŠ¡è€—æ—¶ < 3ç§’ (ç›¸æ¯”CPU 50ç§’)
- [ ] 10,200ç»„åˆæ€»è€—æ—¶ < 8å°æ—¶ (ç›®æ ‡6.5å°æ—¶)
- [ ] GPUåˆ©ç”¨ç‡ > 80%
- [ ] æ˜¾å­˜å ç”¨ç¨³å®šåœ¨ 12-14GB
- [ ] æ— å†…å­˜æ³„æ¼ (é•¿æ—¶é—´è¿è¡Œæ˜¾å­˜ä¸å¢é•¿)

### ç”¨æˆ·ä½“éªŒéªŒæ”¶
- [ ] å‰ç«¯å¯åˆ‡æ¢GPU/CPUæ¨¡å¼
- [ ] å®æ—¶æ—¥å¿—æµç•…æ˜¾ç¤º (æ— å¡é¡¿)
- [ ] è¿›åº¦æ¡å‡†ç¡®åæ˜ å®é™…è¿›åº¦
- [ ] å‡ºé”™æ—¶æœ‰æ˜ç¡®æç¤º

---

## ğŸ“š å‚è€ƒèµ„æ–™

### å®˜æ–¹æ–‡æ¡£
1. CuPyå®˜æ–¹æ–‡æ¡£: https://docs.cupy.dev/en/stable/
2. PyTorch CUDAæœ€ä½³å®è·µ: https://pytorch.org/docs/stable/notes/cuda.html
3. Flask-SocketIOæ–‡æ¡£: https://flask-socketio.readthedocs.io/

### æ€§èƒ½ä¼˜åŒ–å‚è€ƒ
1. CUDAç¼–ç¨‹æŒ‡å—: https://docs.nvidia.com/cuda/cuda-c-programming-guide/
2. Pythonå¤šè¿›ç¨‹GPUå…±äº«: https://stackoverflow.com/questions/tagged/multiprocessing+cuda
3. RQAç®—æ³•ä¼˜åŒ–è®ºæ–‡: Marwan et al. (2007) "Recurrence plots for the analysis of complex systems"

---

## ğŸ”§ åç»­æ‰©å±•

### Phase 8: åˆ†å¸ƒå¼æ‰©å±• (å¯é€‰)
å¦‚æœéœ€è¦å¤„ç†æ›´å¤§è§„æ¨¡æ•°æ® (100,000+ç»„åˆ):
- æ·»åŠ Redisä»»åŠ¡é˜Ÿåˆ—
- æ”¯æŒå¤šGPUå¹¶è¡Œ (RTX 3080 + å…¶ä»–GPU)
- é›†ç¾¤éƒ¨ç½² (å¤šå°æœºå™¨ååŒ)

### Phase 9: ç®—æ³•ä¼˜åŒ– (å¯é€‰)
- å®ç°è‡ªé€‚åº”Îµé€‰æ‹©ç®—æ³•
- æ·»åŠ RQAæŒ‡æ ‡ç¼“å­˜æœºåˆ¶
- ä¼˜åŒ–è·ç¦»çŸ©é˜µè®¡ç®— (ä½¿ç”¨CUDA kernel)

---

**æ–‡æ¡£ç‰ˆæœ¬**: v1.0
**æœ€åæ›´æ–°**: 2025-10-01
**ç»´æŠ¤è€…**: Claude AI Assistant
