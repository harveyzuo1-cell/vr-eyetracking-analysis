# Module05 RQAåˆ†ææ¨¡å—è®¾è®¡æ–‡æ¡£

**æ–‡æ¡£ç‰ˆæœ¬**: v1.0
**åˆ›å»ºæ—¥æœŸ**: 2025-10-07
**ä½œè€…**: ç³»ç»Ÿæ¶æ„å›¢é˜Ÿ
**æ¨¡å—åç§°**: Module05 - é€’å½’é‡åŒ–åˆ†æ (Recurrence Quantification Analysis)

---

## ğŸ“‹ ç›®å½•

1. [æ‰§è¡Œæ‘˜è¦](#1-æ‰§è¡Œæ‘˜è¦)
2. [éœ€æ±‚åˆ†æ](#2-éœ€æ±‚åˆ†æ)
3. [ç³»ç»Ÿæ¶æ„è®¾è®¡](#3-ç³»ç»Ÿæ¶æ„è®¾è®¡)
4. [æ ¸å¿ƒåŠŸèƒ½è®¾è®¡](#4-æ ¸å¿ƒåŠŸèƒ½è®¾è®¡)
5. [æ•°æ®æµè®¾è®¡](#5-æ•°æ®æµè®¾è®¡)
6. [APIæ¥å£è®¾è®¡](#6-apiæ¥å£è®¾è®¡)
7. [å¹¶è¡Œå¤„ç†è®¾è®¡](#7-å¹¶è¡Œå¤„ç†è®¾è®¡)
8. [æ•°æ®æŒä¹…åŒ–è®¾è®¡](#8-æ•°æ®æŒä¹…åŒ–è®¾è®¡)
9. [å‰ç«¯äº¤äº’è®¾è®¡](#9-å‰ç«¯äº¤äº’è®¾è®¡)
10. [æ€§èƒ½ä¼˜åŒ–ç­–ç•¥](#10-æ€§èƒ½ä¼˜åŒ–ç­–ç•¥)
11. [å®æ–½è®¡åˆ’](#11-å®æ–½è®¡åˆ’)

---

## 1. æ‰§è¡Œæ‘˜è¦

### 1.1 æ¨¡å—æ¦‚è¿°

Module05 RQAåˆ†ææ¨¡å—æ˜¯çœ¼åŠ¨æ•°æ®åˆ†æå¹³å°çš„æ ¸å¿ƒåˆ†æç»„ä»¶ï¼Œè´Ÿè´£å¯¹æ ¡å‡†åçš„çœ¼åŠ¨è½¨è¿¹æ•°æ®è¿›è¡Œé€’å½’é‡åŒ–åˆ†æï¼ˆRQAï¼‰ï¼Œæå–éçº¿æ€§åŠ¨åŠ›å­¦ç‰¹å¾ï¼Œç”¨äºè®¤çŸ¥åŠŸèƒ½è¯„ä¼°å’Œç–¾ç—…è¯Šæ–­ã€‚

### 1.2 æ ¸å¿ƒä»·å€¼

- **å¤§è§„æ¨¡å‚æ•°ç©ºé—´æ¢ç´¢**: æ”¯æŒ10,200+å‚æ•°ç»„åˆçš„æ‰¹é‡åˆ†æ
- **CPUå¤šçº¿ç¨‹ä¼˜åŒ–**: å……åˆ†åˆ©ç”¨CPUå¤šæ ¸èµ„æºï¼Œé¿å…GPUä¾èµ–
- **å®Œæ•´åˆ†ææµç¨‹**: ä»RQAè®¡ç®—åˆ°ç»Ÿè®¡åˆ†æçš„5æ­¥å®Œæ•´æµç¨‹
- **æ¶æ„æœ€ä½³å®è·µ**: éµå¾ªModule04çš„å“è¶Šæ¶æ„æ¨¡å¼

### 1.3 å…³é”®ç‰¹æ€§

| ç‰¹æ€§ | è¯´æ˜ |
|------|------|
| **æ‰¹é‡å‚æ•°ç”Ÿæˆ** | è‡ªåŠ¨ç”ŸæˆmÃ—Ï„Ã—ÎµÃ—lminå‚æ•°ç»„åˆç©ºé—´ |
| **åŒæ¨¡å¼åˆ†æ** | æ”¯æŒ1D-xå’Œ2D-xyä¸¤ç§åµŒå…¥æ¨¡å¼ |
| **å¹¶è¡Œè®¡ç®—** | CPUå¤šçº¿ç¨‹æ± å¤„ç†ï¼Œå¯é…ç½®çº¿ç¨‹æ•° |
| **å¢é‡ç¼“å­˜** | å·²å®Œæˆå‚æ•°ç»„åˆå¯å¤ç”¨ï¼Œæ”¯æŒæ–­ç‚¹ç»­ä¼  |
| **è¿›åº¦è¿½è¸ª** | å®æ—¶è¿›åº¦åé¦ˆï¼Œæ”¯æŒä»»åŠ¡å–æ¶ˆ |
| **ç»“æœå¯è§†åŒ–** | é€’å½’å›¾ã€æ—¶é—´åºåˆ—å›¾ã€ç»Ÿè®¡å›¾è¡¨ |

---

## 2. éœ€æ±‚åˆ†æ

### 2.1 åŠŸèƒ½éœ€æ±‚

#### 2.1.1 RQAå‚æ•°é…ç½®

**åµŒå…¥ç»´åº¦ (m - Embedding Dimension)**
- èŒƒå›´: 1-10
- æ­¥é•¿: 1
- é»˜è®¤: 2
- è¯´æ˜: ç›¸ç©ºé—´é‡æ„çš„ç»´åº¦

**æ—¶é—´å»¶è¿Ÿ (Ï„ - Time Delay)**
- èŒƒå›´: 1-10
- æ­¥é•¿: 1
- é»˜è®¤: 1
- è¯´æ˜: åµŒå…¥å‘é‡ä¹‹é—´çš„æ—¶é—´é—´éš”

**é€’å½’é˜ˆå€¼ (Îµ - Recurrence Threshold)**
- èŒƒå›´: 0.05-0.1
- æ­¥é•¿: 0.001
- é»˜è®¤: 0.05
- è¯´æ˜: åˆ¤å®šä¸¤ç‚¹ä¸ºé€’å½’ç‚¹çš„è·ç¦»é˜ˆå€¼

**æœ€å°çº¿é•¿ (lmin - Minimum Line Length)**
- èŒƒå›´: 2-3
- æ­¥é•¿: 1
- é»˜è®¤: 2
- è¯´æ˜: è®¡ç®—DETå’ŒENTæ—¶çš„æœ€å°å¯¹è§’çº¿é•¿åº¦

#### 2.1.2 æ‰¹é‡æ‰§è¡Œé…ç½®

```python
# é¢„è®¡ç”Ÿæˆçš„å‚æ•°ç»„åˆæ•°
total_combinations = (
    (m_end - m_start) / m_step + 1 *
    (tau_end - tau_start) / tau_step + 1 *
    (eps_end - eps_start) / eps_step + 1 *
    (lmin_end - lmin_start) / lmin_step + 1
)
# é»˜è®¤é…ç½®: 10 * 10 * 51 * 2 = 10,200 ç»„åˆ
```

#### 2.1.3 RQAæŒ‡æ ‡è®¡ç®—

**æ ¸å¿ƒæŒ‡æ ‡ (ä¸¤ç§æ¨¡å¼)**
- **RR (Recurrence Rate)**: é€’å½’ç‡
- **DET (Determinism)**: ç¡®å®šæ€§
- **ENT (Entropy)**: ç†µ

**åˆ†ææ¨¡å¼**
- **1D-x**: ä»…ä½¿ç”¨xåæ ‡çš„1Dæ—¶é—´åºåˆ—åˆ†æ
- **2D-xy**: ä½¿ç”¨(x,y)åæ ‡çš„2Dè½¨è¿¹åˆ†æ

æ¯ä¸ªå‚æ•°ç»„åˆäº§ç”Ÿ6ä¸ªæŒ‡æ ‡:
- RR-1D-x, DET-1D-x, ENT-1D-x
- RR-2D-xy, DET-2D-xy, ENT-2D-xy

### 2.2 éåŠŸèƒ½éœ€æ±‚

#### 2.2.1 æ€§èƒ½è¦æ±‚

| æŒ‡æ ‡ | è¦æ±‚ | å®ç°ç­–ç•¥ |
|------|------|----------|
| **å•æ–‡ä»¶å¤„ç†æ—¶é—´** | <2ç§’/å‚æ•°ç»„åˆ | CPUå¤šçº¿ç¨‹ |
| **æ‰¹é‡å¤„ç†ååé‡** | >50æ–‡ä»¶/åˆ†é’Ÿ | çº¿ç¨‹æ± å¹¶è¡Œ |
| **å†…å­˜å ç”¨** | <4GB (å³°å€¼) | æµå¼å¤„ç†ï¼ŒåŠæ—¶é‡Šæ”¾ |
| **CPUåˆ©ç”¨ç‡** | >80% (å¤šæ ¸) | åŠ¨æ€çº¿ç¨‹æ± è°ƒæ•´ |
| **ä»»åŠ¡å“åº”æ—¶é—´** | <500ms (å¯åŠ¨) | å¼‚æ­¥ä»»åŠ¡é˜Ÿåˆ— |

#### 2.2.2 å¯é æ€§è¦æ±‚

- **å®¹é”™èƒ½åŠ›**: å•æ–‡ä»¶å¤±è´¥ä¸å½±å“æ•´ä½“æµç¨‹
- **æ–­ç‚¹ç»­ä¼ **: å·²å®Œæˆçš„å‚æ•°ç»„åˆå¯è·³è¿‡
- **æ•°æ®å®Œæ•´æ€§**: æ‰€æœ‰ç»“æœå¸¦æ—¶é—´æˆ³å’Œæ ¡éªŒä¿¡æ¯
- **é”™è¯¯è¿½è¸ª**: è¯¦ç»†é”™è¯¯æ—¥å¿—ï¼Œæ”¯æŒè°ƒè¯•

#### 2.2.3 å¯ç”¨æ€§è¦æ±‚

- **è¿›åº¦å¯è§†åŒ–**: å®æ—¶æ˜¾ç¤ºå·²å®Œæˆ/æ€»æ•°
- **ä»»åŠ¡ç®¡ç†**: æ”¯æŒæš‚åœã€å–æ¶ˆã€é‡å¯
- **ç»“æœæŸ¥è¯¢**: æŒ‰å‚æ•°ã€æŒ‰å—è¯•è€…å¿«é€Ÿæ£€ç´¢
- **å¯¼å‡ºåŠŸèƒ½**: CSVã€Excelã€JSONå¤šæ ¼å¼å¯¼å‡º

---

## 3. ç³»ç»Ÿæ¶æ„è®¾è®¡

### 3.1 æ•´ä½“æ¶æ„

éµå¾ªModule04çš„å“è¶Šæ¶æ„æ¨¡å¼ï¼Œé‡‡ç”¨ä¸‰å±‚åˆ†å±‚æ¶æ„ï¼š

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    Frontend Layer                        â”‚
â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”‚
â”‚   â”‚  Module05.jsx (React Component)              â”‚     â”‚
â”‚   â”‚  - å‚æ•°é…ç½®ç•Œé¢                               â”‚     â”‚
â”‚   â”‚  - è¿›åº¦ç›‘æ§é¢æ¿                               â”‚     â”‚
â”‚   â”‚  - ç»“æœå¯è§†åŒ–                                 â”‚     â”‚
â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                           â†• HTTP/JSON
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                      API Layer                           â”‚
â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”‚
â”‚   â”‚  api.py (Flask Blueprint)                    â”‚     â”‚
â”‚   â”‚  - RESTful API endpoints                     â”‚     â”‚
â”‚   â”‚  - å‚æ•°éªŒè¯è£…é¥°å™¨                            â”‚     â”‚
â”‚   â”‚  - ç»Ÿä¸€é”™è¯¯å¤„ç†                              â”‚     â”‚
â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â”‚
â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”‚
â”‚   â”‚  utils.py (Decorators & Validators)          â”‚     â”‚
â”‚   â”‚  - @handle_api_errors                        â”‚     â”‚
â”‚   â”‚  - @validate_params                          â”‚     â”‚
â”‚   â”‚  - @monitor_performance                      â”‚     â”‚
â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                           â†•
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    Service Layer                         â”‚
â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”‚
â”‚   â”‚  service.py (RQAAnalysisService)             â”‚     â”‚
â”‚   â”‚  - å‚æ•°ç»„åˆç”Ÿæˆ                              â”‚     â”‚
â”‚   â”‚  - ä»»åŠ¡è°ƒåº¦ç®¡ç†                              â”‚     â”‚
â”‚   â”‚  - ç»“æœèšåˆ                                  â”‚     â”‚
â”‚   â”‚  - ç¼“å­˜ç®¡ç†                                  â”‚     â”‚
â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                           â†•
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                  Business Logic Layer                    â”‚
â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚   â”‚rqa_analyzer.pyâ”‚  â”‚task_executor.pyâ”‚  â”‚visualizer.pyâ”‚ â”‚
â”‚   â”‚- RQAæ ¸å¿ƒç®—æ³• â”‚  â”‚- å¤šçº¿ç¨‹è°ƒåº¦   â”‚  â”‚- å›¾è¡¨ç”Ÿæˆ   â”‚ â”‚
â”‚   â”‚- åµŒå…¥é‡æ„    â”‚  â”‚- è¿›åº¦è¿½è¸ª     â”‚  â”‚- é€’å½’å›¾     â”‚ â”‚
â”‚   â”‚- é€’å½’çŸ©é˜µ    â”‚  â”‚- ä»»åŠ¡é˜Ÿåˆ—     â”‚  â”‚- ç»Ÿè®¡å›¾     â”‚ â”‚
â”‚   â”‚- æŒ‡æ ‡è®¡ç®—    â”‚  â”‚- é”™è¯¯æ¢å¤     â”‚  â”‚              â”‚ â”‚
â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                           â†•
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                   Data Access Layer                      â”‚
â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”‚
â”‚   â”‚  data_loader.py                              â”‚     â”‚
â”‚   â”‚  - è¯»å–æ ¡å‡†åæ•°æ® (02_processed)             â”‚     â”‚
â”‚   â”‚  - ROIé…ç½®åŠ è½½                               â”‚     â”‚
â”‚   â”‚  - å—è¯•è€…ä¿¡æ¯                                â”‚     â”‚
â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                           â†•
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                   Data Persistence                       â”‚
â”‚   data/05_rqa_analysis/                                 â”‚
â”‚   â”œâ”€â”€ cache/                                            â”‚
â”‚   â”‚   â””â”€â”€ param_combinations.json                      â”‚
â”‚   â”œâ”€â”€ results/                                          â”‚
â”‚   â”‚   â”œâ”€â”€ m{m}_tau{tau}_eps{eps}_lmin{lmin}/          â”‚
â”‚   â”‚   â”‚   â”œâ”€â”€ step1_rqa_calculation/                   â”‚
â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ control_rqa_results.csv             â”‚
â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ mci_rqa_results.csv                 â”‚
â”‚   â”‚   â”‚   â”‚   â””â”€â”€ ad_rqa_results.csv                  â”‚
â”‚   â”‚   â”‚   â”œâ”€â”€ step2_data_merging/                      â”‚
â”‚   â”‚   â”‚   â”‚   â””â”€â”€ merged_data.csv                     â”‚
â”‚   â”‚   â”‚   â”œâ”€â”€ step3_feature_enrichment/                â”‚
â”‚   â”‚   â”‚   â”‚   â””â”€â”€ enriched_features.csv               â”‚
â”‚   â”‚   â”‚   â”œâ”€â”€ step4_statistical_analysis/              â”‚
â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ descriptive_stats.csv               â”‚
â”‚   â”‚   â”‚   â”‚   â””â”€â”€ group_comparison.csv                â”‚
â”‚   â”‚   â”‚   â”œâ”€â”€ step5_visualization/                     â”‚
â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ recurrence_plots/                   â”‚
â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ time_series_plots/                  â”‚
â”‚   â”‚   â”‚   â”‚   â””â”€â”€ statistical_plots/                  â”‚
â”‚   â”‚   â”‚   â””â”€â”€ metadata.json                           â”‚
â”‚   â””â”€â”€ exports/                                         â”‚
â”‚       â””â”€â”€ rqa_results_{timestamp}.xlsx                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### 3.2 æ ¸å¿ƒç»„ä»¶

#### 3.2.1 Serviceå±‚ - RQAAnalysisService

```python
class RQAAnalysisService:
    """RQAåˆ†ææœåŠ¡ - æ ¸å¿ƒè°ƒåº¦å™¨"""

    def __init__(self):
        self.data_root = Path(Config.DATA_ROOT)
        self.processed_dir = self.data_root / '02_processed'
        self.results_dir = self.data_root / '05_rqa_analysis' / 'results'
        self.cache_dir = self.data_root / '05_rqa_analysis' / 'cache'

        # ä»»åŠ¡æ‰§è¡Œå™¨ï¼ˆCPUå¤šçº¿ç¨‹ï¼‰
        self.task_executor = RQATaskExecutor(max_workers=cpu_count())

        # RQAåˆ†æå™¨
        self.rqa_analyzer = RQAAnalyzer()

        # è¿›åº¦è¿½è¸ª
        self.progress_tracker = ProgressTracker()

    def generate_param_combinations(self, param_ranges: Dict) -> List[Dict]:
        """ç”Ÿæˆå‚æ•°ç»„åˆç©ºé—´"""

    def analyze_batch(self, param_combinations: List[Dict],
                     groups: List[str]) -> Dict:
        """æ‰¹é‡RQAåˆ†æï¼ˆ5æ­¥æµç¨‹ï¼‰"""

    def get_analysis_status(self, param_signature: str) -> Dict:
        """è·å–åˆ†æçŠ¶æ€"""

    def cancel_analysis(self, task_id: str) -> bool:
        """å–æ¶ˆåˆ†æä»»åŠ¡"""
```

#### 3.2.2 ä¸šåŠ¡é€»è¾‘å±‚ - RQAAnalyzer

```python
class RQAAnalyzer:
    """RQAæ ¸å¿ƒç®—æ³•å®ç°"""

    def analyze_single_file(self, csv_path: str, params: Dict) -> Dict:
        """
        å•æ–‡ä»¶RQAåˆ†æ

        Args:
            csv_path: æ ¡å‡†åCSVæ–‡ä»¶è·¯å¾„
            params: {m, tau, eps, lmin}

        Returns:
            {
                'RR-1D-x': float,
                'DET-1D-x': float,
                'ENT-1D-x': float,
                'RR-2D-xy': float,
                'DET-2D-xy': float,
                'ENT-2D-xy': float,
                'metadata': {...}
            }
        """

    def embed_signal_1d(self, x: np.ndarray, m: int, tau: int) -> np.ndarray:
        """1Dä¿¡å·åµŒå…¥é‡æ„"""

    def embed_signal_2d(self, x: np.ndarray, y: np.ndarray,
                       m: int, tau: int) -> np.ndarray:
        """2Dä¿¡å·åµŒå…¥é‡æ„"""

    def compute_recurrence_matrix(self, embedded: np.ndarray,
                                  eps: float, metric: str) -> np.ndarray:
        """è®¡ç®—é€’å½’çŸ©é˜µ"""

    def compute_rqa_metrics(self, rp: np.ndarray, lmin: int) -> Dict:
        """è®¡ç®—RQAæŒ‡æ ‡ (RR, DET, ENT)"""

    def extract_diagonal_lengths(self, rp: np.ndarray) -> Dict[int, int]:
        """æå–å¯¹è§’çº¿é•¿åº¦åˆ†å¸ƒ"""
```

#### 3.2.3 ä»»åŠ¡æ‰§è¡Œå™¨ - RQATaskExecutor

```python
class RQATaskExecutor:
    """CPUå¤šçº¿ç¨‹ä»»åŠ¡æ‰§è¡Œå™¨"""

    def __init__(self, max_workers: int = None):
        self.max_workers = max_workers or cpu_count()
        self.executor = ThreadPoolExecutor(max_workers=self.max_workers)
        self.active_tasks = {}
        self.task_results = {}

    def submit_batch_task(self, task_id: str, files: List[str],
                         params: Dict, callback: Callable) -> Future:
        """æäº¤æ‰¹é‡åˆ†æä»»åŠ¡"""

    def get_task_progress(self, task_id: str) -> Dict:
        """è·å–ä»»åŠ¡è¿›åº¦"""

    def cancel_task(self, task_id: str) -> bool:
        """å–æ¶ˆä»»åŠ¡"""

    def _process_file_batch(self, files: List[str], params: Dict) -> List[Dict]:
        """æ‰¹é‡å¤„ç†æ–‡ä»¶ï¼ˆçº¿ç¨‹æ± å¹¶è¡Œï¼‰"""
```

---

## 4. æ ¸å¿ƒåŠŸèƒ½è®¾è®¡

### 4.1 äº”æ­¥RQAåˆ†ææµç¨‹

#### Step 1: RQAè®¡ç®— (RQA Calculation)

**è¾“å…¥**:
- æ ¡å‡†åçœ¼åŠ¨æ•°æ®: `data/02_processed/{group}/{subject_id}_{task_id}_calibrated.csv`
- RQAå‚æ•°: `{m, tau, eps, lmin}`

**å¤„ç†**:
```python
def step1_rqa_calculation(params: Dict, groups: List[str]) -> Dict:
    """
    å¯¹æ‰€æœ‰å—è¯•è€…çš„æ‰€æœ‰ä»»åŠ¡è¿›è¡ŒRQAè®¡ç®—

    æµç¨‹:
    1. æ‰«ææ•°æ®ç›®å½•ï¼Œè·å–æ‰€æœ‰CSVæ–‡ä»¶
    2. ç”Ÿæˆæ–‡ä»¶åˆ—è¡¨ (control, mci, ad)
    3. å¤šçº¿ç¨‹å¹¶è¡Œå¤„ç†æ¯ä¸ªæ–‡ä»¶
    4. ä¿å­˜ç»“æœåˆ°CSV

    è¾“å‡º:
    - control_rqa_results.csv
    - mci_rqa_results.csv
    - ad_rqa_results.csv

    CSVæ ¼å¼:
    | subject_id | task_id | RR-1D-x | DET-1D-x | ENT-1D-x | RR-2D-xy | DET-2D-xy | ENT-2D-xy |
    """
    results = {'control': [], 'mci': [], 'ad': []}

    for group in groups:
        csv_files = scan_calibrated_files(group)

        # å¤šçº¿ç¨‹å¤„ç†
        with ThreadPoolExecutor(max_workers=cpu_count()) as executor:
            futures = [
                executor.submit(rqa_analyzer.analyze_single_file, file, params)
                for file in csv_files
            ]

            for future in as_completed(futures):
                result = future.result()
                results[group].append(result)

        # ä¿å­˜ç»“æœ
        df = pd.DataFrame(results[group])
        output_path = get_step_directory(params, 'step1_rqa_calculation')
        df.to_csv(output_path / f'{group}_rqa_results.csv', index=False)

    return {
        'success': True,
        'total_files_processed': sum(len(r) for r in results.values()),
        'output_files': [...]
    }
```

#### Step 2: æ•°æ®åˆå¹¶ (Data Merging)

**è¾“å…¥**:
- Step1çš„ä¸‰ä¸ªCSVæ–‡ä»¶

**å¤„ç†**:
```python
def step2_data_merging(params: Dict) -> Dict:
    """
    åˆå¹¶ä¸‰ç»„æ•°æ®ï¼Œæ·»åŠ åˆ†ç»„æ ‡ç­¾

    è¾“å‡º:
    - merged_data.csv

    CSVæ ¼å¼:
    | ID | Group | subject_id | task_id | RR-1D-x | ... | ENT-2D-xy |
    """
    step1_dir = get_step_directory(params, 'step1_rqa_calculation')

    # è¯»å–ä¸‰ç»„æ•°æ®
    control = pd.read_csv(step1_dir / 'control_rqa_results.csv')
    mci = pd.read_csv(step1_dir / 'mci_rqa_results.csv')
    ad = pd.read_csv(step1_dir / 'ad_rqa_results.csv')

    # æ·»åŠ åˆ†ç»„æ ‡ç­¾
    control['Group'] = 'Control'
    mci['Group'] = 'MCI'
    ad['Group'] = 'AD'

    # ç”ŸæˆID
    control['ID'] = control['subject_id'] + '_' + control['task_id']
    mci['ID'] = mci['subject_id'] + '_' + mci['task_id']
    ad['ID'] = ad['subject_id'] + '_' + ad['task_id']

    # åˆå¹¶
    merged = pd.concat([control, mci, ad], ignore_index=True)

    # ä¿å­˜
    step2_dir = get_step_directory(params, 'step2_data_merging')
    merged.to_csv(step2_dir / 'merged_data.csv', index=False)

    return {'success': True, 'total_records': len(merged)}
```

#### Step 3: ç‰¹å¾å¢å¼º (Feature Enrichment)

**è¾“å…¥**:
- Step2çš„merged_data.csv
- Module04çš„äº‹ä»¶åˆ†æç»“æœ
- å—è¯•è€…MMSEä¿¡æ¯

**å¤„ç†**:
```python
def step3_feature_enrichment(params: Dict) -> Dict:
    """
    å¢å¼ºç‰¹å¾ï¼šåˆå¹¶äº‹ä»¶åˆ†ææ•°æ®å’Œå—è¯•è€…ä¿¡æ¯

    è¾“å‡º:
    - enriched_features.csv

    å¢åŠ çš„åˆ—:
    - fixation_count, saccade_count (æ¥è‡ªModule04)
    - avg_fixation_duration, avg_saccade_amplitude
    - roi_bg_ratio, roi_inst_ratio, roi_kw_ratio
    - mmse_total_score, mmse_orientation, mmse_memory, etc.
    - age, education_level
    """
    step2_dir = get_step_directory(params, 'step2_data_merging')
    merged = pd.read_csv(step2_dir / 'merged_data.csv')

    # 1. åˆå¹¶Module04äº‹ä»¶æ•°æ®
    event_features = load_event_features()  # ä»Module04ç¼“å­˜åŠ è½½
    enriched = merged.merge(event_features,
                           on=['subject_id', 'task_id'],
                           how='left')

    # 2. åˆå¹¶MMSEæ•°æ®
    mmse_data = load_mmse_data()  # ä»SubjectManageråŠ è½½
    enriched = enriched.merge(mmse_data,
                             on='subject_id',
                             how='left')

    # 3. è®¡ç®—è¡ç”Ÿç‰¹å¾
    enriched['rqa_complexity_1d'] = (
        enriched['DET-1D-x'] * enriched['ENT-1D-x']
    )
    enriched['rqa_complexity_2d'] = (
        enriched['DET-2D-xy'] * enriched['ENT-2D-xy']
    )

    # ä¿å­˜
    step3_dir = get_step_directory(params, 'step3_feature_enrichment')
    enriched.to_csv(step3_dir / 'enriched_features.csv', index=False)

    return {'success': True, 'features_count': len(enriched.columns)}
```

#### Step 4: ç»Ÿè®¡åˆ†æ (Statistical Analysis)

**è¾“å…¥**:
- Step3çš„enriched_features.csv

**å¤„ç†**:
```python
def step4_statistical_analysis(params: Dict) -> Dict:
    """
    ç»Ÿè®¡åˆ†æï¼šæè¿°æ€§ç»Ÿè®¡ + ç»„é—´æ¯”è¾ƒ

    è¾“å‡º:
    - descriptive_stats.csv (å„ç»„å‡å€¼ã€æ ‡å‡†å·®)
    - group_comparison.csv (ANOVA/t-testç»“æœ)
    - correlation_matrix.csv (ç‰¹å¾ç›¸å…³æ€§)
    """
    step3_dir = get_step_directory(params, 'step3_feature_enrichment')
    data = pd.read_csv(step3_dir / 'enriched_features.csv')

    # 1. æè¿°æ€§ç»Ÿè®¡
    numeric_cols = data.select_dtypes(include=[np.number]).columns
    desc_stats = data.groupby('Group')[numeric_cols].agg(['mean', 'std', 'min', 'max'])

    # 2. ç»„é—´æ¯”è¾ƒï¼ˆANOVAï¼‰
    from scipy.stats import f_oneway
    comparison_results = []

    for col in numeric_cols:
        control_vals = data[data['Group'] == 'Control'][col].dropna()
        mci_vals = data[data['Group'] == 'MCI'][col].dropna()
        ad_vals = data[data['Group'] == 'AD'][col].dropna()

        if len(control_vals) > 0 and len(mci_vals) > 0 and len(ad_vals) > 0:
            f_stat, p_value = f_oneway(control_vals, mci_vals, ad_vals)
            comparison_results.append({
                'feature': col,
                'f_statistic': f_stat,
                'p_value': p_value,
                'significant': p_value < 0.05
            })

    # 3. ç›¸å…³æ€§çŸ©é˜µ
    corr_matrix = data[numeric_cols].corr()

    # ä¿å­˜
    step4_dir = get_step_directory(params, 'step4_statistical_analysis')
    desc_stats.to_csv(step4_dir / 'descriptive_stats.csv')
    pd.DataFrame(comparison_results).to_csv(step4_dir / 'group_comparison.csv', index=False)
    corr_matrix.to_csv(step4_dir / 'correlation_matrix.csv')

    return {
        'success': True,
        'significant_features': sum(r['significant'] for r in comparison_results)
    }
```

#### Step 5: å¯è§†åŒ– (Visualization)

**è¾“å…¥**:
- å‰4æ­¥çš„æ‰€æœ‰æ•°æ®

**å¤„ç†**:
```python
def step5_visualization(params: Dict) -> Dict:
    """
    ç”Ÿæˆå¯è§†åŒ–å›¾è¡¨

    è¾“å‡º:
    - recurrence_plots/ (é€’å½’å›¾ï¼Œæ¯ä¸ªå—è¯•è€…-ä»»åŠ¡ä¸€å¼ )
    - time_series_plots/ (æ—¶é—´åºåˆ—å›¾)
    - statistical_plots/
      - group_comparison_boxplot.png
      - rqa_metrics_heatmap.png
      - correlation_matrix_heatmap.png
      - feature_distribution_violin.png
    """
    visualizer = RQAVisualizer()

    # 1. é€’å½’å›¾ï¼ˆæŠ½æ ·å±•ç¤ºï¼‰
    sample_files = select_representative_samples(data, n=20)
    for file in sample_files:
        rp = compute_recurrence_matrix(file, params)
        plot = visualizer.plot_recurrence_matrix(rp, file)
        save_plot(plot, 'recurrence_plots', f'{file.stem}.png')

    # 2. ç»Ÿè®¡å›¾è¡¨
    step4_dir = get_step_directory(params, 'step4_statistical_analysis')
    enriched_data = pd.read_csv(step3_dir / 'enriched_features.csv')

    # ç»„é—´æ¯”è¾ƒç®±çº¿å›¾
    visualizer.plot_group_comparison_boxplot(
        enriched_data,
        features=['RR-1D-x', 'DET-1D-x', 'ENT-1D-x']
    )

    # RQAæŒ‡æ ‡çƒ­åŠ›å›¾
    visualizer.plot_rqa_heatmap(enriched_data)

    # ç›¸å…³æ€§çŸ©é˜µ
    corr_matrix = pd.read_csv(step4_dir / 'correlation_matrix.csv', index_col=0)
    visualizer.plot_correlation_heatmap(corr_matrix)

    return {
        'success': True,
        'plots_generated': count_generated_plots()
    }
```

### 4.2 RQAæ ¸å¿ƒç®—æ³•

#### 4.2.1 ç›¸ç©ºé—´é‡æ„ (Phase Space Reconstruction)

**1DåµŒå…¥**:
```python
def embed_signal_1d(x: np.ndarray, m: int, tau: int) -> np.ndarray:
    """
    1Dæ—¶é—´å»¶è¿ŸåµŒå…¥

    Args:
        x: 1Dä¿¡å·ï¼Œshape=(N,)
        m: åµŒå…¥ç»´åº¦
        tau: æ—¶é—´å»¶è¿Ÿ

    Returns:
        åµŒå…¥çŸ©é˜µï¼Œshape=(N-(m-1)*tau, m)

    Example:
        x = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]
        m = 3, tau = 2

        embedded = [
            [0, 2, 4],  # i=0: x[0], x[2], x[4]
            [1, 3, 5],  # i=1: x[1], x[3], x[5]
            [2, 4, 6],  # i=2: x[2], x[4], x[6]
            [3, 5, 7],  # i=3: x[3], x[5], x[7]
            [4, 6, 8],  # i=4: x[4], x[6], x[8]
            [5, 7, 9],  # i=5: x[5], x[7], x[9]
        ]
    """
    N = len(x)
    rows = N - (m - 1) * tau

    if rows <= 0:
        raise ValueError(f"ä¿¡å·å¤ªçŸ­ï¼šN={N}, m={m}, tau={tau}")

    embedded = np.zeros((rows, m))
    for i in range(rows):
        for j in range(m):
            embedded[i, j] = x[i + j * tau]

    return embedded
```

**2DåµŒå…¥**:
```python
def embed_signal_2d(x: np.ndarray, y: np.ndarray,
                   m: int, tau: int) -> np.ndarray:
    """
    2Dæ—¶é—´å»¶è¿ŸåµŒå…¥

    Args:
        x, y: 2Dè½¨è¿¹ï¼Œshape=(N,)
        m: åµŒå…¥ç»´åº¦
        tau: æ—¶é—´å»¶è¿Ÿ

    Returns:
        åµŒå…¥çŸ©é˜µï¼Œshape=(N-(m-1)*tau, 2*m)

    Example:
        x = [0, 1, 2, 3, 4]
        y = [5, 6, 7, 8, 9]
        m = 2, tau = 1

        embedded = [
            [0, 5, 1, 6],  # i=0: (x[0],y[0]), (x[1],y[1])
            [1, 6, 2, 7],  # i=1: (x[1],y[1]), (x[2],y[2])
            [2, 7, 3, 8],  # i=2: (x[2],y[2]), (x[3],y[3])
            [3, 8, 4, 9],  # i=3: (x[3],y[3]), (x[4],y[4])
        ]
    """
    N = len(x)
    if len(y) != N:
        raise ValueError("xå’Œyé•¿åº¦å¿…é¡»ç›¸åŒ")

    rows = N - (m - 1) * tau
    if rows <= 0:
        raise ValueError(f"ä¿¡å·å¤ªçŸ­ï¼šN={N}, m={m}, tau={tau}")

    embedded = np.zeros((rows, 2 * m))
    for i in range(rows):
        for j in range(m):
            embedded[i, 2*j] = x[i + j * tau]
            embedded[i, 2*j + 1] = y[i + j * tau]

    return embedded
```

#### 4.2.2 é€’å½’çŸ©é˜µè®¡ç®— (Recurrence Matrix)

```python
def compute_recurrence_matrix(embedded: np.ndarray,
                             eps: float,
                             metric: str = '1d_abs') -> np.ndarray:
    """
    è®¡ç®—é€’å½’çŸ©é˜µ

    Args:
        embedded: åµŒå…¥çŸ©é˜µï¼Œshape=(M, d)
        eps: é€’å½’é˜ˆå€¼
        metric: è·ç¦»åº¦é‡ ('1d_abs' æˆ– 'euclidean')

    Returns:
        é€’å½’çŸ©é˜µ RPï¼Œshape=(M, M)ï¼Œå€¼ä¸º0æˆ–1

    Definition:
        RP[i,j] = 1 if dist(embedded[i], embedded[j]) <= eps
                  0 otherwise
    """
    M = embedded.shape[0]
    RP = np.zeros((M, M), dtype=np.int8)

    if metric == '1d_abs':
        # 1Dï¼šç»å¯¹å·®ä¹‹å’Œ
        for i in range(M):
            for j in range(M):
                dist = np.sum(np.abs(embedded[i] - embedded[j]))
                if dist <= eps:
                    RP[i, j] = 1

    elif metric == 'euclidean':
        # æ¬§å‡ é‡Œå¾—è·ç¦»
        for i in range(M):
            for j in range(M):
                dist = np.sqrt(np.sum((embedded[i] - embedded[j])**2))
                if dist <= eps:
                    RP[i, j] = 1

    return RP
```

**ä¼˜åŒ–ç‰ˆæœ¬ï¼ˆå‘é‡åŒ–ï¼‰**:
```python
def compute_recurrence_matrix_vectorized(embedded: np.ndarray,
                                        eps: float,
                                        metric: str = 'euclidean') -> np.ndarray:
    """
    å‘é‡åŒ–ç‰ˆæœ¬ï¼ˆæ›´å¿«ï¼‰

    ä½¿ç”¨scipy.spatial.distance.pdist + squareform
    """
    from scipy.spatial.distance import pdist, squareform

    if metric == '1d_abs':
        # æ›¼å“ˆé¡¿è·ç¦»ï¼ˆL1èŒƒæ•°ï¼‰
        dist_matrix = squareform(pdist(embedded, metric='cityblock'))
    else:
        # æ¬§å‡ é‡Œå¾—è·ç¦»ï¼ˆL2èŒƒæ•°ï¼‰
        dist_matrix = squareform(pdist(embedded, metric='euclidean'))

    RP = (dist_matrix <= eps).astype(np.int8)
    return RP
```

#### 4.2.3 RQAæŒ‡æ ‡è®¡ç®—

**å¯¹è§’çº¿æå–**:
```python
def extract_diagonal_lengths(RP: np.ndarray) -> Dict[int, int]:
    """
    æå–å¯¹è§’çº¿ä¸­è¿ç»­1çš„é•¿åº¦åˆ†å¸ƒ

    Args:
        RP: é€’å½’çŸ©é˜µï¼Œshape=(M, M)

    Returns:
        {é•¿åº¦: å‡ºç°æ¬¡æ•°}

    Example:
        RP = [
            [1, 1, 0, 0],
            [1, 1, 1, 0],
            [0, 1, 1, 1],
            [0, 0, 1, 1]
        ]

        å¯¹è§’çº¿d=-3: [0]           -> æ— è¿ç»­æ®µ
        å¯¹è§’çº¿d=-2: [0, 0]         -> æ— è¿ç»­æ®µ
        å¯¹è§’çº¿d=-1: [1, 1, 1]      -> ä¸€æ®µé•¿åº¦3
        å¯¹è§’çº¿d=0:  [1, 1, 1, 1]   -> ä¸€æ®µé•¿åº¦4
        å¯¹è§’çº¿d=1:  [1, 1, 1]      -> ä¸€æ®µé•¿åº¦3
        å¯¹è§’çº¿d=2:  [0, 0]         -> æ— è¿ç»­æ®µ
        å¯¹è§’çº¿d=3:  [0]            -> æ— è¿ç»­æ®µ

        ç»“æœ: {3: 2, 4: 1}  # é•¿åº¦3å‡ºç°2æ¬¡ï¼Œé•¿åº¦4å‡ºç°1æ¬¡
    """
    M = RP.shape[0]
    length_counts = {}

    # éå†æ‰€æœ‰å¯¹è§’çº¿
    for d in range(-(M-1), M):
        diagonal = []
        for i in range(M):
            j = i + d
            if 0 <= j < M:
                diagonal.append(RP[i, j])

        # æå–è¿ç»­1çš„é•¿åº¦
        idx = 0
        while idx < len(diagonal):
            if diagonal[idx] == 1:
                length = 1
                idx += 1
                while idx < len(diagonal) and diagonal[idx] == 1:
                    length += 1
                    idx += 1
                length_counts[length] = length_counts.get(length, 0) + 1
            else:
                idx += 1

    return length_counts
```

**RQAæŒ‡æ ‡**:
```python
def compute_rqa_metrics(RP: np.ndarray, lmin: int = 2) -> Dict[str, float]:
    """
    è®¡ç®—RQAæŒ‡æ ‡

    Args:
        RP: é€’å½’çŸ©é˜µ
        lmin: æœ€å°çº¿é•¿

    Returns:
        {
            'RR': float,   # Recurrence Rate
            'DET': float,  # Determinism
            'ENT': float   # Entropy
        }
    """
    M = RP.shape[0]

    # 1. RR: é€’å½’ç‡ = é€’å½’ç‚¹æ•°é‡ / æ€»ç‚¹æ•°
    total_points = M * M
    recurrence_points = np.sum(RP)
    RR = recurrence_points / total_points

    # 2. æå–å¯¹è§’çº¿é•¿åº¦åˆ†å¸ƒ
    length_counts = extract_diagonal_lengths(RP)

    # è®¡ç®—æ€»é•¿åº¦
    total_length = sum(length * count for length, count in length_counts.items())

    # 3. DET: ç¡®å®šæ€§ = (é•¿åº¦>=lminçš„å¯¹è§’çº¿é•¿åº¦ä¹‹å’Œ) / (æ‰€æœ‰å¯¹è§’çº¿é•¿åº¦ä¹‹å’Œ)
    det_length = sum(
        length * count
        for length, count in length_counts.items()
        if length >= lmin
    )
    DET = det_length / total_length if total_length > 0 else 0.0

    # 4. ENT: ç†µ = -Î£ p(l) * log2(p(l))  (å¯¹äºl >= lmin)
    # p(l) = é•¿åº¦ä¸ºlçš„å¯¹è§’çº¿æ®µæ•° / é•¿åº¦>=lminçš„å¯¹è§’çº¿æ®µæ€»æ•°
    total_lines_lmin = sum(
        count
        for length, count in length_counts.items()
        if length >= lmin
    )

    ENT = 0.0
    if total_lines_lmin > 0:
        for length, count in length_counts.items():
            if length >= lmin:
                p_l = count / total_lines_lmin
                if p_l > 1e-12:  # é¿å…log(0)
                    ENT += -p_l * np.log2(p_l)

    return {
        'RR': RR,
        'DET': DET,
        'ENT': ENT
    }
```

---

## 5. æ•°æ®æµè®¾è®¡

### 5.1 æ•°æ®æµè½¬å›¾

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  ç”¨æˆ·è¾“å…¥å‚æ•°                                               â”‚
â”‚  - m: [1, 10, 1]                                           â”‚
â”‚  - tau: [1, 10, 1]                                         â”‚
â”‚  - eps: [0.05, 0.1, 0.001]                                 â”‚
â”‚  - lmin: [2, 3, 1]                                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                        â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  å‚æ•°ç»„åˆç”Ÿæˆå™¨                                             â”‚
â”‚  - ç”Ÿæˆ10,200ä¸ªå‚æ•°ç»„åˆ                                     â”‚
â”‚  - æ£€æŸ¥ç¼“å­˜ï¼Œè·³è¿‡å·²å®Œæˆçš„ç»„åˆ                               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                        â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  ä»»åŠ¡è°ƒåº¦å™¨                                                 â”‚
â”‚  - åˆ›å»ºä»»åŠ¡é˜Ÿåˆ—                                             â”‚
â”‚  - åˆ†é…ç»™çº¿ç¨‹æ±                                              â”‚
â”‚  - ç›‘æ§è¿›åº¦                                                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                        â†“
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â†“                               â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ çº¿ç¨‹1        â”‚  ...         â”‚ çº¿ç¨‹N        â”‚
â”‚ - å‚æ•°ç»„åˆ1  â”‚              â”‚ - å‚æ•°ç»„åˆN  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
        â†“                               â†“
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                        â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  å•ä¸ªå‚æ•°ç»„åˆå¤„ç†                                           â”‚
â”‚  For params in param_combinations:                         â”‚
â”‚    1. åŠ è½½æ ¡å‡†åæ•°æ®                                        â”‚
â”‚    2. 1DåµŒå…¥ + é€’å½’çŸ©é˜µ + RQAæŒ‡æ ‡                          â”‚
â”‚    3. 2DåµŒå…¥ + é€’å½’çŸ©é˜µ + RQAæŒ‡æ ‡                          â”‚
â”‚    4. ä¿å­˜ç»“æœ                                              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                        â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  ç»“æœèšåˆ                                                   â”‚
â”‚  - æ”¶é›†æ‰€æœ‰çº¿ç¨‹ç»“æœ                                         â”‚
â”‚  - ç”ŸæˆCSVæ–‡ä»¶                                              â”‚
â”‚  - æ›´æ–°è¿›åº¦                                                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                        â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  5æ­¥æµç¨‹æ‰§è¡Œ                                                â”‚
â”‚  Step 1: RQAè®¡ç®— â†’ CSV                                     â”‚
â”‚  Step 2: æ•°æ®åˆå¹¶ â†’ CSV                                    â”‚
â”‚  Step 3: ç‰¹å¾å¢å¼º â†’ CSV                                    â”‚
â”‚  Step 4: ç»Ÿè®¡åˆ†æ â†’ CSV + å›¾è¡¨                             â”‚
â”‚  Step 5: å¯è§†åŒ– â†’ å›¾ç‰‡                                     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                        â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  ç»“æœè¿”å›                                                   â”‚
â”‚  - ä»»åŠ¡ID                                                  â”‚
â”‚  - å®Œæˆè¿›åº¦                                                 â”‚
â”‚  - ç»“æœè·¯å¾„                                                 â”‚
â”‚  - é¢„è§ˆå›¾è¡¨                                                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### 5.2 æ•°æ®ç»“æ„å®šä¹‰

#### 5.2.1 å‚æ•°ç»„åˆ

```python
@dataclass
class RQAParams:
    """RQAå‚æ•°"""
    m: int              # åµŒå…¥ç»´åº¦
    tau: int            # æ—¶é—´å»¶è¿Ÿ
    eps: float          # é€’å½’é˜ˆå€¼
    lmin: int           # æœ€å°çº¿é•¿

    def to_dict(self) -> Dict:
        return {
            'm': self.m,
            'tau': self.tau,
            'eps': self.eps,
            'lmin': self.lmin
        }

    def signature(self) -> str:
        """ç”Ÿæˆå”¯ä¸€ç­¾å"""
        return f"m{self.m}_tau{self.tau}_eps{self.eps}_lmin{self.lmin}"
```

#### 5.2.2 åˆ†æç»“æœ

```python
@dataclass
class RQAResult:
    """å•æ–‡ä»¶RQAç»“æœ"""
    subject_id: str
    task_id: str
    group: str

    # 1DæŒ‡æ ‡
    rr_1d: float
    det_1d: float
    ent_1d: float

    # 2DæŒ‡æ ‡
    rr_2d: float
    det_2d: float
    ent_2d: float

    # å…ƒæ•°æ®
    params: RQAParams
    processing_time: float
    timestamp: str

    def to_dict(self) -> Dict:
        return {
            'subject_id': self.subject_id,
            'task_id': self.task_id,
            'group': self.group,
            'RR-1D-x': self.rr_1d,
            'DET-1D-x': self.det_1d,
            'ENT-1D-x': self.ent_1d,
            'RR-2D-xy': self.rr_2d,
            'DET-2D-xy': self.det_2d,
            'ENT-2D-xy': self.ent_2d,
            'processing_time': self.processing_time,
            'timestamp': self.timestamp
        }
```

#### 5.2.3 ä»»åŠ¡çŠ¶æ€

```python
@dataclass
class TaskStatus:
    """æ‰¹é‡ä»»åŠ¡çŠ¶æ€"""
    task_id: str
    params: RQAParams
    total_files: int
    processed_files: int
    failed_files: int
    current_step: int  # 1-5
    status: str  # 'pending', 'running', 'completed', 'failed', 'cancelled'
    start_time: datetime
    end_time: Optional[datetime]
    error_message: Optional[str]

    @property
    def progress(self) -> float:
        """è¿›åº¦ç™¾åˆ†æ¯”"""
        if self.total_files == 0:
            return 0.0
        return (self.processed_files / self.total_files) * 100

    @property
    def eta(self) -> Optional[timedelta]:
        """é¢„è®¡å‰©ä½™æ—¶é—´"""
        if self.processed_files == 0:
            return None

        elapsed = (datetime.now() - self.start_time).total_seconds()
        avg_time_per_file = elapsed / self.processed_files
        remaining_files = self.total_files - self.processed_files

        return timedelta(seconds=avg_time_per_file * remaining_files)
```

---

## 6. APIæ¥å£è®¾è®¡

### 6.1 APIç«¯ç‚¹åˆ—è¡¨

| ç«¯ç‚¹ | æ–¹æ³• | åŠŸèƒ½ | è£…é¥°å™¨ |
|------|------|------|--------|
| `/api/m05/health` | GET | å¥åº·æ£€æŸ¥ | - |
| `/api/m05/params/generate` | POST | ç”Ÿæˆå‚æ•°ç»„åˆ | `@validate_params`, `@handle_api_errors` |
| `/api/m05/params/history` | GET | è·å–å‚æ•°å†å² | `@handle_api_errors` |
| `/api/m05/analyze/batch` | POST | æ‰¹é‡RQAåˆ†æ | `@validate_params`, `@handle_api_errors` |
| `/api/m05/analyze/status` | GET | æŸ¥è¯¢ä»»åŠ¡çŠ¶æ€ | `@validate_params`, `@handle_api_errors` |
| `/api/m05/analyze/cancel` | POST | å–æ¶ˆä»»åŠ¡ | `@validate_params`, `@handle_api_errors` |
| `/api/m05/results/list` | GET | åˆ—å‡ºç»“æœ | `@handle_api_errors` |
| `/api/m05/results/download` | GET | ä¸‹è½½ç»“æœ | `@validate_params`, `@handle_api_errors` |
| `/api/m05/results/preview` | GET | é¢„è§ˆç»“æœ | `@validate_params`, `@handle_api_errors` |
| `/api/m05/visualize/recurrence-plot` | POST | ç”Ÿæˆé€’å½’å›¾ | `@validate_params`, `@handle_api_errors` |
| `/api/m05/visualize/statistics` | GET | ç»Ÿè®¡å›¾è¡¨ | `@validate_params`, `@handle_api_errors` |

### 6.2 è¯¦ç»†æ¥å£è§„èŒƒ

#### 6.2.1 ç”Ÿæˆå‚æ•°ç»„åˆ

```python
@m05_bp.route('/params/generate', methods=['POST'])
@validate_params('m_range', 'tau_range', 'eps_range', 'lmin_range')
@handle_api_errors
def generate_param_combinations():
    """
    ç”Ÿæˆå‚æ•°ç»„åˆç©ºé—´

    Request Body:
    {
        "m_range": {
            "start": 1,
            "end": 10,
            "step": 1
        },
        "tau_range": {
            "start": 1,
            "end": 10,
            "step": 1
        },
        "eps_range": {
            "start": 0.05,
            "end": 0.1,
            "step": 0.001
        },
        "lmin_range": {
            "start": 2,
            "end": 3,
            "step": 1
        }
    }

    Response:
    {
        "success": true,
        "total_combinations": 10200,
        "combinations": [
            {"m": 1, "tau": 1, "eps": 0.05, "lmin": 2},
            {"m": 1, "tau": 1, "eps": 0.051, "lmin": 2},
            ...
        ],
        "estimated_time_minutes": 340
    }
    """
    data = request.get_json()

    service = get_service()
    result = service.generate_param_combinations(
        m_range=data['m_range'],
        tau_range=data['tau_range'],
        eps_range=data['eps_range'],
        lmin_range=data['lmin_range']
    )

    return jsonify(result)
```

#### 6.2.2 æ‰¹é‡RQAåˆ†æ

```python
@m05_bp.route('/analyze/batch', methods=['POST'])
@validate_params('param_combinations', 'groups')
@handle_api_errors
@monitor_performance  # æ–°å¢æ€§èƒ½ç›‘æ§è£…é¥°å™¨
def analyze_batch():
    """
    æ‰¹é‡RQAåˆ†æï¼ˆå¼‚æ­¥ï¼‰

    Request Body:
    {
        "param_combinations": [
            {"m": 2, "tau": 1, "eps": 0.05, "lmin": 2},
            ...
        ],
        "groups": ["control", "mci", "ad"],
        "execute_steps": [1, 2, 3, 4, 5],  // å¯é€‰ï¼šæŒ‡å®šæ‰§è¡Œå“ªäº›æ­¥éª¤
        "max_workers": 8  // å¯é€‰ï¼šæœ€å¤§çº¿ç¨‹æ•°
    }

    Response:
    {
        "success": true,
        "task_id": "rqa_20251007_153045_abc123",
        "total_combinations": 10200,
        "estimated_time_minutes": 340,
        "message": "ä»»åŠ¡å·²å¯åŠ¨ï¼Œè¯·ä½¿ç”¨task_idæŸ¥è¯¢è¿›åº¦"
    }
    """
    data = request.get_json()

    service = get_service()
    result = service.analyze_batch(
        param_combinations=data['param_combinations'],
        groups=data['groups'],
        execute_steps=data.get('execute_steps', [1, 2, 3, 4, 5]),
        max_workers=data.get('max_workers')
    )

    return jsonify(result)
```

#### 6.2.3 æŸ¥è¯¢ä»»åŠ¡çŠ¶æ€

```python
@m05_bp.route('/analyze/status', methods=['GET'])
@validate_params('task_id')
@handle_api_errors
def get_task_status():
    """
    æŸ¥è¯¢ä»»åŠ¡çŠ¶æ€

    Query Parameters:
        task_id: ä»»åŠ¡ID

    Response:
    {
        "success": true,
        "task_id": "rqa_20251007_153045_abc123",
        "status": "running",  // pending, running, completed, failed, cancelled
        "progress": 45.2,  // ç™¾åˆ†æ¯”
        "current_step": 1,
        "total_files": 500,
        "processed_files": 226,
        "failed_files": 3,
        "start_time": "2025-10-07T15:30:45",
        "elapsed_time_seconds": 1234,
        "eta_seconds": 1500,
        "current_param": {"m": 2, "tau": 3, "eps": 0.056, "lmin": 2},
        "errors": [
            {"file": "control_legacy_10_q4_calibrated.csv", "error": "..."}
        ]
    }
    """
    task_id = request.args.get('task_id')

    service = get_service()
    status = service.get_task_status(task_id)

    return jsonify(status)
```

#### 6.2.4 ç”Ÿæˆé€’å½’å›¾

```python
@m05_bp.route('/visualize/recurrence-plot', methods=['POST'])
@validate_params('subject_id', 'task_id', 'params')
@handle_api_errors
def generate_recurrence_plot():
    """
    ç”Ÿæˆå•ä¸ªå—è¯•è€…-ä»»åŠ¡çš„é€’å½’å›¾

    Request Body:
    {
        "subject_id": "control_legacy_1",
        "task_id": "q1",
        "params": {"m": 2, "tau": 1, "eps": 0.05, "lmin": 2},
        "mode": "1d"  // '1d' or '2d'
    }

    Response:
    {
        "success": true,
        "plot_base64": "iVBORw0KGgoAAAANSUh...",  // PNGå›¾ç‰‡çš„base64ç¼–ç 
        "metrics": {
            "RR": 0.234,
            "DET": 0.876,
            "ENT": 2.345
        },
        "embedding_info": {
            "original_length": 500,
            "embedded_length": 498,
            "embedding_dim": 2
        }
    }
    """
    data = request.get_json()

    service = get_service()
    result = service.generate_recurrence_plot(
        subject_id=data['subject_id'],
        task_id=data['task_id'],
        params=data['params'],
        mode=data.get('mode', '1d')
    )

    return jsonify(result)
```

---

## 7. å¹¶è¡Œå¤„ç†è®¾è®¡

### 7.1 å¤šçº¿ç¨‹æ¶æ„

```python
class RQATaskExecutor:
    """CPUå¤šçº¿ç¨‹ä»»åŠ¡æ‰§è¡Œå™¨"""

    def __init__(self, max_workers: int = None):
        """
        åˆå§‹åŒ–æ‰§è¡Œå™¨

        Args:
            max_workers: æœ€å¤§çº¿ç¨‹æ•°ï¼Œé»˜è®¤ä¸ºCPUæ ¸å¿ƒæ•°
        """
        from multiprocessing import cpu_count

        self.max_workers = max_workers or cpu_count()
        self.executor = ThreadPoolExecutor(max_workers=self.max_workers)

        # ä»»åŠ¡ç®¡ç†
        self.active_tasks = {}  # {task_id: Future}
        self.task_status = {}   # {task_id: TaskStatus}

        # çº¿ç¨‹å®‰å…¨çš„è¿›åº¦è®¡æ•°å™¨
        self.progress_lock = threading.Lock()

        logger.info(f"RQATaskExecutor initialized: max_workers={self.max_workers}")

    def submit_batch_task(self, task_id: str,
                         param_combinations: List[Dict],
                         groups: List[str],
                         callback: Callable = None) -> str:
        """
        æäº¤æ‰¹é‡ä»»åŠ¡

        Args:
            task_id: ä»»åŠ¡ID
            param_combinations: å‚æ•°ç»„åˆåˆ—è¡¨
            groups: åˆ†ç»„åˆ—è¡¨
            callback: å®Œæˆå›è°ƒå‡½æ•°

        Returns:
            task_id
        """
        # åˆå§‹åŒ–ä»»åŠ¡çŠ¶æ€
        total_files = self._count_total_files(groups)
        self.task_status[task_id] = TaskStatus(
            task_id=task_id,
            params=param_combinations[0],  # ç¬¬ä¸€ä¸ªå‚æ•°ç»„åˆ
            total_files=total_files * len(param_combinations),
            processed_files=0,
            failed_files=0,
            current_step=1,
            status='pending',
            start_time=datetime.now(),
            end_time=None,
            error_message=None
        )

        # æäº¤åˆ°çº¿ç¨‹æ± 
        future = self.executor.submit(
            self._execute_batch_task,
            task_id,
            param_combinations,
            groups
        )

        # æ·»åŠ å®Œæˆå›è°ƒ
        if callback:
            future.add_done_callback(callback)

        self.active_tasks[task_id] = future
        self.task_status[task_id].status = 'running'

        return task_id

    def _execute_batch_task(self, task_id: str,
                           param_combinations: List[Dict],
                           groups: List[str]):
        """
        æ‰§è¡Œæ‰¹é‡ä»»åŠ¡ï¼ˆåœ¨çº¿ç¨‹ä¸­è¿è¡Œï¼‰

        æµç¨‹:
        1. éå†æ‰€æœ‰å‚æ•°ç»„åˆ
        2. å¯¹æ¯ä¸ªç»„åˆï¼Œå¹¶è¡Œå¤„ç†æ‰€æœ‰æ–‡ä»¶
        3. æ›´æ–°è¿›åº¦
        4. æ‰§è¡Œ5æ­¥æµç¨‹
        """
        try:
            for param_idx, params in enumerate(param_combinations):
                logger.info(f"Task {task_id}: Processing param {param_idx+1}/{len(param_combinations)}")

                # æ›´æ–°å½“å‰å‚æ•°
                self.task_status[task_id].current_param = params

                # Step 1: RQAè®¡ç®—ï¼ˆå¹¶è¡Œå¤„ç†æ–‡ä»¶ï¼‰
                self._execute_step1_parallel(task_id, params, groups)

                # Step 2-5: æ•°æ®å¤„ç†ï¼ˆä¸²è¡Œï¼‰
                self._execute_step2(task_id, params)
                self._execute_step3(task_id, params)
                self._execute_step4(task_id, params)
                self._execute_step5(task_id, params)

            # æ ‡è®°å®Œæˆ
            self.task_status[task_id].status = 'completed'
            self.task_status[task_id].end_time = datetime.now()

        except Exception as e:
            logger.error(f"Task {task_id} failed: {e}", exc_info=True)
            self.task_status[task_id].status = 'failed'
            self.task_status[task_id].error_message = str(e)
            self.task_status[task_id].end_time = datetime.now()

    def _execute_step1_parallel(self, task_id: str, params: Dict, groups: List[str]):
        """
        Step 1: å¹¶è¡ŒRQAè®¡ç®—

        ç­–ç•¥:
        - å°†æ‰€æœ‰CSVæ–‡ä»¶åˆ†æˆbatch
        - æ¯ä¸ªbatchæäº¤ç»™çº¿ç¨‹æ± 
        - ç­‰å¾…æ‰€æœ‰batchå®Œæˆ
        """
        from concurrent.futures import as_completed

        results = {group: [] for group in groups}

        for group in groups:
            # æ‰«ææ–‡ä»¶
            csv_files = self._scan_calibrated_files(group)
            logger.info(f"Task {task_id}: Found {len(csv_files)} files for group {group}")

            # æ‰¹é‡æäº¤åˆ°çº¿ç¨‹æ± 
            batch_size = 10  # æ¯æ¬¡æäº¤10ä¸ªæ–‡ä»¶
            futures = []

            for i in range(0, len(csv_files), batch_size):
                batch = csv_files[i:i+batch_size]
                future = self.executor.submit(
                    self._process_file_batch,
                    task_id,
                    batch,
                    params
                )
                futures.append(future)

            # æ”¶é›†ç»“æœ
            for future in as_completed(futures):
                try:
                    batch_results = future.result()
                    results[group].extend(batch_results)

                    # æ›´æ–°è¿›åº¦
                    with self.progress_lock:
                        self.task_status[task_id].processed_files += len(batch_results)

                except Exception as e:
                    logger.error(f"Batch processing failed: {e}")
                    with self.progress_lock:
                        self.task_status[task_id].failed_files += batch_size

            # ä¿å­˜è¯¥ç»„ç»“æœ
            df = pd.DataFrame(results[group])
            output_dir = get_step_directory(params, 'step1_rqa_calculation')
            df.to_csv(output_dir / f'{group}_rqa_results.csv', index=False)
            logger.info(f"Task {task_id}: Saved {len(results[group])} results for group {group}")

    def _process_file_batch(self, task_id: str,
                           files: List[Path],
                           params: Dict) -> List[Dict]:
        """
        å¤„ç†ä¸€æ‰¹æ–‡ä»¶ï¼ˆåœ¨å•ä¸ªçº¿ç¨‹ä¸­ï¼‰

        Args:
            task_id: ä»»åŠ¡ID
            files: æ–‡ä»¶è·¯å¾„åˆ—è¡¨
            params: RQAå‚æ•°

        Returns:
            ç»“æœåˆ—è¡¨
        """
        from .rqa_analyzer import RQAAnalyzer

        analyzer = RQAAnalyzer()
        results = []

        for file_path in files:
            try:
                result = analyzer.analyze_single_file(str(file_path), params)
                results.append(result)

            except Exception as e:
                logger.error(f"File {file_path} analysis failed: {e}")
                # ç»§ç»­å¤„ç†ä¸‹ä¸€ä¸ªæ–‡ä»¶ï¼Œä¸ä¸­æ–­æ•´ä¸ªbatch

        return results

    def cancel_task(self, task_id: str) -> bool:
        """
        å–æ¶ˆä»»åŠ¡

        æ³¨æ„: å·²ç»åœ¨è¿è¡Œçš„çº¿ç¨‹æ— æ³•ç«‹å³å–æ¶ˆï¼Œ
        ä½†ä¼šè®¾ç½®æ ‡å¿—ä½ï¼Œçº¿ç¨‹ä¼šåœ¨ä¸‹ä¸€ä¸ªæ£€æŸ¥ç‚¹é€€å‡º
        """
        if task_id in self.active_tasks:
            future = self.active_tasks[task_id]
            cancelled = future.cancel()

            if cancelled or self.task_status[task_id].status == 'running':
                self.task_status[task_id].status = 'cancelled'
                self.task_status[task_id].end_time = datetime.now()
                return True

        return False
```

### 7.2 æ€§èƒ½ä¼˜åŒ–ç­–ç•¥

#### 7.2.1 æ‰¹å¤„ç†ç­–ç•¥

```python
# ç­–ç•¥1: åŠ¨æ€batchå¤§å°
def calculate_optimal_batch_size(file_count: int,
                                 max_workers: int) -> int:
    """
    æ ¹æ®æ–‡ä»¶æ•°é‡å’Œçº¿ç¨‹æ•°è®¡ç®—æœ€ä¼˜batchå¤§å°

    åŸåˆ™:
    - batchå¤ªå°: æäº¤å¼€é”€å¤§
    - batchå¤ªå¤§: è´Ÿè½½ä¸å‡è¡¡
    - æœ€ä¼˜: batch_count = 2-4 * max_workers
    """
    target_batches = max_workers * 3
    batch_size = max(1, file_count // target_batches)
    return batch_size

# ç­–ç•¥2: ä¼˜å…ˆçº§é˜Ÿåˆ—
class PriorityTaskQueue:
    """ä¼˜å…ˆçº§ä»»åŠ¡é˜Ÿåˆ—"""

    def __init__(self):
        self.queue = PriorityQueue()

    def add_task(self, file_path: Path, priority: int = 0):
        """
        æ·»åŠ ä»»åŠ¡

        ä¼˜å…ˆçº§è§„åˆ™:
        - å°æ–‡ä»¶ä¼˜å…ˆ (å¤„ç†å¿«ï¼Œæå‡å“åº”é€Ÿåº¦)
        - å¤±è´¥è¿‡çš„æ–‡ä»¶å»¶å
        """
        file_size = file_path.stat().st_size
        # å°æ–‡ä»¶ = é«˜ä¼˜å…ˆçº§ (æ•°å€¼è¶Šå°è¶Šä¼˜å…ˆ)
        adjusted_priority = priority + (file_size // 1024)

        self.queue.put((adjusted_priority, file_path))

    def get_batch(self, batch_size: int) -> List[Path]:
        """è·å–ä¸€æ‰¹ä»»åŠ¡"""
        batch = []
        for _ in range(batch_size):
            if not self.queue.empty():
                _, file_path = self.queue.get()
                batch.append(file_path)
        return batch
```

#### 7.2.2 å†…å­˜ä¼˜åŒ–

```python
# ç­–ç•¥1: æµå¼å¤„ç†
def analyze_file_streaming(csv_path: str, params: Dict) -> Dict:
    """
    æµå¼å¤„ç†å¤§æ–‡ä»¶

    ç­–ç•¥:
    - ä½¿ç”¨pandas chunksizeè¯»å–
    - é€å—è®¡ç®—åµŒå…¥
    - å¢é‡æ„å»ºé€’å½’çŸ©é˜µ
    """
    chunk_size = 10000
    all_embedded = []

    for chunk in pd.read_csv(csv_path, chunksize=chunk_size):
        x = chunk['x'].values
        y = chunk['y'].values

        # åµŒå…¥
        embedded = embed_signal_2d(x, y, params['m'], params['tau'])
        all_embedded.append(embedded)

        # é‡Šæ”¾chunkå†…å­˜
        del chunk, x, y

    # åˆå¹¶åµŒå…¥
    final_embedded = np.vstack(all_embedded)
    del all_embedded

    # è®¡ç®—é€’å½’çŸ©é˜µï¼ˆåˆ†å—è®¡ç®—ï¼‰
    rp = compute_recurrence_matrix_chunked(final_embedded, params['eps'])

    return compute_rqa_metrics(rp, params['lmin'])

# ç­–ç•¥2: æ˜¾å¼å†…å­˜é‡Šæ”¾
import gc

def process_param_combination(params: Dict, files: List[Path]):
    """å¤„ç†å•ä¸ªå‚æ•°ç»„åˆ"""
    results = []

    for file_path in files:
        result = analyze_single_file(file_path, params)
        results.append(result)

        # æ¯å¤„ç†10ä¸ªæ–‡ä»¶ï¼Œå¼ºåˆ¶åƒåœ¾å›æ”¶
        if len(results) % 10 == 0:
            gc.collect()

    return results
```

#### 7.2.3 CPUäº²å’Œæ€§ä¼˜åŒ–

```python
import os
import psutil

def set_cpu_affinity(worker_id: int, max_workers: int):
    """
    è®¾ç½®CPUäº²å’Œæ€§ï¼Œé¿å…çº¿ç¨‹è¿ç§»

    ç­–ç•¥:
    - å°†çº¿ç¨‹ç»‘å®šåˆ°ç‰¹å®šCPUæ ¸å¿ƒ
    - å‡å°‘ä¸Šä¸‹æ–‡åˆ‡æ¢
    - æå‡ç¼“å­˜å‘½ä¸­ç‡
    """
    cpu_count = psutil.cpu_count(logical=True)

    # ä¸ºæ¯ä¸ªworkeråˆ†é…ä¸“å±CPUæ ¸å¿ƒ
    assigned_cpus = [(worker_id * cpu_count) // max_workers]

    try:
        p = psutil.Process(os.getpid())
        p.cpu_affinity(assigned_cpus)
        logger.debug(f"Worker {worker_id} bound to CPU {assigned_cpus}")
    except Exception as e:
        logger.warning(f"Failed to set CPU affinity: {e}")
```

---

## 8. æ•°æ®æŒä¹…åŒ–è®¾è®¡

### 8.1 ç›®å½•ç»“æ„

```
data/05_rqa_analysis/
â”œâ”€â”€ cache/
â”‚   â”œâ”€â”€ param_combinations.json          # æ‰€æœ‰ç”Ÿæˆçš„å‚æ•°ç»„åˆ
â”‚   â”œâ”€â”€ task_history.json                # ä»»åŠ¡å†å²è®°å½•
â”‚   â””â”€â”€ last_analysis.json               # æœ€è¿‘ä¸€æ¬¡åˆ†æçš„ç¼“å­˜
â”‚
â”œâ”€â”€ results/
â”‚   â”œâ”€â”€ m2_tau1_eps0.05_lmin2/           # å‚æ•°ç»„åˆç›®å½•
â”‚   â”‚   â”œâ”€â”€ metadata.json                # å…ƒæ•°æ®
â”‚   â”‚   â”œâ”€â”€ step1_rqa_calculation/
â”‚   â”‚   â”‚   â”œâ”€â”€ control_rqa_results.csv
â”‚   â”‚   â”‚   â”œâ”€â”€ mci_rqa_results.csv
â”‚   â”‚   â”‚   â””â”€â”€ ad_rqa_results.csv
â”‚   â”‚   â”œâ”€â”€ step2_data_merging/
â”‚   â”‚   â”‚   â””â”€â”€ merged_data.csv
â”‚   â”‚   â”œâ”€â”€ step3_feature_enrichment/
â”‚   â”‚   â”‚   â””â”€â”€ enriched_features.csv
â”‚   â”‚   â”œâ”€â”€ step4_statistical_analysis/
â”‚   â”‚   â”‚   â”œâ”€â”€ descriptive_stats.csv
â”‚   â”‚   â”‚   â”œâ”€â”€ group_comparison.csv
â”‚   â”‚   â”‚   â””â”€â”€ correlation_matrix.csv
â”‚   â”‚   â””â”€â”€ step5_visualization/
â”‚   â”‚       â”œâ”€â”€ recurrence_plots/
â”‚   â”‚       â”‚   â”œâ”€â”€ control_legacy_1_q1.png
â”‚   â”‚       â”‚   â””â”€â”€ ...
â”‚   â”‚       â”œâ”€â”€ time_series_plots/
â”‚   â”‚       â””â”€â”€ statistical_plots/
â”‚   â”‚           â”œâ”€â”€ group_comparison_boxplot.png
â”‚   â”‚           â”œâ”€â”€ rqa_heatmap.png
â”‚   â”‚           â””â”€â”€ correlation_heatmap.png
â”‚   â”‚
â”‚   â”œâ”€â”€ m2_tau1_eps0.051_lmin2/
â”‚   â””â”€â”€ ...
â”‚
â””â”€â”€ exports/
    â”œâ”€â”€ rqa_analysis_20251007_153045.xlsx  # å®Œæ•´å¯¼å‡º
    â”œâ”€â”€ rqa_analysis_20251007_153045.zip   # æ‰“åŒ…ä¸‹è½½
    â””â”€â”€ summary_report_20251007.pdf        # åˆ†ææŠ¥å‘Š
```

### 8.2 å…ƒæ•°æ®æ ¼å¼

```json
{
  "signature": "m2_tau1_eps0.05_lmin2",
  "parameters": {
    "m": 2,
    "tau": 1,
    "eps": 0.05,
    "lmin": 2
  },
  "creation_time": "2025-10-07T15:30:45",
  "last_updated": "2025-10-07T16:45:22",
  "steps_completed": {
    "step1_rqa_calculation": {
      "completed": true,
      "timestamp": "2025-10-07T15:45:12",
      "files_processed": 500,
      "files_failed": 3
    },
    "step2_data_merging": {
      "completed": true,
      "timestamp": "2025-10-07T15:46:00",
      "total_records": 497
    },
    "step3_feature_enrichment": {
      "completed": true,
      "timestamp": "2025-10-07T15:50:30",
      "features_added": 15
    },
    "step4_statistical_analysis": {
      "completed": true,
      "timestamp": "2025-10-07T16:00:00",
      "significant_features": 8
    },
    "step5_visualization": {
      "completed": true,
      "timestamp": "2025-10-07T16:45:22",
      "plots_generated": 45
    }
  },
  "statistics": {
    "total_processing_time_seconds": 4537,
    "avg_time_per_file_seconds": 2.1,
    "memory_peak_mb": 1024
  },
  "errors": [
    {
      "step": 1,
      "file": "control_legacy_10_q4_calibrated.csv",
      "error": "Signal too short for embedding",
      "timestamp": "2025-10-07T15:35:12"
    }
  ]
}
```

### 8.3 å¢é‡ç¼“å­˜ç­–ç•¥

```python
class RQACache:
    """RQAç»“æœç¼“å­˜ç®¡ç†å™¨"""

    def __init__(self, cache_dir: Path):
        self.cache_dir = cache_dir
        self.cache_dir.mkdir(parents=True, exist_ok=True)

        self.param_cache_file = cache_dir / 'param_combinations.json'
        self.task_history_file = cache_dir / 'task_history.json'

    def is_param_completed(self, params: Dict, step: int = 5) -> bool:
        """
        æ£€æŸ¥å‚æ•°ç»„åˆæ˜¯å¦å·²å®ŒæˆæŒ‡å®šæ­¥éª¤

        Args:
            params: RQAå‚æ•°
            step: æ­¥éª¤ç¼–å· (1-5)

        Returns:
            æ˜¯å¦å·²å®Œæˆ
        """
        signature = generate_param_signature(params)
        param_dir = get_param_directory(params)
        metadata_file = param_dir / 'metadata.json'

        if not metadata_file.exists():
            return False

        with open(metadata_file, 'r', encoding='utf-8') as f:
            metadata = json.load(f)

        step_key = f'step{step}_{"_".join(["rqa_calculation", "data_merging", "feature_enrichment", "statistical_analysis", "visualization"][step-1].split())}'
        return metadata.get('steps_completed', {}).get(step_key, {}).get('completed', False)

    def get_completed_params(self, all_params: List[Dict]) -> Tuple[List[Dict], List[Dict]]:
        """
        åˆ†ç¦»å·²å®Œæˆå’Œæœªå®Œæˆçš„å‚æ•°ç»„åˆ

        Args:
            all_params: æ‰€æœ‰å‚æ•°ç»„åˆ

        Returns:
            (completed, pending)
        """
        completed = []
        pending = []

        for params in all_params:
            if self.is_param_completed(params, step=5):
                completed.append(params)
            else:
                pending.append(params)

        return completed, pending

    def save_partial_result(self, params: Dict, step: int, data: Dict):
        """
        ä¿å­˜éƒ¨åˆ†ç»“æœï¼ˆæ–­ç‚¹ç»­ä¼ ï¼‰

        Args:
            params: RQAå‚æ•°
            step: å½“å‰æ­¥éª¤
            data: ç»“æœæ•°æ®
        """
        param_dir = get_param_directory(params)
        partial_file = param_dir / f'step{step}_partial.json'

        with open(partial_file, 'w', encoding='utf-8') as f:
            json.dump(data, f, ensure_ascii=False, indent=2)

        logger.info(f"Saved partial result for step {step}: {partial_file}")

    def load_partial_result(self, params: Dict, step: int) -> Optional[Dict]:
        """åŠ è½½éƒ¨åˆ†ç»“æœ"""
        param_dir = get_param_directory(params)
        partial_file = param_dir / f'step{step}_partial.json'

        if not partial_file.exists():
            return None

        with open(partial_file, 'r', encoding='utf-8') as f:
            return json.load(f)
```

---

## 9. å‰ç«¯äº¤äº’è®¾è®¡

### 9.1 é¡µé¢å¸ƒå±€

```jsx
// Module05.jsx
import React, { useState, useEffect } from 'react';
import { Card, Row, Col, Tabs, Button, Progress, message } from 'antd';

const Module05 = () => {
  return (
    <div className="module05-container">
      <Tabs defaultActiveKey="1">
        {/* Tab 1: å‚æ•°é…ç½® */}
        <Tabs.TabPane tab="å‚æ•°é…ç½®" key="1">
          <ParamConfigPanel />
        </Tabs.TabPane>

        {/* Tab 2: æ‰¹é‡æ‰§è¡Œ */}
        <Tabs.TabPane tab="æ‰¹é‡æ‰§è¡Œ" key="2">
          <BatchExecutionPanel />
        </Tabs.TabPane>

        {/* Tab 3: ç»“æœæŸ¥çœ‹ */}
        <Tabs.TabPane tab="ç»“æœæŸ¥çœ‹" key="3">
          <ResultsViewer />
        </Tabs.TabPane>

        {/* Tab 4: å¯è§†åŒ– */}
        <Tabs.TabPane tab="å¯è§†åŒ–åˆ†æ" key="4">
          <VisualizationPanel />
        </Tabs.TabPane>
      </Tabs>
    </div>
  );
};
```

### 9.2 å‚æ•°é…ç½®ç»„ä»¶

```jsx
const ParamConfigPanel = () => {
  const [paramRanges, setParamRanges] = useState({
    m: { start: 1, end: 10, step: 1 },
    tau: { start: 1, end: 10, step: 1 },
    eps: { start: 0.05, end: 0.1, step: 0.001 },
    lmin: { start: 2, end: 3, step: 1 }
  });

  const [totalCombinations, setTotalCombinations] = useState(0);

  useEffect(() => {
    calculateTotalCombinations();
  }, [paramRanges]);

  const calculateTotalCombinations = () => {
    const { m, tau, eps, lmin } = paramRanges;
    const total =
      Math.floor((m.end - m.start) / m.step + 1) *
      Math.floor((tau.end - tau.start) / tau.step + 1) *
      Math.floor((eps.end - eps.start) / eps.step + 1) *
      Math.floor((lmin.end - lmin.start) / lmin.step + 1);

    setTotalCombinations(total);
  };

  const handleGenerate = async () => {
    try {
      const response = await axios.post('/api/m05/params/generate', {
        m_range: paramRanges.m,
        tau_range: paramRanges.tau,
        eps_range: paramRanges.eps,
        lmin_range: paramRanges.lmin
      });

      if (response.data.success) {
        message.success(`æˆåŠŸç”Ÿæˆ ${response.data.total_combinations} ä¸ªå‚æ•°ç»„åˆ`);
      }
    } catch (error) {
      message.error('ç”Ÿæˆå‚æ•°ç»„åˆå¤±è´¥');
    }
  };

  return (
    <Card title="RQAå‚æ•°é…ç½®">
      <Row gutter={[16, 16]}>
        <Col span={6}>
          <ParamRangeInput
            label="åµŒå…¥ç»´åº¦ (m)"
            value={paramRanges.m}
            onChange={(val) => setParamRanges({...paramRanges, m: val})}
          />
        </Col>
        <Col span={6}>
          <ParamRangeInput
            label="æ—¶é—´å»¶è¿Ÿ (Ï„)"
            value={paramRanges.tau}
            onChange={(val) => setParamRanges({...paramRanges, tau: val})}
          />
        </Col>
        <Col span={6}>
          <ParamRangeInput
            label="é€’å½’é˜ˆå€¼ (Îµ)"
            value={paramRanges.eps}
            onChange={(val) => setParamRanges({...paramRanges, eps: val})}
            step={0.001}
          />
        </Col>
        <Col span={6}>
          <ParamRangeInput
            label="æœ€å°çº¿é•¿ (lmin)"
            value={paramRanges.lmin}
            onChange={(val) => setParamRanges({...paramRanges, lmin: val})}
          />
        </Col>
      </Row>

      <div style={{marginTop: 24, textAlign: 'center'}}>
        <h3>é¢„è®¡ç”Ÿæˆ: <span style={{color: '#1890ff', fontSize: 32}}>{totalCombinations.toLocaleString()}</span> ä¸ªå‚æ•°ç»„åˆ</h3>
        <Button type="primary" size="large" onClick={handleGenerate}>
          ç”Ÿæˆå‚æ•°ç»„åˆ
        </Button>
      </div>
    </Card>
  );
};
```

### 9.3 æ‰¹é‡æ‰§è¡Œé¢æ¿

```jsx
const BatchExecutionPanel = () => {
  const [taskStatus, setTaskStatus] = useState(null);
  const [isRunning, setIsRunning] = useState(false);

  // è½®è¯¢ä»»åŠ¡çŠ¶æ€
  useEffect(() => {
    let interval;
    if (isRunning && taskStatus?.task_id) {
      interval = setInterval(async () => {
        const response = await axios.get('/api/m05/analyze/status', {
          params: { task_id: taskStatus.task_id }
        });

        setTaskStatus(response.data);

        if (response.data.status === 'completed' ||
            response.data.status === 'failed') {
          setIsRunning(false);
          clearInterval(interval);
        }
      }, 2000);  // æ¯2ç§’æ›´æ–°ä¸€æ¬¡
    }

    return () => clearInterval(interval);
  }, [isRunning, taskStatus?.task_id]);

  const handleStartAnalysis = async () => {
    try {
      const response = await axios.post('/api/m05/analyze/batch', {
        param_combinations: [...],  // ä»çŠ¶æ€è·å–
        groups: ['control', 'mci', 'ad'],
        execute_steps: [1, 2, 3, 4, 5]
      });

      if (response.data.success) {
        setTaskStatus(response.data);
        setIsRunning(true);
        message.success('åˆ†æä»»åŠ¡å·²å¯åŠ¨');
      }
    } catch (error) {
      message.error('å¯åŠ¨åˆ†æå¤±è´¥');
    }
  };

  const handleCancelAnalysis = async () => {
    try {
      await axios.post('/api/m05/analyze/cancel', {
        task_id: taskStatus.task_id
      });

      setIsRunning(false);
      message.info('ä»»åŠ¡å·²å–æ¶ˆ');
    } catch (error) {
      message.error('å–æ¶ˆä»»åŠ¡å¤±è´¥');
    }
  };

  return (
    <Card title="æ‰¹é‡æ‰§è¡Œ">
      {!isRunning ? (
        <Button type="primary" size="large" onClick={handleStartAnalysis}>
          å¼€å§‹æ‰¹é‡åˆ†æ
        </Button>
      ) : (
        <div>
          <h3>ä»»åŠ¡è¿›åº¦</h3>
          <Progress
            percent={taskStatus?.progress || 0}
            status={taskStatus?.status === 'failed' ? 'exception' : 'active'}
          />

          <Row gutter={[16, 16]} style={{marginTop: 16}}>
            <Col span={8}>
              <Statistic
                title="å·²å¤„ç†æ–‡ä»¶"
                value={taskStatus?.processed_files || 0}
                suffix={`/ ${taskStatus?.total_files || 0}`}
              />
            </Col>
            <Col span={8}>
              <Statistic
                title="å½“å‰æ­¥éª¤"
                value={`Step ${taskStatus?.current_step || 1}`}
              />
            </Col>
            <Col span={8}>
              <Statistic
                title="é¢„è®¡å‰©ä½™æ—¶é—´"
                value={formatETA(taskStatus?.eta_seconds)}
              />
            </Col>
          </Row>

          <div style={{marginTop: 16}}>
            <Button danger onClick={handleCancelAnalysis}>
              å–æ¶ˆä»»åŠ¡
            </Button>
          </div>
        </div>
      )}
    </Card>
  );
};
```

---

## 10. æ€§èƒ½ä¼˜åŒ–ç­–ç•¥

### 10.1 è®¡ç®—ä¼˜åŒ–

| ä¼˜åŒ–é¡¹ | ç­–ç•¥ | é¢„æœŸæå‡ |
|--------|------|----------|
| **å‘é‡åŒ–è®¡ç®—** | ä½¿ç”¨NumPyå‘é‡åŒ–ä»£æ›¿Pythonå¾ªç¯ | 10-50x |
| **è·ç¦»çŸ©é˜µè®¡ç®—** | ä½¿ç”¨scipy.spatial.distance.pdist | 5-10x |
| **å†…å­˜å¯¹é½** | ä½¿ç”¨np.ascontiguousarray | 1.2-1.5x |
| **æ•°æ®ç±»å‹ä¼˜åŒ–** | RPçŸ©é˜µä½¿ç”¨int8è€Œéint32/int64 | å‡å°‘75%å†…å­˜ |
| **ç¨€ç–çŸ©é˜µ** | å¯¹äºä½RRçŸ©é˜µä½¿ç”¨scipy.sparse | å‡å°‘50-90%å†…å­˜ |

### 10.2 I/Oä¼˜åŒ–

```python
# ç­–ç•¥1: æ‰¹é‡è¯»å–
def load_files_batch(file_paths: List[Path],
                    columns: List[str] = None) -> Dict[Path, pd.DataFrame]:
    """
    æ‰¹é‡è¯»å–CSVæ–‡ä»¶

    ä¼˜åŠ¿:
    - å‡å°‘ç£ç›˜I/Oæ¬¡æ•°
    - åˆ©ç”¨æ“ä½œç³»ç»Ÿé¢„è¯»ç¼“å­˜
    """
    data = {}
    for path in file_paths:
        df = pd.read_csv(path, usecols=columns)  # åªè¯»å–éœ€è¦çš„åˆ—
        data[path] = df
    return data

# ç­–ç•¥2: åˆ—é€‰æ‹©
# åªè¯»å–x, yåˆ—ï¼Œå¿½ç•¥å…¶ä»–åˆ—
df = pd.read_csv(csv_path, usecols=['x', 'y', 'milliseconds'])

# ç­–ç•¥3: å‹ç¼©ä¿å­˜
df.to_csv(output_path, compression='gzip')  # å‡å°‘50-70%ç£ç›˜ç©ºé—´
```

### 10.3 å¹¶è¡Œåº¦è°ƒä¼˜

```python
def get_optimal_workers(task_type: str) -> int:
    """
    æ ¹æ®ä»»åŠ¡ç±»å‹ç¡®å®šæœ€ä¼˜çº¿ç¨‹æ•°

    Args:
        task_type: 'cpu_bound' æˆ– 'io_bound'

    Returns:
        æœ€ä¼˜çº¿ç¨‹æ•°
    """
    from multiprocessing import cpu_count

    if task_type == 'cpu_bound':
        # CPUå¯†é›†å‹ï¼šçº¿ç¨‹æ•° = CPUæ ¸å¿ƒæ•°
        return cpu_count()

    elif task_type == 'io_bound':
        # I/Oå¯†é›†å‹ï¼šçº¿ç¨‹æ•° = 2-4 * CPUæ ¸å¿ƒæ•°
        return cpu_count() * 2

    return cpu_count()

# RQAåˆ†ææ˜¯CPUå¯†é›†å‹
max_workers = get_optimal_workers('cpu_bound')
```

---

## 11. å®æ–½è®¡åˆ’

### 11.1 å¼€å‘é˜¶æ®µï¼ˆ3å‘¨ï¼‰

**Week 1: æ ¸å¿ƒåŠŸèƒ½å¼€å‘**
- Day 1-2: é¡¹ç›®ç»“æ„æ­å»º
  - åˆ›å»ºç›®å½•ç»“æ„
  - é…ç½®æ–‡ä»¶å’Œå¸¸é‡å®šä¹‰
  - åŸºç¡€å·¥å…·å‡½æ•°

- Day 3-4: RQAæ ¸å¿ƒç®—æ³•
  - åµŒå…¥é‡æ„å‡½æ•°
  - é€’å½’çŸ©é˜µè®¡ç®—
  - RQAæŒ‡æ ‡è®¡ç®—
  - å•å…ƒæµ‹è¯•

- Day 5-7: Serviceå±‚å’ŒAPIå±‚
  - RQAAnalysisServiceå®ç°
  - APIç«¯ç‚¹å¼€å‘
  - è£…é¥°å™¨å®ç°
  - é›†æˆæµ‹è¯•

**Week 2: å¹¶è¡Œå¤„ç†å’Œä¼˜åŒ–**
- Day 8-10: å¤šçº¿ç¨‹æ‰§è¡Œå™¨
  - RQATaskExecutorå®ç°
  - ä»»åŠ¡è°ƒåº¦é€»è¾‘
  - è¿›åº¦è¿½è¸ª
  - é”™è¯¯æ¢å¤

- Day 11-12: æ€§èƒ½ä¼˜åŒ–
  - å‘é‡åŒ–è®¡ç®—
  - å†…å­˜ä¼˜åŒ–
  - I/Oä¼˜åŒ–
  - æ€§èƒ½æµ‹è¯•

- Day 13-14: 5æ­¥æµç¨‹å®ç°
  - Step 1: RQAè®¡ç®—
  - Step 2: æ•°æ®åˆå¹¶
  - Step 3: ç‰¹å¾å¢å¼º
  - Step 4: ç»Ÿè®¡åˆ†æ
  - Step 5: å¯è§†åŒ–

**Week 3: å‰ç«¯å¼€å‘å’Œé›†æˆ**
- Day 15-17: å‰ç«¯ç»„ä»¶
  - Module05.jsxä¸»é¡µé¢
  - å‚æ•°é…ç½®é¢æ¿
  - æ‰¹é‡æ‰§è¡Œé¢æ¿
  - ç»“æœæŸ¥çœ‹å™¨

- Day 18-19: å¯è§†åŒ–
  - é€’å½’å›¾ç”Ÿæˆ
  - ç»Ÿè®¡å›¾è¡¨
  - äº¤äº’åŠŸèƒ½

- Day 20-21: é›†æˆæµ‹è¯•å’Œä¼˜åŒ–
  - ç«¯åˆ°ç«¯æµ‹è¯•
  - æ€§èƒ½æµ‹è¯•
  - Bugä¿®å¤
  - æ–‡æ¡£å®Œå–„

### 11.2 å·¥ä½œé‡ä¼°ç®—

| ä»»åŠ¡ | å·¥æ—¶(å°æ—¶) | ä¼˜å…ˆçº§ |
|------|-----------|--------|
| é¡¹ç›®ç»“æ„æ­å»º | 4 | P0 |
| RQAæ ¸å¿ƒç®—æ³• | 16 | P0 |
| Serviceå±‚ | 12 | P0 |
| APIå±‚ | 10 | P0 |
| å¤šçº¿ç¨‹æ‰§è¡Œå™¨ | 16 | P0 |
| 5æ­¥æµç¨‹ | 20 | P0 |
| å‰ç«¯UI | 20 | P1 |
| å¯è§†åŒ– | 12 | P1 |
| æ€§èƒ½ä¼˜åŒ– | 16 | P1 |
| æµ‹è¯• | 20 | P0 |
| æ–‡æ¡£ | 8 | P2 |
| **æ€»è®¡** | **154å°æ—¶** | - |

**äººåŠ›é…ç½®**: 2äºº Ã— 3å‘¨ = 240å°æ—¶ (å«buffer)

### 11.3 é‡Œç¨‹ç¢‘

| é‡Œç¨‹ç¢‘ | æ—¥æœŸ | äº¤ä»˜ç‰© |
|--------|------|--------|
| M1: æ ¸å¿ƒç®—æ³•å®Œæˆ | Week 1 End | RQAç®—æ³•å•å…ƒæµ‹è¯•é€šè¿‡ |
| M2: åç«¯å®Œæˆ | Week 2 End | APIå…¨éƒ¨ç«¯ç‚¹å¯ç”¨ |
| M3: MVPå®Œæˆ | Week 3 Mid | å‰åç«¯åŸºæœ¬åŠŸèƒ½å¯ç”¨ |
| M4: æ­£å¼å‘å¸ƒ | Week 3 End | å®Œæ•´åŠŸèƒ½ + æ–‡æ¡£ |

---

## é™„å½•

### A. æŠ€æœ¯æ ˆ

**åç«¯**:
- Python 3.8+
- Flask (Webæ¡†æ¶)
- NumPy (æ•°å€¼è®¡ç®—)
- Pandas (æ•°æ®å¤„ç†)
- SciPy (ç§‘å­¦è®¡ç®—)
- Matplotlib/Seaborn (å¯è§†åŒ–)
- ThreadPoolExecutor (å¹¶è¡Œå¤„ç†)

**å‰ç«¯**:
- React 18
- Ant Design 5
- Axios (HTTPå®¢æˆ·ç«¯)
- ECharts (å›¾è¡¨åº“)

**æ¶æ„æ¨¡å¼**:
- ä¸‰å±‚æ¶æ„ (API - Service - Business Logic)
- è£…é¥°å™¨æ¨¡å¼ (é”™è¯¯å¤„ç†ã€å‚æ•°éªŒè¯)
- å•ä¾‹æ¨¡å¼ (Serviceæ‡’åŠ è½½)
- å·¥å‚æ¨¡å¼ (å‚æ•°ç»„åˆç”Ÿæˆ)

### B. å‚è€ƒæ–‡çŒ®

1. Marwan, N., et al. (2007). "Recurrence plots for the analysis of complex systems." *Physics Reports*, 438(5-6), 237-329.

2. Zbilut, J. P., & Webber Jr, C. L. (1992). "Embeddings and delays as derived from quantification of recurrence plots." *Physics Letters A*, 171(3-4), 199-203.

3. Anderson, N. C., et al. (2013). "Eye tracking in human-computer interaction and usability research." *Psychological Methods*, 18(3), 338-356.

### C. è¯æ±‡è¡¨

| æœ¯è¯­ | è‹±æ–‡ | è¯´æ˜ |
|------|------|------|
| é€’å½’é‡åŒ–åˆ†æ | Recurrence Quantification Analysis (RQA) | éçº¿æ€§æ—¶é—´åºåˆ—åˆ†ææ–¹æ³• |
| åµŒå…¥ç»´åº¦ | Embedding Dimension (m) | ç›¸ç©ºé—´é‡æ„çš„ç»´åº¦ |
| æ—¶é—´å»¶è¿Ÿ | Time Delay (Ï„) | åµŒå…¥å‘é‡é—´çš„æ—¶é—´é—´éš” |
| é€’å½’é˜ˆå€¼ | Recurrence Threshold (Îµ) | åˆ¤å®šé€’å½’ç‚¹çš„è·ç¦»é˜ˆå€¼ |
| é€’å½’ç‡ | Recurrence Rate (RR) | é€’å½’ç‚¹å æ€»ç‚¹æ•°çš„æ¯”ä¾‹ |
| ç¡®å®šæ€§ | Determinism (DET) | å½¢æˆå¯¹è§’çº¿ç»“æ„çš„é€’å½’ç‚¹æ¯”ä¾‹ |
| ç†µ | Entropy (ENT) | å¯¹è§’çº¿é•¿åº¦åˆ†å¸ƒçš„é¦™å†œç†µ |

---

**æ–‡æ¡£ç»“æŸ**
