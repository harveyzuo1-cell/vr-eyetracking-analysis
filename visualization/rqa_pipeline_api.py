#!/usr/bin/env python
# -*- coding: utf-8 -*-
"""
RQAåˆ†ææµç¨‹APIæ¨¡å—
å®Œæ•´çš„çœ¼åŠ¨æ•°æ®é€’å½’é‡åŒ–åˆ†ææµç¨‹ï¼Œä»æ•°æ®å¤„ç†åˆ°ç»Ÿè®¡åˆ†æå†åˆ°å¯è§†åŒ–
"""

import os
import math
import json
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import base64
import io
import time
from datetime import datetime
from flask import Blueprint, request, jsonify
from typing import Dict, List
import traceback

# åˆ›å»ºè“å›¾
rqa_pipeline_bp = Blueprint('rqa_pipeline', __name__)

# å…¨å±€é…ç½®
plt.rcParams['font.sans-serif'] = ['SimHei', 'Arial', 'DejaVu Sans']
plt.rcParams['axes.unicode_minus'] = False

# åŸºç¡€æ•°æ®ç›®å½•
BASE_DATA_DIR = 'data'
PIPELINE_RESULTS_DIR = os.path.join(BASE_DATA_DIR, 'rqa_pipeline_results')
MODULE10_DATASET_ROOT = os.path.join(BASE_DATA_DIR, 'module10_datasets')

# çœ¼åŠ¨æ•°æ®è·¯å¾„ (æ ¡å‡†åçš„æ•°æ®)
CONTROL_DATA_DIR = os.path.join(BASE_DATA_DIR, 'control_calibrated')
MCI_DATA_DIR = os.path.join(BASE_DATA_DIR, 'mci_calibrated')
AD_DATA_DIR = os.path.join(BASE_DATA_DIR, 'ad_calibrated')

# ç¡®ä¿ç›®å½•å­˜åœ¨
os.makedirs(PIPELINE_RESULTS_DIR, exist_ok=True)
os.makedirs(MODULE10_DATASET_ROOT, exist_ok=True)


###############################################################################
# å‚æ•°ç®¡ç†å’Œç›®å½•ç»“æ„
###############################################################################

def generate_param_signature(params):
    """ç”Ÿæˆå‚æ•°ç­¾å"""
    m = params.get('m', 2)
    tau = params.get('tau', params.get('delay', 1))  # å…¼å®¹delayå’Œtau
    eps = params.get('eps', 0.05)
    lmin = params.get('lmin', 2)
    return f"m{m}_tau{tau}_eps{eps}_lmin{lmin}"


def get_param_directory(params):
    """è·å–å‚æ•°å¯¹åº”çš„ç›®å½•è·¯å¾„"""
    signature = generate_param_signature(params)
    return os.path.join(PIPELINE_RESULTS_DIR, signature)


def get_step_directory(params, step_name):
    """è·å–ç‰¹å®šæ­¥éª¤çš„ç›®å½•è·¯å¾„"""
    param_dir = get_param_directory(params)
    step_dir = os.path.join(param_dir, step_name)
    os.makedirs(step_dir, exist_ok=True)
    return step_dir


def save_param_metadata(params, step_completed):
    """ä¿å­˜å‚æ•°å…ƒæ•°æ®"""
    param_dir = get_param_directory(params)
    os.makedirs(param_dir, exist_ok=True)
    
    metadata_file = os.path.join(param_dir, 'metadata.json')
    
    # è¯»å–ç°æœ‰å…ƒæ•°æ®
    metadata = {}
    if os.path.exists(metadata_file):
        try:
            with open(metadata_file, 'r', encoding='utf-8') as f:
                metadata = json.load(f)
        except:
            pass
    
    # æ›´æ–°å…ƒæ•°æ®
    metadata.update({
        'signature': generate_param_signature(params),
        'parameters': params,
        'last_updated': datetime.now().isoformat(),
        f'step_{step_completed}_completed': True
    })
    
    # ä¿å­˜å…ƒæ•°æ®
    with open(metadata_file, 'w', encoding='utf-8') as f:
        json.dump(metadata, f, indent=2, ensure_ascii=False)


def get_param_history():
    """è·å–æ‰€æœ‰å‚æ•°å†å²è®°å½•"""
    history = []
    
    if not os.path.exists(PIPELINE_RESULTS_DIR):
        return history
    
    for param_folder in os.listdir(PIPELINE_RESULTS_DIR):
        param_path = os.path.join(PIPELINE_RESULTS_DIR, param_folder)
        if os.path.isdir(param_path):
            metadata_file = os.path.join(param_path, 'metadata.json')
            if os.path.exists(metadata_file):
                try:
                    with open(metadata_file, 'r', encoding='utf-8') as f:
                        metadata = json.load(f)
                    
                    # è®¡ç®—å®Œæˆæ­¥éª¤æ•°
                    completed_steps = sum(1 for i in range(1, 6) 
                                        if metadata.get(f'step_{i}_completed', False))
                    
                    history.append({
                        'signature': metadata.get('signature', param_folder),
                        'params': metadata.get('parameters', {}),
                        'completed_steps': completed_steps,
                        'progress': (completed_steps / 5) * 100,
                        'last_updated': metadata.get('last_updated', '')
                    })
                except:
                    # å¦‚æœå…ƒæ•°æ®æŸåï¼Œå°è¯•è§£ææ–‡ä»¶å¤¹å
                    try:
                        # è§£ææ–‡ä»¶å¤¹å: m2_tau1_eps0.05_lmin2
                        parts = param_folder.split('_')
                        params = {}
                        for part in parts:
                            if part.startswith('m'):
                                params['m'] = int(part[1:])
                            elif part.startswith('tau'):
                                params['tau'] = int(part[3:])
                            elif part.startswith('eps'):
                                params['eps'] = float(part[3:])
                            elif part.startswith('lmin'):
                                params['lmin'] = int(part[4:])
                        
                        # æ£€æŸ¥å®Œæˆçš„æ­¥éª¤
                        step_dirs = ['step1_rqa_calculation', 'step2_data_merging', 
                                   'step3_feature_enrichment', 'step4_statistical_analysis', 
                                   'step5_visualization']
                        completed_steps = sum(1 for step_dir in step_dirs 
                                            if os.path.exists(os.path.join(param_path, step_dir)))
                        
                        history.append({
                            'signature': param_folder,
                            'params': params,
                            'completed_steps': completed_steps,
                            'progress': (completed_steps / 5) * 100,
                            'last_updated': ''
                        })
                    except:
                        pass
    
    # æŒ‰æœ€åæ›´æ–°æ—¶é—´æ’åº
    history.sort(key=lambda x: x.get('last_updated', ''), reverse=True)
    return history


###############################################################################
# RQAè®¡ç®—æ¨¡å— - æ ¸å¿ƒç®—æ³•å®ç°
###############################################################################

def load_xy_time_series(csv_path):
    """
    è¯»å– CSVï¼ˆéœ€åŒ…å« 'x','y' æˆ– 'GazePointX_normalized','GazePointY_normalized' åˆ—ï¼‰ï¼Œ
    è¿”å›: x_, y_, t_, df
    """
    df = pd.read_csv(csv_path)

    # æ”¯æŒä¸¤ç§åˆ—åæ ¼å¼
    if 'x' in df.columns and 'y' in df.columns:
        x_ = df['x'].to_numpy()
        y_ = df['y'].to_numpy()
    elif 'GazePointX_normalized' in df.columns and 'GazePointY_normalized' in df.columns:
        x_ = df['GazePointX_normalized'].to_numpy()
        y_ = df['GazePointY_normalized'].to_numpy()
    else:
        raise ValueError(f"{csv_path} ç¼ºå°‘ x/y æˆ– GazePointX_normalized/GazePointY_normalized åˆ—!")

    if 'milliseconds' in df.columns:
        t_ = df['milliseconds'].to_numpy()
    else:
        t_ = np.arange(len(x_))
    return x_, y_, t_, df


def embed_seq_1d(x_, m=2, delay=1):
    """1D: åªç”¨ x åºåˆ— => shape=(N-(m-1)*delay, m)"""
    N = len(x_)
    rows = N - (m-1)*delay
    if rows <= 0:
        return np.empty((0, m))
    emb = []
    for i in range(rows):
        emb.append([x_[i+k*delay] for k in range(m)])
    return np.array(emb)


def embed_seq_2d(x_, y_, m=2, delay=1):
    """2D: ç”¨ (x,y)ï¼Œshape=(N-(m-1)*delay, 2*m)"""
    N = len(x_)
    if len(y_) != N:
        raise ValueError("x_ å’Œ y_ é•¿åº¦ä¸åŒ¹é…!")
    rows = N - (m-1)*delay
    if rows <= 0:
        return np.empty((0, 2*m))
    emb = np.zeros((rows, 2*m), dtype=float)
    for i in range(rows):
        for k in range(m):
            emb[i, 2*k] = x_[i + k*delay]
            emb[i, 2*k+1] = y_[i + k*delay]
    return emb


def compute_rp_1dabs(emb_data, eps=0.05):
    """å½“åªç”¨ 1D embedding æ—¶ï¼Œç”¨ç»å¯¹å·®ä¹‹å’Œ â‰¤ eps => 1"""
    M = emb_data.shape[0]
    m = emb_data.shape[1]
    RP = np.zeros((M, M), dtype=int)
    for i in range(M):
        for j in range(M):
            dist = sum(abs(emb_data[i, k] - emb_data[j, k]) for k in range(m))
            if dist <= eps:
                RP[i, j] = 1
    return RP


def compute_rp_euclid(emb_data, eps=0.05):
    """å¯¹ 2D embedding ç”¨æ¬§å‡ é‡Œå¾—è·ç¦» â‰¤ eps => 1"""
    M = emb_data.shape[0]
    RP = np.zeros((M, M), dtype=int)
    for i in range(M):
        for j in range(M):
            dist = math.sqrt(np.sum((emb_data[i]-emb_data[j])**2))
            if dist <= eps:
                RP[i, j] = 1
    return RP


def extract_diag_lengths(RP):
    """ç»Ÿè®¡å¯¹è§’çº¿ä¸­ '1' çš„è¿ç»­æ®µ => {é•¿åº¦:å‡ºç°æ¬¡æ•°}"""
    N = RP.shape[0]
    length_counts = {}
    for d in range(-(N-1), N):
        line_vals = []
        for i in range(N):
            j = i + d
            if 0 <= j < N:
                line_vals.append(RP[i, j])
        
        idx = 0
        L = len(line_vals)
        while idx < L:
            if line_vals[idx] == 1:
                seg_len = 1
                idx2 = idx + 1
                while idx2 < L and line_vals[idx2] == 1:
                    seg_len += 1
                    idx2 += 1
                length_counts[seg_len] = length_counts.get(seg_len, 0) + 1
                idx = idx2
            else:
                idx += 1
    return length_counts


def compute_rqa_measures(RP, lmin=2):
    """è¿”å› RR, DET, ENT"""
    N = RP.shape[0]
    sum_ones = RP.sum()
    RR = sum_ones/(N*N)
    
    length_dict = extract_diag_lengths(RP)
    denom_all = 0
    for l, c_ in length_dict.items():
        denom_all += l*c_
    
    numer_det = 0
    denom_ent = 0
    for l, c_ in length_dict.items():
        if l >= lmin:
            numer_det += l*c_
            denom_ent += c_
    
    DET = numer_det/denom_all if denom_all > 0 else 0
    
    ENT = 0
    if denom_ent > 0:
        sum_counts_lmin = sum(c_ for (ll, c_) in length_dict.items() if ll >= lmin)
        for l, c_ in length_dict.items():
            if l >= lmin:
                p_l = c_ / sum_counts_lmin
                if p_l > 1e-12:
                    ENT += - p_l * math.log2(p_l)
    return RR, DET, ENT


def process_single_rqa_file(csv_path, m=2, delay=1, eps=0.05, lmin=2):
    """å¤„ç†å•ä¸ªæ–‡ä»¶çš„RQAè®¡ç®—"""
    try:
        x_, y_, t_, df = load_xy_time_series(csv_path)
        
        if len(x_) < 5:
            return None
        
        # æå–æ–‡ä»¶åä¸­çš„folderå’Œqä¿¡æ¯
        filename = os.path.basename(csv_path)
        # å‡è®¾æ–‡ä»¶åæ ¼å¼: n3q1_preprocessed_calibrated.csv
        parts = filename.replace('_preprocessed_calibrated.csv', '').replace('.csv', '')
        
        result = {
            "filename": filename,
            "folder": None,
            "q": None,
            "RR-2D-xy": np.nan,
            "RR-1D-x": np.nan,
            "DET-2D-xy": np.nan,
            "DET-1D-x": np.nan,
            "ENT-2D-xy": np.nan,
            "ENT-1D-x": np.nan,
        }
        
        # è§£æfolderå’Œq
        if parts.startswith('n') and 'q' in parts:
            # Controlç»„: n3q1 -> folder=3, q=1
            folder_q = parts[1:]  # å»æ‰'n'
            if 'q' in folder_q:
                folder_str, q_str = folder_q.split('q')
                try:
                    result["folder"] = int(folder_str)
                    result["q"] = int(q_str)
                except:
                    pass
        elif parts.startswith('m') and 'q' in parts:
            # MCIç»„: m1q1 -> folder=1, q=1
            folder_q = parts[1:]  # å»æ‰'m'
            if 'q' in folder_q:
                folder_str, q_str = folder_q.split('q')
                try:
                    result["folder"] = int(folder_str)
                    result["q"] = int(q_str)
                except:
                    pass
        elif parts.startswith('ad') and 'q' in parts:
            # ADç»„: ad1q1 -> folder=1, q=1
            folder_q = parts[2:]  # å»æ‰'ad'
            if 'q' in folder_q:
                folder_str, q_str = folder_q.split('q')
                try:
                    result["folder"] = int(folder_str)
                    result["q"] = int(q_str)
                except:
                    pass
        
        # 2D-xyåˆ†æ
        emb_2d = embed_seq_2d(x_, y_, m=m, delay=delay)
        if emb_2d.shape[0] >= 2:
            RP_2d = compute_rp_euclid(emb_2d, eps=eps)
            rr2d, det2d, ent2d = compute_rqa_measures(RP_2d, lmin=lmin)
            result["RR-2D-xy"] = rr2d
            result["DET-2D-xy"] = det2d
            result["ENT-2D-xy"] = ent2d
        
        # 1D-xåˆ†æ
        emb_1d = embed_seq_1d(x_, m=m, delay=delay)
        if emb_1d.shape[0] >= 2:
            RP_1d = compute_rp_1dabs(emb_1d, eps=eps)
            rr1d, det1d, ent1d = compute_rqa_measures(RP_1d, lmin=lmin)
            result["RR-1D-x"] = rr1d
            result["DET-1D-x"] = det1d
            result["ENT-1D-x"] = ent1d
        
        return result
        
    except Exception as e:
        print(f"å¤„ç†æ–‡ä»¶ {csv_path} å‡ºé”™: {e}")
        return None


###############################################################################
# æ•°æ®å¤„ç†å‡½æ•°
###############################################################################

def merge_group_data(control_csv, mci_csv, ad_csv=None):
    """åˆå¹¶ä¸‰ç»„æ•°æ®"""
    try:
        all_data_list = []
        
        # è¯»å–Controlç»„
        if os.path.exists(control_csv):
            ctg = pd.read_csv(control_csv)
            ctg["Group"] = "Control"
            ctg["ID"] = ctg.apply(lambda row: f"n{row['folder']}q{row['q']}", axis=1)
            all_data_list.append(ctg)
        
        # è¯»å–MCIç»„
        if os.path.exists(mci_csv):
            mci = pd.read_csv(mci_csv)
            mci["Group"] = "MCI"
            mci["ID"] = mci.apply(lambda row: f"m{row['folder']}q{row['q']}", axis=1)
            all_data_list.append(mci)
        
        # è¯»å–ADç»„ï¼ˆå¦‚æœæä¾›ï¼‰
        if ad_csv and os.path.exists(ad_csv):
            ad = pd.read_csv(ad_csv)
            ad["Group"] = "AD"
            ad["ID"] = ad.apply(lambda row: f"ad{row['folder']}q{row['q']}", axis=1)
            all_data_list.append(ad)
        
        if not all_data_list:
            raise ValueError("æ²¡æœ‰æ‰¾åˆ°æœ‰æ•ˆçš„æ•°æ®æ–‡ä»¶")
        
        # åˆå¹¶æ•°æ®
        all_data = pd.concat(all_data_list, ignore_index=True)
        
        # è°ƒæ•´åˆ—é¡ºåº
        cols = [
            "ID", "Group", "folder", "q",
            "RR-2D-xy", "RR-1D-x", "DET-2D-xy", "DET-1D-x", "ENT-2D-xy", "ENT-1D-x"
        ]
        all_data = all_data[cols]
        
        return all_data
        
    except Exception as e:
        print(f"åˆå¹¶æ•°æ®å‡ºé”™: {e}")
        raise


def build_event_aggregates(events_csv_path):
    """æ„é€ äº‹ä»¶çº§èšåˆ"""
    if not os.path.exists(events_csv_path):
        return pd.DataFrame()

    df_evt = pd.read_csv(events_csv_path)
    df_evt["EventType"] = df_evt["EventType"].astype(str).str.lower().str.strip()
    df_evt["ADQ_ID"] = df_evt["ADQ_ID"].astype(str)
    
    # å¤„ç†å¯èƒ½å­˜åœ¨çš„åˆ—åï¼Œå…¼å®¹ä¸åŒçš„æ•°æ®æ ¼å¼
    available_cols = df_evt.columns.tolist()
    
    # æ˜ å°„åˆ—åï¼ˆå¤„ç†ä¸åŒçš„å‘½åçº¦å®šï¼‰
    col_mapping = {
        'Duration_ms': ['Duration_ms', 'duration_ms', 'Duration'],
        'Amplitude': ['Amplitude_deg', 'SaccadeAmplitude', 'amplitude', 'Amplitude'],
        'MaxVel': ['MaxVel', 'SaccadeMaxVel', 'max_vel', 'MaxVelocity']
    }
    
    # æ‰¾åˆ°å®é™…å­˜åœ¨çš„åˆ—å
    actual_cols = {}
    for key, possible_names in col_mapping.items():
        for name in possible_names:
            if name in available_cols:
                actual_cols[key] = name
                break
        if key not in actual_cols:
            actual_cols[key] = None
    
    # æ•°å€¼åŒ–å¤„ç†å­˜åœ¨çš„åˆ—
    for key, col_name in actual_cols.items():
        if col_name and col_name in df_evt.columns:
            df_evt[col_name] = pd.to_numeric(df_evt[col_name], errors="coerce")

    # fixationèšåˆ
    fix_data = df_evt[df_evt["EventType"] == "fixation"].copy()
    if not fix_data.empty and actual_cols['Duration_ms']:
        fix_agg = fix_data.groupby("ADQ_ID").agg({actual_cols['Duration_ms']: "sum"})
        fix_agg["FixCount"] = fix_data.groupby("ADQ_ID")[actual_cols['Duration_ms']].count()
        fix_agg.rename(columns={actual_cols['Duration_ms']: "FixDurSum"}, inplace=True)
        fix_agg.reset_index(inplace=True)
    else:
        # å¦‚æœæ²¡æœ‰fixationæ•°æ®æˆ–Durationåˆ—ï¼Œåˆ›å»ºç©ºçš„èšåˆ
        fix_agg = pd.DataFrame(columns=["ADQ_ID", "FixDurSum", "FixCount"])

    # saccadeèšåˆ
    sacc_data = df_evt[df_evt["EventType"] == "saccade"].copy()
    if not sacc_data.empty and actual_cols['Amplitude'] and actual_cols['MaxVel']:
        sacc_agg = sacc_data.groupby("ADQ_ID").agg({
            actual_cols['Amplitude']: "mean",
            actual_cols['MaxVel']: "max"
        })
        sacc_agg["SaccCount"] = sacc_data.groupby("ADQ_ID")[actual_cols['Amplitude']].count()
        sacc_agg.rename(columns={
            actual_cols['Amplitude']: "SaccAmpMean",
            actual_cols['MaxVel']: "SaccMaxVelPeak"
        }, inplace=True)
        sacc_agg.reset_index(inplace=True)
    else:
        # å¦‚æœæ²¡æœ‰saccadeæ•°æ®æˆ–ç›¸å…³åˆ—ï¼Œåˆ›å»ºç©ºçš„èšåˆ
        sacc_agg = pd.DataFrame(columns=["ADQ_ID", "SaccAmpMean", "SaccCount", "SaccMaxVelPeak"])

    # åˆå¹¶
    if not fix_agg.empty and not sacc_agg.empty:
        df_evt_agg = pd.merge(fix_agg, sacc_agg, on="ADQ_ID", how="outer")
    elif not fix_agg.empty:
        df_evt_agg = fix_agg
        # æ·»åŠ ç¼ºå¤±çš„saccadeåˆ—
        for col in ["SaccAmpMean", "SaccCount", "SaccMaxVelPeak"]:
            df_evt_agg[col] = 0
    elif not sacc_agg.empty:
        df_evt_agg = sacc_agg
        # æ·»åŠ ç¼ºå¤±çš„fixationåˆ—
        for col in ["FixDurSum", "FixCount"]:
            df_evt_agg[col] = 0
    else:
        # å¦‚æœéƒ½æ²¡æœ‰æ•°æ®ï¼Œè¿”å›ç©ºDataFrame
        return pd.DataFrame()
    
    # å¡«å……ç¼ºå¤±å€¼
    for c in ["FixDurSum", "FixCount", "SaccAmpMean", "SaccCount", "SaccMaxVelPeak"]:
        if c in df_evt_agg.columns:
            df_evt_agg[c] = df_evt_agg[c].fillna(0)

    return df_evt_agg


def build_roi_aggregates(roi_csv_path):
    """æ„é€ ROIçº§èšåˆ"""
    if not os.path.exists(roi_csv_path):
        return pd.DataFrame()

    df_roi = pd.read_csv(roi_csv_path)
    df_roi["ADQ_ID"] = df_roi["ADQ_ID"].astype(str)
    
    # å¤„ç†å¯èƒ½å­˜åœ¨çš„åˆ—åï¼Œå…¼å®¹ä¸åŒçš„æ•°æ®æ ¼å¼
    available_cols = df_roi.columns.tolist()
    
    col_mapping = {
        'RegressionCount': ['RegressionCount', 'regression_count', 'RegCount'],
        'FixationDuration': ['FixationDuration', 'FixTime', 'fixation_duration', 'FixDur']
    }
    
    # æ‰¾åˆ°å®é™…å­˜åœ¨çš„åˆ—å
    actual_cols = {}
    for key, possible_names in col_mapping.items():
        for name in possible_names:
            if name in available_cols:
                actual_cols[key] = name
                break
        if key not in actual_cols:
            actual_cols[key] = None

    # æ•°å€¼åŒ–å¤„ç†å­˜åœ¨çš„åˆ—
    for key, col_name in actual_cols.items():
        if col_name and col_name in df_roi.columns:
            df_roi[col_name] = pd.to_numeric(df_roi[col_name], errors="coerce")

    # æ‰§è¡Œèšåˆæ“ä½œï¼ˆå¦‚æœç›¸å…³åˆ—å­˜åœ¨ï¼‰
    agg_dict = {}
    if actual_cols['RegressionCount']:
        agg_dict[actual_cols['RegressionCount']] = "sum"
    if actual_cols['FixationDuration']:
        agg_dict[actual_cols['FixationDuration']] = "sum"

    if agg_dict:
        grp_agg = df_roi.groupby("ADQ_ID").agg(agg_dict)
        
        # é‡å‘½ååˆ—ä¸ºæ ‡å‡†åŒ–åç§°
        if actual_cols['RegressionCount'] and actual_cols['RegressionCount'] in grp_agg.columns:
            grp_agg.rename(columns={actual_cols['RegressionCount']: "RegCountSum"}, inplace=True)
        else:
            grp_agg["RegCountSum"] = 0 # å¦‚æœç¼ºå¤±åˆ™æ·»åŠ 
            
        if actual_cols['FixationDuration'] and actual_cols['FixationDuration'] in grp_agg.columns:
            grp_agg.rename(columns={actual_cols['FixationDuration']: "ROIFixDurSum"}, inplace=True)
        else:
            grp_agg["ROIFixDurSum"] = 0 # å¦‚æœç¼ºå¤±åˆ™æ·»åŠ 

        grp_agg.reset_index(inplace=True)
    else:
        # å¦‚æœæ²¡æœ‰ç›¸å…³åˆ—ç”¨äºèšåˆï¼Œè¿”å›å¸¦æœ‰æœŸæœ›åˆ—çš„ç©ºDataFrame
        grp_agg = pd.DataFrame(columns=["ADQ_ID", "RegCountSum", "ROIFixDurSum"])

    # å¡«å……ä»»ä½•å‰©ä½™çš„NaNå€¼ä¸º0
    for c in ["RegCountSum", "ROIFixDurSum"]:
        if c in grp_agg.columns:
            grp_agg[c] = grp_agg[c].fillna(0)

    return grp_agg


###############################################################################
# å¯è§†åŒ–å‡½æ•°
###############################################################################

def create_group_bar_charts(df, metrics=["RR-2D-xy", "DET-2D-xy", "ENT-2D-xy"]):
    """åˆ›å»ºç»„çº§æ¡å½¢å›¾"""
    colors = {'Control': '#ADD8E6', 'MCI': '#FFB6A4', 'AD': '#98FB98'}
    charts = []
    
    for metric in metrics:
        if metric not in df.columns:
            continue
            
        try:
            plt.figure(figsize=(8, 6))
            
            # ç§»é™¤NaNå€¼
            df_clean = df.dropna(subset=[metric])
            if df_clean.empty:
                print(f"è­¦å‘Š: {metric} åˆ—ä¸­æ²¡æœ‰æœ‰æ•ˆæ•°æ®")
                continue
            
            # è®¡ç®—ç»„åˆ«ç»Ÿè®¡
            group_stats = df_clean.groupby('Group')[metric].agg(['mean', 'std']).reset_index()
            
            # å¤„ç†NaNå€¼
            group_stats['mean'] = group_stats['mean'].fillna(0)
            group_stats['std'] = group_stats['std'].fillna(0)
            
            # ç»˜åˆ¶æ¡å½¢å›¾
            bars = plt.bar(group_stats['Group'], group_stats['mean'], 
                          color=[colors.get(g, '#cccccc') for g in group_stats['Group']],
                          yerr=group_stats['std'], capsize=5, alpha=0.8)
            
            plt.title(f'Group-level {metric} (mean Â± std)', fontsize=14, fontweight='bold')
            plt.xlabel('Group', fontsize=12, fontweight='bold')
            plt.ylabel(f'{metric} Value', fontsize=12, fontweight='bold')
            plt.grid(True, alpha=0.3)
            
            # æ·»åŠ æ•°å€¼æ ‡ç­¾
            for i, (bar, mean_val, std_val) in enumerate(zip(bars, group_stats['mean'], group_stats['std'])):
                if not (np.isnan(mean_val) or np.isnan(std_val)):
                    height = bar.get_height()
                    plt.text(bar.get_x() + bar.get_width()/2., height + std_val + height*0.01,
                            f'{mean_val:.4f}', ha='center', va='bottom', fontweight='bold')
            
            plt.tight_layout()
            
            # ä¿å­˜ä¸ºbase64
            buffer = io.BytesIO()
            plt.savefig(buffer, format='png', dpi=150, bbox_inches='tight', facecolor='white')
            buffer.seek(0)
            image_base64 = base64.b64encode(buffer.getvalue()).decode()
            buffer.close()
            plt.close()
            
            charts.append({
                'title': f'{metric} ç»„åˆ«å¯¹æ¯”',
                'metric': metric,
                'image': image_base64
            })
            
        except Exception as e:
            print(f"ç”Ÿæˆ {metric} æ¡å½¢å›¾æ—¶å‡ºé”™: {e}")
            plt.close()  # ç¡®ä¿å…³é—­å›¾å½¢
            continue
    
    return charts


def create_task_trend_chart(df, metric="RR-2D-xy"):
    """åˆ›å»ºä»»åŠ¡é—´å˜åŒ–æŠ˜çº¿å›¾ - Average RR (2D-xy) across tasks by Group"""
    colors = {'Control': '#4472C4', 'MCI': '#E15759', 'AD': '#70AD47'}  # æ›´æ¸…æ™°çš„é¢œè‰²
    
    try:
        print(f"ğŸ“Š å¼€å§‹ç”Ÿæˆ {metric} è¶‹åŠ¿å›¾...")
        
        # æ£€æŸ¥å¿…è¦çš„åˆ—æ˜¯å¦å­˜åœ¨
        required_cols = [metric, 'Group', 'q']
        missing_cols = [col for col in required_cols if col not in df.columns]
        if missing_cols:
            print(f"âŒ ç¼ºå°‘å¿…è¦åˆ—: {missing_cols}")
            return None
        
        print(f"âœ… æ•°æ®åˆ—æ£€æŸ¥é€šè¿‡ï¼ŒåŸå§‹æ•°æ®å½¢çŠ¶: {df.shape}")
        print(f"ğŸ“ˆ åŒ…å«çš„ç»„: {df['Group'].unique()}")
        print(f"ğŸ“Š åŒ…å«çš„qå€¼: {sorted(df['q'].unique())}")
        
        # ç§»é™¤NaNå€¼
        df_clean = df.dropna(subset=required_cols)
        print(f"ğŸ§¹ æ¸…ç†åæ•°æ®å½¢çŠ¶: {df_clean.shape}")
        
        if df_clean.empty:
            print(f"âŒ è­¦å‘Š: æ¸…ç†åçš„æ•°æ®ä¸ºç©ºï¼Œæ— æ³•ç”Ÿæˆè¶‹åŠ¿å›¾")
            return None
        
        # æ£€æŸ¥æ¯ä¸ªç»„çš„æ•°æ®
        for group in ['Control', 'MCI', 'AD']:
            group_data = df_clean[df_clean['Group'] == group]
            print(f"ğŸ‘¥ ç»„ {group}: {len(group_data)} æ¡è®°å½•")
            if not group_data.empty:
                print(f"   - qå€¼èŒƒå›´: {sorted(group_data['q'].unique())}")
                print(f"   - {metric}å€¼èŒƒå›´: {group_data[metric].min():.6f} - {group_data[metric].max():.6f}")
        
        # å¼€å§‹åˆ›å»ºå›¾è¡¨
        plt.figure(figsize=(12, 8))
        
        # è®¡ç®—æ¯ä¸ªç»„åœ¨æ¯ä¸ªqå€¼çš„ç»Ÿè®¡
        print("ğŸ“Š å¼€å§‹è®¡ç®—ç»„çº§ç»Ÿè®¡...")
        avg_by_group = df_clean.groupby(['Group', 'q'])[metric].agg(['mean', 'std', 'count']).reset_index()
        print(f"ğŸ“‹ èšåˆåæ•°æ®å½¢çŠ¶: {avg_by_group.shape}")
        print(f"ğŸ“‹ èšåˆæ•°æ®é¢„è§ˆ:\n{avg_by_group.head(10)}")
        
        # å¤„ç†NaNå€¼
        avg_by_group['mean'] = avg_by_group['mean'].fillna(0)
        avg_by_group['std'] = avg_by_group['std'].fillna(0)
        
        lines_plotted = 0
        legend_labels = []
        
        # ä¸ºæ¯ä¸ªç»„ç”»çº¿
        for group in ['Control', 'MCI', 'AD']:
            if group not in avg_by_group['Group'].values:
                print(f"âš ï¸  è·³è¿‡ç»„ {group}ï¼šæ•°æ®ä¸­ä¸å­˜åœ¨")
                continue
                
            group_data = avg_by_group[avg_by_group['Group'] == group].sort_values('q')
            if group_data.empty:
                print(f"âš ï¸  è·³è¿‡ç»„ {group}ï¼šæ— æœ‰æ•ˆæ•°æ®")
                continue
            
            # è®¡ç®—æ€»æ ·æœ¬æ•°ï¼ˆæ‰€æœ‰qå€¼çš„å¹³å‡ï¼‰
            total_count = group_data['count'].mean()
            print(f"ğŸ¨ ç»˜åˆ¶ç»„ {group}ï¼Œæ•°æ®ç‚¹æ•°: {len(group_data)}ï¼Œå¹³å‡æ ·æœ¬æ•°: {total_count:.1f}")
            
            # ç»˜åˆ¶ä¸»çº¿
            line = plt.plot(group_data['q'], group_data['mean'], 
                    marker='o', label=f'{group} (nâ‰ˆ{total_count:.0f})', 
                    color=colors[group], linewidth=3, markersize=8, 
                    markerfacecolor='white', markeredgewidth=2, markeredgecolor=colors[group])
            
            # æ·»åŠ æ ‡å‡†å·®åŒºåŸŸ
            plt.fill_between(group_data['q'],
                            group_data['mean'] - group_data['std'],
                            group_data['mean'] + group_data['std'],
                            color=colors[group], alpha=0.2)
            
            lines_plotted += 1
            legend_labels.append(f'{group} (nâ‰ˆ{total_count:.0f})')
            
            # æ‰“å°æ¯ä¸ªæ•°æ®ç‚¹çš„è¯¦ç»†ä¿¡æ¯
            for _, row in group_data.iterrows():
                print(f"   Q{int(row['q'])}: mean={row['mean']:.6f}, std={row['std']:.6f}, count={row['count']}")
        
        if lines_plotted == 0:
            print(f"âŒ æ²¡æœ‰ä»»ä½•ç»„çš„æ•°æ®å¯ä»¥ç»˜åˆ¶")
            plt.close()
            return None
        
        print(f"âœ… æˆåŠŸç»˜åˆ¶äº† {lines_plotted} ä¸ªç»„çš„è¶‹åŠ¿çº¿")
        
        # è®¾ç½®å›¾è¡¨æ ·å¼
        plt.title(f"Average {metric} across tasks by Group", fontsize=16, fontweight='bold', pad=20)
        plt.xlabel("Task (Q)", fontsize=14, fontweight='bold')
        plt.ylabel(f"Mean {metric} Value", fontsize=14, fontweight='bold')
        
        # è®¾ç½®xè½´åˆ»åº¦
        all_q_values = sorted(df_clean['q'].unique())
        plt.xticks(all_q_values, [f'Q{int(q)}' for q in all_q_values])
        
        # è®¾ç½®ç½‘æ ¼
        plt.grid(True, alpha=0.3, linestyle='--')
        
        # æ˜¾ç¤ºå›¾ä¾‹
        legend = plt.legend(title="Cognitive Groups", 
                          loc='best', frameon=True, fancybox=True, shadow=True)
        # è®¾ç½®å›¾ä¾‹æ ‡é¢˜åŠ ç²—
        legend.get_title().set_fontweight('bold')
        
        # æ ¼å¼åŒ–yè½´
        plt.ticklabel_format(style='scientific', axis='y', scilimits=(0,0))
        
        plt.tight_layout()
        
        # ä¿å­˜ä¸ºbase64
        buffer = io.BytesIO()
        plt.savefig(buffer, format='png', dpi=150, bbox_inches='tight', facecolor='white')
        buffer.seek(0)
        image_base64 = base64.b64encode(buffer.getvalue()).decode()
        buffer.close()
        plt.close()
        
        print(f"âœ… æˆåŠŸç”Ÿæˆ {metric} è¶‹åŠ¿å›¾")
        return {
            'title': f'Average {metric} across tasks by Group',
            'metric': metric,
            'image': image_base64,
            'description': f'è·¨ä»»åŠ¡çš„å¹³å‡{metric}å˜åŒ–è¶‹åŠ¿ (æŒ‰è®¤çŸ¥ç»„åˆ†ç»„)'
        }
        
    except Exception as e:
        print(f"âŒ ç”Ÿæˆ {metric} è¶‹åŠ¿å›¾æ—¶å‡ºé”™: {e}")
        import traceback
        traceback.print_exc()
        plt.close()  # ç¡®ä¿å…³é—­å›¾å½¢
        return None


###############################################################################
# APIè·¯ç”±
###############################################################################

@rqa_pipeline_bp.route('/api/rqa-pipeline/calculate', methods=['POST'])
def rqa_calculate():
    """æ­¥éª¤1: RQAè®¡ç®—"""
    try:
        data = request.get_json()
        parameters = data.get('parameters', {})
        
        # é»˜è®¤å‚æ•°
        m = parameters.get('m', 2)
        delay = parameters.get('delay', parameters.get('tau', 1))  # å…¼å®¹tauå’Œdelay
        eps = parameters.get('eps', 0.05)
        lmin = parameters.get('lmin', 2)
        
        # è·å–å‚æ•°å¯¹åº”çš„ç›®å½•
        step_dir = get_step_directory(parameters, 'step1_rqa_calculation')
        
        # æ•°æ®ç›®å½• - éœ€è¦æ ¹æ®å®é™…æƒ…å†µè°ƒæ•´
        data_dirs = [
            'data/control_calibrated',
            'data/mci_calibrated', 
            'data/ad_calibrated'
        ]
        
        results = []
        
        # å¤„ç†æ‰€æœ‰æ•°æ®ç›®å½•
        for data_dir in data_dirs:
            if os.path.exists(data_dir):
                for root, dirs, files in os.walk(data_dir):
                    for file in files:
                        if file.endswith('_calibrated.csv'):
                            csv_path = os.path.join(root, file)
                            result = process_single_rqa_file(csv_path, m, delay, eps, lmin)
                            if result:
                                results.append(result)
        
        # æ ¹æ®ç»„åˆ«ä¿å­˜ç»“æœåˆ°å‚æ•°ç‰¹å®šç›®å½•
        control_results = [r for r in results if r['filename'].startswith('n')]
        mci_results = [r for r in results if r['filename'].startswith('m')]
        ad_results = [r for r in results if r['filename'].startswith('ad')]
        
        # ä¿å­˜åˆ°å‚æ•°ç‰¹å®šç›®å½•çš„CSVæ–‡ä»¶
        if control_results:
            control_df = pd.DataFrame(control_results)
            control_path = os.path.join(step_dir, 'RQA_1D2D_summary_control.csv')
            control_df.to_csv(control_path, index=False)
        
        if mci_results:
            mci_df = pd.DataFrame(mci_results)
            mci_path = os.path.join(step_dir, 'RQA_1D2D_summary_mci.csv')
            mci_df.to_csv(mci_path, index=False)
        
        if ad_results:
            ad_df = pd.DataFrame(ad_results)
            ad_path = os.path.join(step_dir, 'RQA_1D2D_summary_ad.csv')
            ad_df.to_csv(ad_path, index=False)
        
        # ä¿å­˜å‚æ•°å…ƒæ•°æ®
        save_param_metadata(parameters, 1)
        
        return jsonify({
            'status': 'success',
            'message': 'RQAè®¡ç®—å®Œæˆ',
            'data': {
                'param_signature': generate_param_signature(parameters),
                'total_files': len(results),
                'control_files': len(control_results),
                'mci_files': len(mci_results),
                'ad_files': len(ad_results),
                'output_directory': step_dir
            }
        })
        
    except Exception as e:
        print(f"RQAè®¡ç®—é”™è¯¯: {e}")
        traceback.print_exc()
        return jsonify({
            'status': 'error',
            'message': f'RQAè®¡ç®—å¤±è´¥: {str(e)}'
        }), 500


@rqa_pipeline_bp.route('/api/rqa-pipeline/merge', methods=['POST'])
def data_merge():
    """æ­¥éª¤2: æ•°æ®åˆå¹¶"""
    try:
        data = request.get_json()
        parameters = data.get('parameters', {})
        
        # è·å–å‚æ•°å¯¹åº”çš„ç›®å½•
        step1_dir = get_step_directory(parameters, 'step1_rqa_calculation')
        step2_dir = get_step_directory(parameters, 'step2_data_merging')
        
        # ä»æ­¥éª¤1çš„ç»“æœè¯»å–æ•°æ®
        control_path = os.path.join(step1_dir, 'RQA_1D2D_summary_control.csv')
        mci_path = os.path.join(step1_dir, 'RQA_1D2D_summary_mci.csv')
        ad_path = os.path.join(step1_dir, 'RQA_1D2D_summary_ad.csv')
        
        # åˆå¹¶ä¸‰ç»„æ•°æ®
        merged_data = merge_group_data(control_path, mci_path, ad_path)
        
        # ä¿å­˜åˆå¹¶ç»“æœåˆ°æ­¥éª¤2ç›®å½•
        output_path = os.path.join(step2_dir, 'All_Subjects_RQA_EyeMetrics.csv')
        merged_data.to_csv(output_path, index=False)
        
        # ä¿å­˜å‚æ•°å…ƒæ•°æ®
        save_param_metadata(parameters, 2)
        
        return jsonify({
            'status': 'success',
            'message': 'æ•°æ®åˆå¹¶å®Œæˆ',
            'data': {
                'param_signature': generate_param_signature(parameters),
                'output_file': output_path,
                'total_records': len(merged_data),
                'groups': merged_data['Group'].value_counts().to_dict()
            }
        })
        
    except Exception as e:
        print(f"æ•°æ®åˆå¹¶é”™è¯¯: {e}")
        traceback.print_exc()
        return jsonify({
            'status': 'error',
            'message': f'æ•°æ®åˆå¹¶å¤±è´¥: {str(e)}'
        }), 500


@rqa_pipeline_bp.route('/api/rqa-pipeline/enrich', methods=['POST'])
def feature_enrichment():
    """æ­¥éª¤3: ç‰¹å¾è¡¥å……"""
    try:
        data = request.get_json()
        parameters = data.get('parameters', {})
        
        # è·å–å‚æ•°å¯¹åº”çš„ç›®å½•
        step2_dir = get_step_directory(parameters, 'step2_data_merging')
        step3_dir = get_step_directory(parameters, 'step3_feature_enrichment')
        
        # è¯»å–åŸºç¡€RQAæ•°æ®
        rqa_path = os.path.join(step2_dir, 'All_Subjects_RQA_EyeMetrics.csv')
        if not os.path.exists(rqa_path):
            raise FileNotFoundError("æœªæ‰¾åˆ°RQAæ•°æ®æ–‡ä»¶ï¼Œè¯·å…ˆæ‰§è¡Œæ•°æ®åˆå¹¶æ­¥éª¤")
        
        df_rqa = pd.read_csv(rqa_path, dtype={"ID": str})
        df_rqa.rename(columns={"ID": "ADQ_ID"}, inplace=True)
        df_rqa["ADQ_ID"] = df_rqa["ADQ_ID"].str.replace(r"\.0$", "", regex=True)
        
        # æ„å»ºäº‹ä»¶èšåˆ
        events_path = 'data/event_analysis_results/All_Events.csv'
        df_evt_agg = build_event_aggregates(events_path)
        
        # æ„å»ºROIèšåˆ
        roi_path = 'data/event_analysis_results/All_ROI_Summary.csv'
        df_roi_agg = build_roi_aggregates(roi_path)
        
        # åˆå¹¶ç‰¹å¾
        if not df_evt_agg.empty and not df_roi_agg.empty:
            df_agg = pd.merge(df_evt_agg, df_roi_agg, on="ADQ_ID", how="outer")
        elif not df_evt_agg.empty:
            df_agg = df_evt_agg
        elif not df_roi_agg.empty:
            df_agg = df_roi_agg
        else:
            df_agg = pd.DataFrame()
        
        # åˆå¹¶åˆ°RQAæ•°æ®
        if not df_agg.empty:
            df_final = pd.merge(df_rqa, df_agg, on="ADQ_ID", how="left")
        else:
            df_final = df_rqa
        
        # æ¢å¤IDåˆ—å
        df_final.rename(columns={"ADQ_ID": "ID"}, inplace=True)
        
        # ä¿å­˜ç»“æœåˆ°æ­¥éª¤3ç›®å½•
        output_path = os.path.join(step3_dir, 'All_Subjects_RQA_EyeMetrics_Filled.csv')
        df_final.to_csv(output_path, index=False)
        
        # ä¿å­˜å‚æ•°å…ƒæ•°æ®
        save_param_metadata(parameters, 3)
        
        return jsonify({
            'status': 'success',
            'message': 'ç‰¹å¾è¡¥å……å®Œæˆ',
            'data': {
                'param_signature': generate_param_signature(parameters),
                'output_file': output_path,
                'total_records': len(df_final),
                'added_features': list(df_agg.columns) if not df_agg.empty else []
            }
        })
        
    except Exception as e:
        print(f"ç‰¹å¾è¡¥å……é”™è¯¯: {e}")
        traceback.print_exc()
        return jsonify({
            'status': 'error',
            'message': f'ç‰¹å¾è¡¥å……å¤±è´¥: {str(e)}'
        }), 500


@rqa_pipeline_bp.route('/api/rqa-pipeline/analyze', methods=['POST'])
def statistical_analysis():
    """æ­¥éª¤4: ç»Ÿè®¡åˆ†æ"""
    try:
        data = request.get_json()
        parameters = data.get('parameters', {})
        
        # è·å–å‚æ•°å¯¹åº”çš„ç›®å½•
        step3_dir = get_step_directory(parameters, 'step3_feature_enrichment')
        step4_dir = get_step_directory(parameters, 'step4_statistical_analysis')
        
        # è¯»å–å¡«å……åçš„æ•°æ®
        filled_path = os.path.join(step3_dir, 'All_Subjects_RQA_EyeMetrics_Filled.csv')
        if not os.path.exists(filled_path):
            raise FileNotFoundError("æœªæ‰¾åˆ°å¡«å……æ•°æ®æ–‡ä»¶ï¼Œè¯·å…ˆæ‰§è¡Œç‰¹å¾è¡¥å……æ­¥éª¤")
        
        df = pd.read_csv(filled_path)
        
        # RQAæŒ‡æ ‡
        rqa_vars = ["RR-2D-xy", "DET-2D-xy", "ENT-2D-xy"]
        
        # ç»„çº§ç»Ÿè®¡
        group_stats = df.groupby("Group")[rqa_vars].describe()
        group_stats_path = os.path.join(step4_dir, 'group_stats_output.csv')
        group_stats.to_csv(group_stats_path)
        
        # å¤šå±‚æ¬¡ç»Ÿè®¡
        multi_level_stats = df.groupby(["Group", "folder", "q"])[rqa_vars].agg(["mean", "std"])
        multi_level_path = os.path.join(step4_dir, 'multi_level_stats_output.csv')
        multi_level_stats.to_csv(multi_level_path)
        
        # å‡†å¤‡è¿”å›æ•°æ®
        group_summary = []
        for group in df['Group'].unique():
            group_data = df[df['Group'] == group]
            group_summary.append({
                'Group': group,
                'Count': len(group_data),
                'RR_mean': group_data['RR-2D-xy'].mean(),
                'RR_std': group_data['RR-2D-xy'].std(),
                'DET_mean': group_data['DET-2D-xy'].mean(),
                'DET_std': group_data['DET-2D-xy'].std(),
                'ENT_mean': group_data['ENT-2D-xy'].mean(),
                'ENT_std': group_data['ENT-2D-xy'].std(),
            })
        
        # ä¿å­˜å‚æ•°å…ƒæ•°æ®
        save_param_metadata(parameters, 4)
        
        return jsonify({
            'status': 'success',
            'message': 'ç»Ÿè®¡åˆ†æå®Œæˆ',
            'data': {
                'param_signature': generate_param_signature(parameters),
                'group_stats_file': group_stats_path,
                'multi_level_stats_file': multi_level_path,
                'group_summary': group_summary
            }
        })
        
    except Exception as e:
        print(f"ç»Ÿè®¡åˆ†æé”™è¯¯: {e}")
        traceback.print_exc()
        return jsonify({
            'status': 'error',
            'message': f'ç»Ÿè®¡åˆ†æå¤±è´¥: {str(e)}'
        }), 500


@rqa_pipeline_bp.route('/api/rqa-pipeline/visualize', methods=['POST'])
def create_visualization():
    """æ­¥éª¤5: å¯è§†åŒ–"""
    try:
        data = request.get_json()
        parameters = data.get('parameters', {})
        
        print(f"å¼€å§‹å¯è§†åŒ–æ­¥éª¤ï¼Œå‚æ•°: {parameters}")
        
        # è·å–å‚æ•°å¯¹åº”çš„ç›®å½•
        step4_dir = get_step_directory(parameters, 'step4_statistical_analysis')
        step5_dir = get_step_directory(parameters, 'step5_visualization')
        
        print(f"å¯è§†åŒ–è¾“å‡ºç›®å½•: {step5_dir}")
        
        # è¯»å–å¡«å……åçš„æ•°æ®ï¼ˆä»æ­¥éª¤3ç›®å½•ï¼‰
        step3_dir = get_step_directory(parameters, 'step3_feature_enrichment')
        filled_path = os.path.join(step3_dir, 'All_Subjects_RQA_EyeMetrics_Filled.csv')
        if not os.path.exists(filled_path):
            raise FileNotFoundError("æœªæ‰¾åˆ°å¡«å……æ•°æ®æ–‡ä»¶ï¼Œè¯·å…ˆæ‰§è¡Œç‰¹å¾è¡¥å……æ­¥éª¤")
        
        print(f"è¯»å–æ•°æ®æ–‡ä»¶: {filled_path}")
        df = pd.read_csv(filled_path)
        print(f"æ•°æ®å½¢çŠ¶: {df.shape}")
        print(f"æ•°æ®åˆ—: {df.columns.tolist()}")
        print(f"æ•°æ®ç»„åˆ«: {df['Group'].value_counts()}")
        print(f"æ•°æ®qå€¼: {df['q'].value_counts()}")
        
        # ç”Ÿæˆæ¡å½¢å›¾
        print("\n=== å¼€å§‹ç”Ÿæˆç»„çº§æ¡å½¢å›¾ ===")
        bar_charts = create_group_bar_charts(df, ["RR-2D-xy", "DET-2D-xy", "ENT-2D-xy"])
        print(f"æˆåŠŸç”Ÿæˆ {len(bar_charts)} ä¸ªæ¡å½¢å›¾")
        
        # ä¿å­˜æ¡å½¢å›¾åˆ°æ–‡ä»¶
        for i, chart in enumerate(bar_charts):
            chart_filename = f"bar_chart_{chart['metric'].replace('-', '_')}.png"
            chart_path = os.path.join(step5_dir, chart_filename)
            
            # è§£ç base64å¹¶ä¿å­˜å›¾ç‰‡
            image_data = base64.b64decode(chart['image'])
            with open(chart_path, 'wb') as f:
                f.write(image_data)
            print(f"ä¿å­˜æ¡å½¢å›¾: {chart_path}")
        
        # ç”ŸæˆæŠ˜çº¿å›¾ï¼šAverage RR (2D-xy) across tasks by Group
        print("\n=== å¼€å§‹ç”Ÿæˆä»»åŠ¡é—´å˜åŒ–è¶‹åŠ¿å›¾ ===")
        trend_chart = create_task_trend_chart(df, "RR-2D-xy")
        
        # ç»„åˆæ‰€æœ‰å›¾è¡¨
        all_charts = bar_charts[:]  # å¤åˆ¶æ¡å½¢å›¾åˆ—è¡¨
        if trend_chart:  # åªæœ‰åœ¨æˆåŠŸç”Ÿæˆæ—¶æ‰æ·»åŠ 
            all_charts.append(trend_chart)
            print("âœ… è¶‹åŠ¿å›¾å·²æˆåŠŸæ·»åŠ åˆ°å›¾è¡¨åˆ—è¡¨")
            
            # ä¿å­˜è¶‹åŠ¿å›¾åˆ°æ–‡ä»¶
            trend_filename = f"trend_chart_{trend_chart['metric'].replace('-', '_')}.png"
            trend_path = os.path.join(step5_dir, trend_filename)
            
            # è§£ç base64å¹¶ä¿å­˜å›¾ç‰‡
            image_data = base64.b64decode(trend_chart['image'])
            with open(trend_path, 'wb') as f:
                f.write(image_data)
            print(f"ä¿å­˜è¶‹åŠ¿å›¾: {trend_path}")
        else:
            print("âŒ è­¦å‘Šï¼šè¶‹åŠ¿å›¾ç”Ÿæˆå¤±è´¥ï¼Œå°†è·³è¿‡")
            print("æ£€æŸ¥æ•°æ®æ˜¯å¦åŒ…å«å¿…è¦çš„åˆ—å’Œæœ‰æ•ˆå€¼...")
            print(f"RR-2D-xyåˆ—å­˜åœ¨: {'RR-2D-xy' in df.columns}")
            print(f"Groupåˆ—å­˜åœ¨: {'Group' in df.columns}")
            print(f"qåˆ—å­˜åœ¨: {'q' in df.columns}")
            if 'RR-2D-xy' in df.columns:
                print(f"RR-2D-xyéç©ºå€¼æ•°é‡: {df['RR-2D-xy'].notna().sum()}")
                print(f"RR-2D-xyç»Ÿè®¡: {df['RR-2D-xy'].describe()}")
        
        print(f"\n=== æ€»å…±ç”Ÿæˆäº† {len(all_charts)} ä¸ªå›¾è¡¨ ===")
        
        # ä¿å­˜å›¾è¡¨åˆ—è¡¨åˆ°JSONæ–‡ä»¶
        charts_file = os.path.join(step5_dir, 'visualization_charts.json')
        with open(charts_file, 'w', encoding='utf-8') as f:
            json.dump(all_charts, f, indent=2, ensure_ascii=False)
        print(f"ä¿å­˜å›¾è¡¨JSON: {charts_file}")
        
        # å‡†å¤‡ç»„åˆ«ç»Ÿè®¡æ•°æ®
        print("\n=== è®¡ç®—ç»„åˆ«ç»Ÿè®¡æ•°æ® ===")
        group_stats = []
        for group in df['Group'].unique():
            group_data = df[df['Group'] == group]
            print(f"ç»„ {group}: {len(group_data)} æ¡è®°å½•")
            
            # å®‰å…¨çš„ç»Ÿè®¡è®¡ç®—ï¼Œå¤„ç†NaNå€¼
            def safe_float(value):
                return float(value) if not np.isnan(value) else 0.0
            
            group_stats.append({
                'Group': group,
                'RR_mean': safe_float(group_data['RR-2D-xy'].mean()),
                'RR_std': safe_float(group_data['RR-2D-xy'].std()),
                'DET_mean': safe_float(group_data['DET-2D-xy'].mean()),
                'DET_std': safe_float(group_data['DET-2D-xy'].std()),
                'ENT_mean': safe_float(group_data['ENT-2D-xy'].mean()),
                'ENT_std': safe_float(group_data['ENT-2D-xy'].std()),
            })
        
        # ä¿å­˜ç»Ÿè®¡æ•°æ®
        stats_file = os.path.join(step5_dir, 'group_statistics.json')
        with open(stats_file, 'w', encoding='utf-8') as f:
            json.dump(group_stats, f, indent=2, ensure_ascii=False)
        print(f"ä¿å­˜ç»Ÿè®¡æ•°æ®: {stats_file}")
        
        # åˆ—å‡ºç”Ÿæˆçš„æ‰€æœ‰æ–‡ä»¶
        print(f"\n=== ç”Ÿæˆçš„æ–‡ä»¶åˆ—è¡¨ ===")
        for file in os.listdir(step5_dir):
            file_path = os.path.join(step5_dir, file)
            file_size = os.path.getsize(file_path)
            print(f"- {file} ({file_size} bytes)")
        
        # ä¿å­˜å‚æ•°å…ƒæ•°æ®
        save_param_metadata(parameters, 5)
        
        return jsonify({
            'status': 'success',
            'message': 'å¯è§†åŒ–ç”Ÿæˆå®Œæˆ',
            'data': {
                'param_signature': generate_param_signature(parameters),
                'charts': all_charts,
                'group_stats': group_stats,
                'total_charts': len(all_charts),
                'output_directory': step5_dir,
                'generated_files': os.listdir(step5_dir)
            }
        })
        
    except Exception as e:
        print(f"å¯è§†åŒ–é”™è¯¯: {e}")
        traceback.print_exc()
        return jsonify({
            'status': 'error',
            'message': f'å¯è§†åŒ–å¤±è´¥: {str(e)}'
        }), 500


@rqa_pipeline_bp.route('/api/rqa-pipeline/status', methods=['GET'])
def get_pipeline_status():
    """è·å–æµç¨‹çŠ¶æ€"""
    try:
        # è·å–å‚æ•°ï¼ˆå¦‚æœæä¾›ï¼‰
        params = request.args.to_dict()
        
        if params:
            # æ£€æŸ¥ç‰¹å®šå‚æ•°ç»„åˆçš„çŠ¶æ€
            param_dir = get_param_directory(params)
            step_dirs = ['step1_rqa_calculation', 'step2_data_merging', 
                        'step3_feature_enrichment', 'step4_statistical_analysis', 
                        'step5_visualization']
            
            status = {}
            for i, step_dir in enumerate(step_dirs, 1):
                step_path = os.path.join(param_dir, step_dir)
                status[f'step{i}'] = os.path.exists(step_path) and bool(os.listdir(step_path) if os.path.exists(step_path) else False)
        else:
            # æ£€æŸ¥é»˜è®¤ä½ç½®çš„çŠ¶æ€ï¼ˆå…¼å®¹æ—§ç‰ˆæœ¬ï¼‰
            status = {
                'step1': os.path.exists('data/RQA_1D2D_summary_control.csv'),
                'step2': os.path.exists('data/All_Subjects_RQA_EyeMetrics.csv'),
                'step3': os.path.exists('data/All_Subjects_RQA_EyeMetrics_Filled.csv'),
                'step4': os.path.exists('data/group_stats_output.csv'),
                'step5': os.path.exists('data/multi_level_stats_output.csv'),
            }
        
        completed_steps = sum(status.values())
        progress = (completed_steps / 5) * 100
        
        return jsonify({
            'status': 'success',
            'data': {
                'step_status': status,
                'completed_steps': completed_steps,
                'total_steps': 5,
                'progress_percentage': progress,
                'param_signature': generate_param_signature(params) if params else None
            }
        })
        
    except Exception as e:
        return jsonify({
            'status': 'error',
            'message': f'è·å–çŠ¶æ€å¤±è´¥: {str(e)}'
        }), 500


@rqa_pipeline_bp.route('/api/rqa-pipeline/param-history', methods=['GET'])
def get_param_history_api():
    """è·å–å‚æ•°å†å²è®°å½•"""
    try:
        history = get_param_history()
        
        return jsonify({
            'success': True,
            'history': history,
            'total_records': len(history)
        })
        
    except Exception as e:
        print(f"è·å–å‚æ•°å†å²é”™è¯¯: {e}")
        traceback.print_exc()
        return jsonify({
            'success': False,
            'message': f'è·å–å‚æ•°å†å²å¤±è´¥: {str(e)}'
        }), 500


@rqa_pipeline_bp.route('/api/rqa-pipeline/results/<signature>', methods=['GET'])
def get_param_results(signature):
    """è·å–æŒ‡å®šå‚æ•°ç»„åˆçš„ç»“æœ"""
    try:
        # æ ¹æ®ç­¾åæ‰¾åˆ°å¯¹åº”çš„ç›®å½•
        param_path = os.path.join(PIPELINE_RESULTS_DIR, signature)
        
        if not os.path.exists(param_path):
            return jsonify({
                'status': 'error',
                'message': f'æœªæ‰¾åˆ°å‚æ•°ç»„åˆ {signature} çš„ç»“æœ'
            }), 404
        
        # æ£€æŸ¥å“ªäº›æ­¥éª¤å·²å®Œæˆ
        step_dirs = ['step1_rqa_calculation', 'step2_data_merging', 
                    'step3_feature_enrichment', 'step4_statistical_analysis', 
                    'step5_visualization']
        
        completed_steps = []
        results = {}
        
        for step_dir in step_dirs:
            step_path = os.path.join(param_path, step_dir)
            if os.path.exists(step_path) and os.listdir(step_path):
                completed_steps.append(step_dir)
        
        # å¦‚æœå¯è§†åŒ–æ­¥éª¤å®Œæˆï¼Œè¿”å›å›¾è¡¨æ•°æ®
        if 'step5_visualization' in completed_steps:
            charts_file = os.path.join(param_path, 'step5_visualization', 'visualization_charts.json')
            stats_file = os.path.join(param_path, 'step5_visualization', 'group_statistics.json')
            
            if os.path.exists(charts_file):
                with open(charts_file, 'r', encoding='utf-8') as f:
                    results['charts'] = json.load(f)
                    
            if os.path.exists(stats_file):
                with open(stats_file, 'r', encoding='utf-8') as f:
                    results['group_stats'] = json.load(f)
        
        # è¯»å–å…ƒæ•°æ®
        metadata_file = os.path.join(param_path, 'metadata.json')
        metadata = {}
        if os.path.exists(metadata_file):
            with open(metadata_file, 'r', encoding='utf-8') as f:
                metadata = json.load(f)
        
        return jsonify({
            'status': 'success',
            'data': {
                'signature': signature,
                'metadata': metadata,
                'completed_steps': completed_steps,
                'completed_count': len(completed_steps),
                'total_steps': 5,
                'results': results
            }
        })
        
    except Exception as e:
        print(f"è·å–ç»“æœé”™è¯¯: {e}")
        traceback.print_exc()
        return jsonify({
            'status': 'error',
            'message': f'è·å–ç»“æœå¤±è´¥: {str(e)}'
        }), 500


@rqa_pipeline_bp.route('/api/rqa-pipeline/delete/<signature>', methods=['DELETE'])
def delete_param_results(signature):
    """åˆ é™¤æŒ‡å®šå‚æ•°ç»„åˆçš„ç»“æœ"""
    try:
        param_path = os.path.join(PIPELINE_RESULTS_DIR, signature)
        
        if not os.path.exists(param_path):
            return jsonify({
                'status': 'error',
                'message': f'æœªæ‰¾åˆ°å‚æ•°ç»„åˆ {signature} çš„ç»“æœ'
            }), 404
        
        # åˆ é™¤æ•´ä¸ªå‚æ•°ç›®å½•
        import shutil
        shutil.rmtree(param_path)
        
        return jsonify({
            'status': 'success',
            'message': f'å·²åˆ é™¤å‚æ•°ç»„åˆ {signature} çš„æ‰€æœ‰ç»“æœ'
        })
        
    except Exception as e:
        print(f"åˆ é™¤ç»“æœé”™è¯¯: {e}")
        traceback.print_exc()
        return jsonify({
            'status': 'error',
            'message': f'åˆ é™¤ç»“æœå¤±è´¥: {str(e)}'
        }), 500


###############################################################################
# æ‰¹é‡æ‰§è¡ŒåŠŸèƒ½
###############################################################################

def generate_param_grid(m_range, tau_range, eps_range, lmin_range):
    """
    ç”Ÿæˆå‚æ•°ç½‘æ ¼

    Args:
        m_range: {'start': int, 'end': int, 'step': int}
        tau_range: {'start': int, 'end': int, 'step': int}
        eps_range: {'start': float, 'end': float, 'step': float}
        lmin_range: {'start': int, 'end': int, 'step': int}

    Returns:
        list: å‚æ•°ç»„åˆåˆ—è¡¨ï¼Œæ¯ä¸ªå…ƒç´ ä¸º {'m': int, 'tau': int, 'eps': float, 'lmin': int}
    """
    combinations = []

    # ç”Ÿæˆmå€¼åˆ—è¡¨
    m_values = list(range(m_range['start'], m_range['end'] + 1, m_range['step']))

    # ç”Ÿæˆtauå€¼åˆ—è¡¨
    tau_values = list(range(tau_range['start'], tau_range['end'] + 1, tau_range['step']))

    # ç”Ÿæˆepså€¼åˆ—è¡¨ï¼ˆå¤„ç†æµ®ç‚¹æ•°ç²¾åº¦ï¼‰
    eps_values = []
    current_eps = eps_range['start']
    while current_eps <= eps_range['end'] + 1e-9:  # æ·»åŠ å°å®¹å·®é¿å…æµ®ç‚¹ç²¾åº¦é—®é¢˜
        eps_values.append(round(current_eps, 3))
        current_eps += eps_range['step']

    # ç”Ÿæˆlminå€¼åˆ—è¡¨
    lmin_values = list(range(lmin_range['start'], lmin_range['end'] + 1, lmin_range['step']))

    # ç”Ÿæˆæ‰€æœ‰ç»„åˆ
    for m in m_values:
        for tau in tau_values:
            for eps in eps_values:
                for lmin in lmin_values:
                    combinations.append({
                        'm': m,
                        'tau': tau,
                        'eps': eps,
                        'lmin': lmin
                    })

    return combinations


def execute_full_pipeline_internal(params):
    """
    æ‰§è¡Œå®Œæ•´çš„5æ­¥RQAæµç¨‹ï¼ˆå†…éƒ¨å‡½æ•°ï¼Œç”¨äºæ‰¹å¤„ç†ï¼‰

    Args:
        params: {'m': int, 'tau': int, 'eps': float, 'lmin': int}

    Returns:
        dict: {'success': bool, 'param_signature': str, 'error': str (å¯é€‰)}
    """
    param_signature = generate_param_signature(params)

    try:
        print(f"\n{'='*60}")
        print(f"å¼€å§‹æ‰§è¡Œå‚æ•°ç»„åˆ: {param_signature}")
        print(f"å‚æ•°è¯¦æƒ…: m={params['m']}, Ï„={params['tau']}, Îµ={params['eps']}, l_min={params['lmin']}")
        print(f"{'='*60}\n")

        # æ£€æŸ¥æ˜¯å¦å·²å®Œæˆï¼ˆæ–­ç‚¹ç»­ä¼ ï¼‰
        param_dir = get_param_directory(params)
        metadata_file = os.path.join(param_dir, 'metadata.json')
        if os.path.exists(metadata_file):
            with open(metadata_file, 'r', encoding='utf-8') as f:
                metadata = json.load(f)
            # æ£€æŸ¥æ˜¯å¦æ‰€æœ‰5æ­¥éƒ½å·²å®Œæˆ
            if all(metadata.get(f'step_{i}_completed', False) for i in range(1, 6)):
                print(f"âœ“ å‚æ•°ç»„åˆ {param_signature} å·²å®Œæˆï¼Œè·³è¿‡")
                return {
                    'success': True,
                    'param_signature': param_signature,
                    'skipped': True,
                    'message': 'å·²å­˜åœ¨å®Œæ•´ç»“æœï¼Œå·²è·³è¿‡'
                }

        # Step 1: RQAè®¡ç®—
        print("Step 1/5: RQAè®¡ç®—...")
        step1_dir = get_step_directory(params, 'step1_rqa_calculation')
        data_dirs = [
            'data/control_calibrated',
            'data/mci_calibrated',
            'data/ad_calibrated'
        ]

        results = []
        for data_dir in data_dirs:
            if os.path.exists(data_dir):
                for root, dirs, files in os.walk(data_dir):
                    for file in files:
                        if file.endswith('_calibrated.csv'):
                            csv_path = os.path.join(root, file)
                            result = process_single_rqa_file(csv_path, params['m'], params['tau'], params['eps'], params['lmin'])
                            if result:
                                results.append(result)

        # ä¿å­˜RQAç»“æœ
        control_results = [r for r in results if r['filename'].startswith('n')]
        mci_results = [r for r in results if r['filename'].startswith('m')]
        ad_results = [r for r in results if r['filename'].startswith('ad')]

        if control_results:
            control_df = pd.DataFrame(control_results)
            control_path = os.path.join(step1_dir, 'RQA_1D2D_summary_control.csv')
            control_df.to_csv(control_path, index=False)

        if mci_results:
            mci_df = pd.DataFrame(mci_results)
            mci_path = os.path.join(step1_dir, 'RQA_1D2D_summary_mci.csv')
            mci_df.to_csv(mci_path, index=False)

        if ad_results:
            ad_df = pd.DataFrame(ad_results)
            ad_path = os.path.join(step1_dir, 'RQA_1D2D_summary_ad.csv')
            ad_df.to_csv(ad_path, index=False)

        save_param_metadata(params, 1)
        print(f"âœ“ Step 1 å®Œæˆï¼šå¤„ç†äº† {len(results)} ä¸ªæ–‡ä»¶")

        # Step 2: æ•°æ®åˆå¹¶
        print("Step 2/5: æ•°æ®åˆå¹¶...")
        step2_dir = get_step_directory(params, 'step2_data_merging')
        control_path = os.path.join(step1_dir, 'RQA_1D2D_summary_control.csv')
        mci_path = os.path.join(step1_dir, 'RQA_1D2D_summary_mci.csv')
        ad_path = os.path.join(step1_dir, 'RQA_1D2D_summary_ad.csv')

        merged_data = merge_group_data(control_path, mci_path, ad_path)
        output_path = os.path.join(step2_dir, 'All_Subjects_RQA_EyeMetrics.csv')
        merged_data.to_csv(output_path, index=False)
        save_param_metadata(params, 2)
        print(f"âœ“ Step 2 å®Œæˆï¼šåˆå¹¶äº† {len(merged_data)} æ¡è®°å½•")

        # Step 3: ç‰¹å¾è¡¥å……
        print("Step 3/5: ç‰¹å¾è¡¥å……...")
        step3_dir = get_step_directory(params, 'step3_feature_enrichment')
        rqa_path = os.path.join(step2_dir, 'All_Subjects_RQA_EyeMetrics.csv')

        df_rqa = pd.read_csv(rqa_path, dtype={"ID": str})
        df_rqa.rename(columns={"ID": "ADQ_ID"}, inplace=True)
        df_rqa["ADQ_ID"] = df_rqa["ADQ_ID"].str.replace(r"\.0$", "", regex=True)

        # æ„å»ºäº‹ä»¶èšåˆ
        events_path = 'data/event_analysis_results/All_Events.csv'
        df_evt_agg = build_event_aggregates(events_path)

        # æ„å»ºROIèšåˆ
        roi_path = 'data/event_analysis_results/All_ROI_Summary.csv'
        df_roi_agg = build_roi_aggregates(roi_path)

        # åˆå¹¶ç‰¹å¾
        if not df_evt_agg.empty and not df_roi_agg.empty:
            df_agg = pd.merge(df_evt_agg, df_roi_agg, on="ADQ_ID", how="outer")
        elif not df_evt_agg.empty:
            df_agg = df_evt_agg
        elif not df_roi_agg.empty:
            df_agg = df_roi_agg
        else:
            df_agg = pd.DataFrame()

        if not df_agg.empty:
            df_final = pd.merge(df_rqa, df_agg, on="ADQ_ID", how="left")
        else:
            df_final = df_rqa

        df_final.rename(columns={"ADQ_ID": "ID"}, inplace=True)
        output_path = os.path.join(step3_dir, 'All_Subjects_RQA_EyeMetrics_Filled.csv')
        df_final.to_csv(output_path, index=False)
        save_param_metadata(params, 3)
        print(f"âœ“ Step 3 å®Œæˆï¼šç”Ÿæˆäº† {len(df_final)} æ¡è®°å½•")

        # Step 4: ç»Ÿè®¡åˆ†æ
        print("Step 4/5: ç»Ÿè®¡åˆ†æ...")
        step4_dir = get_step_directory(params, 'step4_statistical_analysis')
        filled_path = os.path.join(step3_dir, 'All_Subjects_RQA_EyeMetrics_Filled.csv')
        df = pd.read_csv(filled_path)

        rqa_vars = ["RR-2D-xy", "DET-2D-xy", "ENT-2D-xy"]
        group_stats = df.groupby("Group")[rqa_vars].describe()
        group_stats_path = os.path.join(step4_dir, 'group_stats_output.csv')
        group_stats.to_csv(group_stats_path)

        multi_level_stats = df.groupby(["Group", "folder", "q"])[rqa_vars].agg(["mean", "std"])
        multi_level_path = os.path.join(step4_dir, 'multi_level_stats_output.csv')
        multi_level_stats.to_csv(multi_level_path)
        save_param_metadata(params, 4)
        print(f"âœ“ Step 4 å®Œæˆï¼šç”Ÿæˆç»Ÿè®¡åˆ†æ")

        # Step 5: å¯è§†åŒ–
        print("Step 5/5: å¯è§†åŒ–...")
        step5_dir = get_step_directory(params, 'step5_visualization')

        # ç”Ÿæˆæ¡å½¢å›¾
        bar_charts = create_group_bar_charts(df, ["RR-2D-xy", "DET-2D-xy", "ENT-2D-xy"])
        for chart in bar_charts:
            chart_filename = f"bar_chart_{chart['metric'].replace('-', '_')}.png"
            chart_path = os.path.join(step5_dir, chart_filename)
            image_data = base64.b64decode(chart['image'])
            with open(chart_path, 'wb') as f:
                f.write(image_data)

        # ç”ŸæˆæŠ˜çº¿å›¾
        trend_chart = create_task_trend_chart(df, "RR-2D-xy")
        if trend_chart:
            trend_filename = f"trend_chart_{trend_chart['metric'].replace('-', '_')}.png"
            trend_path = os.path.join(step5_dir, trend_filename)
            image_data = base64.b64decode(trend_chart['image'])
            with open(trend_path, 'wb') as f:
                f.write(image_data)

        save_param_metadata(params, 5)
        print(f"âœ“ Step 5 å®Œæˆï¼šç”Ÿæˆäº† {len(bar_charts) + (1 if trend_chart else 0)} ä¸ªå›¾è¡¨")

        print(f"\n{'='*60}")
        print(f"âœ… å‚æ•°ç»„åˆ {param_signature} æ‰§è¡ŒæˆåŠŸï¼")
        print(f"{'='*60}\n")

        return {
            'success': True,
            'param_signature': param_signature,
            'message': 'å®Œæ•´æµç¨‹æ‰§è¡ŒæˆåŠŸ'
        }

    except Exception as e:
        error_msg = f"æ‰§è¡Œå¤±è´¥: {str(e)}"
        print(f"\n{'='*60}")
        print(f"âŒ å‚æ•°ç»„åˆ {param_signature} æ‰§è¡Œå¤±è´¥")
        print(f"é”™è¯¯ä¿¡æ¯: {error_msg}")
        print(f"{'='*60}\n")
        traceback.print_exc()

        return {
            'success': False,
            'param_signature': param_signature,
            'error': error_msg
        }


@rqa_pipeline_bp.route('/api/rqa-pipeline/batch-execute', methods=['POST'])
def batch_execute():
    """æ‰¹é‡æ‰§è¡ŒRQAæµç¨‹"""
    try:
        data = request.get_json()

        print("\n" + "="*80)
        print("ğŸš€ å¯åŠ¨æ‰¹é‡æ‰§è¡ŒRQAæµç¨‹")
        print("="*80)

        # è§£æå‚æ•°èŒƒå›´
        m_range = data.get('m_range', {'start': 1, 'end': 10, 'step': 1})
        tau_range = data.get('tau_range', {'start': 1, 'end': 10, 'step': 1})
        eps_range = data.get('eps_range', {'start': 0.05, 'end': 0.1, 'step': 0.01})
        lmin_range = data.get('lmin_range', {'start': 2, 'end': 3, 'step': 1})

        print(f"\nğŸ“‹ å‚æ•°èŒƒå›´é…ç½®:")
        print(f"  - åµŒå…¥ç»´åº¦ (m): {m_range['start']} â†’ {m_range['end']}, æ­¥é•¿ {m_range['step']}")
        print(f"  - æ—¶é—´å»¶è¿Ÿ (Ï„): {tau_range['start']} â†’ {tau_range['end']}, æ­¥é•¿ {tau_range['step']}")
        print(f"  - é€’å½’é˜ˆå€¼ (Îµ): {eps_range['start']} â†’ {eps_range['end']}, æ­¥é•¿ {eps_range['step']}")
        print(f"  - æœ€å°çº¿é•¿ (l_min): {lmin_range['start']} â†’ {lmin_range['end']}, æ­¥é•¿ {lmin_range['step']}")

        # ç”Ÿæˆå‚æ•°ç»„åˆ
        param_combinations = generate_param_grid(m_range, tau_range, eps_range, lmin_range)
        total_count = len(param_combinations)

        print(f"\nğŸ“Š ç”Ÿæˆäº† {total_count} ä¸ªå‚æ•°ç»„åˆ")
        print(f"{'='*80}\n")

        # æ‰¹é‡æ‰§è¡Œ
        results = []
        completed_count = 0
        failed_count = 0
        skipped_count = 0

        for i, params in enumerate(param_combinations):
            param_sig = generate_param_signature(params)
            print(f"\n" + "="*80)
            print(f"[{i+1}/{total_count}] å¤„ç†å‚æ•°ç»„åˆ: {param_sig}")
            print(f"è¿›åº¦: {((i+1)/total_count*100):.1f}%")
            print("="*80)

            result = execute_full_pipeline_internal(params)
            results.append(result)

            if result['success']:
                if result.get('skipped', False):
                    skipped_count += 1
                    print(f"â­ï¸  {param_sig} - å·²è·³è¿‡ï¼ˆå·²å­˜åœ¨å®Œæ•´ç»“æœï¼‰")
                else:
                    completed_count += 1
                    print(f"âœ… {param_sig} - æ‰§è¡ŒæˆåŠŸ")
            else:
                failed_count += 1
                print(f"âŒ {param_sig} - æ‰§è¡Œå¤±è´¥: {result.get('error', 'æœªçŸ¥é”™è¯¯')}")

            # æ¯ä¸ªç»„åˆéƒ½è¾“å‡ºè¿›åº¦æ‘˜è¦
            print(f"\nğŸ“Š å½“å‰ç»Ÿè®¡: æˆåŠŸ={completed_count}, è·³è¿‡={skipped_count}, å¤±è´¥={failed_count}, æ€»è®¡={i+1}/{total_count}\n")

        # æœ€ç»ˆæ‘˜è¦
        print(f"\n{'='*80}")
        print(f"ğŸ‰ æ‰¹é‡æ‰§è¡Œå®Œæˆï¼")
        print(f"{'='*80}")
        print(f"ğŸ“Š æ‰§è¡Œæ‘˜è¦:")
        print(f"   æ€»è®¡: {total_count} ä¸ªå‚æ•°ç»„åˆ")
        print(f"   âœ… æˆåŠŸæ‰§è¡Œ: {completed_count}")
        print(f"   â­ï¸  å·²è·³è¿‡: {skipped_count}")
        print(f"   âŒ æ‰§è¡Œå¤±è´¥: {failed_count}")
        print(f"{'='*80}\n")

        return jsonify({
            'status': 'success',
            'message': f'æ‰¹é‡æ‰§è¡Œå®Œæˆ',
            'data': {
                'total': total_count,
                'completed': completed_count,
                'skipped': skipped_count,
                'failed': failed_count,
                'results': results
            }
        })

    except Exception as e:
        print(f"\n{'='*80}")
        print(f"âŒ æ‰¹é‡æ‰§è¡Œé”™è¯¯: {e}")
        print(f"{'='*80}\n")
        traceback.print_exc()
        return jsonify({
            'status': 'error',
            'message': f'æ‰¹é‡æ‰§è¡Œå¤±è´¥: {str(e)}'
        }), 500


# ============================================================================
# GPUåŠ é€Ÿç‰ˆæœ¬çš„Pipelineå‡½æ•°
# ============================================================================

def execute_full_pipeline_internal_gpu(params):
    """
    GPUåŠ é€Ÿçš„å®Œæ•´RQA pipeline

    Args:
        params: {'m': int, 'tau': int, 'eps': float, 'lmin': int}

    Returns:
        {'success': bool, 'param_signature': str, 'skipped': bool, ...}
    """
    from analysis.rqa_analyzer_gpu import compute_rqa_1d_gpu, compute_rqa_2d_gpu
    import pandas as pd

    param_signature = generate_param_signature(params)

    # æ–­ç‚¹ç»­ä¼ æ£€æŸ¥
    param_dir = os.path.join(MODULE10_DATASET_ROOT, param_signature)
    metadata_file = os.path.join(param_dir, 'metadata.json')

    if os.path.exists(metadata_file):
        with open(metadata_file, 'r', encoding='utf-8') as f:
            metadata = json.load(f)
        if all(metadata.get(f'step_{i}_completed', False) for i in range(1, 6)):
            return {
                'success': True,
                'param_signature': param_signature,
                'skipped': True,
                'message': 'Already completed'
            }

    try:
        # åˆ›å»ºè¾“å‡ºç›®å½•
        os.makedirs(param_dir, exist_ok=True)

        # Step 1: RQAè®¡ç®— (GPUæ‰¹å¤„ç†åŠ é€Ÿ) âš¡
        from analysis.rqa_analyzer_gpu import RQAAnalyzerGPU

        analyzer = RQAAnalyzerGPU(gpu_id=0)
        rqa_results = {}

        for group in ['control', 'mci', 'ad']:
            # åŠ è½½ç»„æ•°æ®
            group_data = load_group_data_for_rqa(group)

            # é™åˆ¶æµ‹è¯•æ•°æ®é‡: åªå¤„ç†å‰5ä¸ªsubject (ç”¨äºå¿«é€Ÿæµ‹è¯•)
            # ç”Ÿäº§ç¯å¢ƒåº”ç§»é™¤æ­¤é™åˆ¶
            subject_ids = list(group_data.keys())[:5]
            print(f"  å¤„ç† {group} ç»„: {len(subject_ids)} ä¸ªsubjects (å…±{len(group_data)}ä¸ª)")

            # å‡†å¤‡æ‰¹é‡æ•°æ®
            trajectories_batch = []
            subject_id_list = []

            for subject_id in subject_ids:
                subject_data = group_data[subject_id]
                traj_x = subject_data['x']
                traj_y = subject_data['y']
                trajectories_batch.append((traj_x, traj_y))
                subject_id_list.append(subject_id)

            # GPUæ‰¹å¤„ç†
            try:
                batch_results = analyzer.analyze_batch_gpu(
                    trajectories_batch,
                    params,
                    batch_size=10
                )

                # ç»„è£…ç»“æœ
                group_results = []
                for subject_id, result in zip(subject_id_list, batch_results):
                    combined_result = {
                        'subject_id': subject_id,
                        'group': group,
                        **result
                    }
                    group_results.append(combined_result)

                rqa_results[group] = group_results

            except Exception as e:
                print(f"Warning: {group} group batch processing failed: {e}")
                rqa_results[group] = []
                continue

        # ä¿å­˜Step 1ç»“æœ
        step1_file = os.path.join(param_dir, 'step1_rqa_results.json')
        with open(step1_file, 'w', encoding='utf-8') as f:
            json.dump(rqa_results, f, indent=2, ensure_ascii=False)

        update_metadata(param_dir, 'step_1_completed', True)

        # Step 2: æ•°æ®åˆå¹¶
        merge_rqa_data(rqa_results, param_dir)
        update_metadata(param_dir, 'step_2_completed', True)

        # Step 3: ç‰¹å¾æå–
        enrich_features_from_rqa(param_dir)
        update_metadata(param_dir, 'step_3_completed', True)

        # Step 4: ç»Ÿè®¡åˆ†æ
        run_statistical_analysis(param_dir)
        update_metadata(param_dir, 'step_4_completed', True)

        # Step 5: å¯è§†åŒ–
        generate_rqa_visualizations(param_dir, params)
        update_metadata(param_dir, 'step_5_completed', True)

        return {
            'success': True,
            'param_signature': param_signature,
            'skipped': False
        }

    except Exception as e:
        return {
            'success': False,
            'param_signature': param_signature,
            'error': str(e)
        }


def load_group_data_for_rqa(group: str) -> Dict[str, Dict]:
    """
    åŠ è½½æŒ‡å®šç»„çš„æ‰€æœ‰å—è¯•è€…æ•°æ®

    Args:
        group: 'control', 'mci', or 'ad'

    Returns:
        {subject_id: {'x': [...], 'y': [...], 'q': int}, ...}
    """
    # ä½¿ç”¨æ ¡å‡†åçš„æ•°æ®ç›®å½•
    group_dirs = {
        'control': CONTROL_DATA_DIR,
        'mci': MCI_DATA_DIR,
        'ad': AD_DATA_DIR
    }

    group_data = {}
    data_dir = group_dirs.get(group)

    if not os.path.exists(data_dir):
        return group_data

    for filename in os.listdir(data_dir):
        if filename.endswith('_preprocessed_calibrated.csv'):
            filepath = os.path.join(data_dir, filename)

            try:
                df = pd.read_csv(filepath)

                # æå–subject_idå’Œé—®é¢˜ç¼–å·
                subject_id = filename.replace('_preprocessed_calibrated.csv', '')

                group_data[subject_id] = {
                    'x': df['GazePointX_normalized'].values,
                    'y': df['GazePointY_normalized'].values,
                    'q': extract_question_number(subject_id)
                }

            except Exception as e:
                print(f"Warning: Failed to load {filename}: {e}")
                continue

    return group_data


def extract_question_number(subject_id: str) -> int:
    """ä»subject_idä¸­æå–é—®é¢˜ç¼–å· (å¦‚ 'n1q3' -> 3)"""
    import re
    match = re.search(r'q(\d+)', subject_id)
    return int(match.group(1)) if match else 0


def merge_rqa_data(rqa_results: Dict, output_dir: str):
    """åˆå¹¶RQAç»“æœåˆ°CSV"""
    all_data = []

    for group in ['control', 'mci', 'ad']:
        for result in rqa_results.get(group, []):
            all_data.append(result)

    df = pd.DataFrame(all_data)
    output_file = os.path.join(output_dir, 'merged_rqa_data.csv')
    df.to_csv(output_file, index=False, encoding='utf-8')


def enrich_features_from_rqa(output_dir: str):
    """ç‰¹å¾æå– (å ä½ç¬¦ï¼Œå¯æ‰©å±•)"""
    pass


def run_statistical_analysis(output_dir: str):
    """ç»Ÿè®¡åˆ†æ (å ä½ç¬¦ï¼Œå¯æ‰©å±•)"""
    pass


def generate_rqa_visualizations(output_dir: str, params: Dict):
    """ç”Ÿæˆå¯è§†åŒ– (å ä½ç¬¦ï¼Œå¯æ‰©å±•)"""
    pass


def update_metadata(param_dir: str, key: str, value):
    """
    æ›´æ–°å‚æ•°ç›®å½•çš„metadata.jsonæ–‡ä»¶

    Args:
        param_dir: å‚æ•°ç›®å½•è·¯å¾„
        key: è¦æ›´æ–°çš„é”®
        value: è¦è®¾ç½®çš„å€¼
    """
    metadata_file = os.path.join(param_dir, 'metadata.json')

    # è¯»å–ç°æœ‰metadataæˆ–åˆ›å»ºæ–°çš„
    if os.path.exists(metadata_file):
        with open(metadata_file, 'r', encoding='utf-8') as f:
            metadata = json.load(f)
    else:
        metadata = {}

    # æ›´æ–°é”®å€¼
    metadata[key] = value
    metadata['updated_at'] = datetime.now().isoformat()

    # ä¿å­˜
    with open(metadata_file, 'w', encoding='utf-8') as f:
        json.dump(metadata, f, indent=2, ensure_ascii=False)


# GPUå¹¶è¡Œæ‰¹å¤„ç†API
@rqa_pipeline_bp.route('/api/rqa-pipeline/batch-execute-gpu', methods=['POST'])
def batch_execute_gpu():
    """GPUå¹¶è¡Œæ‰¹å¤„ç†API"""
    from visualization.parallel_executor import GPUParallelExecutor, calculate_optimal_workers

    data = request.json
    batch_config = data.get('batch_config', {})
    n_workers = data.get('n_workers', calculate_optimal_workers())

    # ç”Ÿæˆå‚æ•°ç»„åˆ
    param_combinations = generate_param_grid(
        batch_config['m_range'],
        batch_config['tau_range'],
        batch_config['eps_range'],
        batch_config['lmin_range']
    )

    total_count = len(param_combinations)

    print(f"\n{'='*80}")
    print(f"GPU Batch Execution Started (Serial Mode)")
    print(f"Total combinations: {total_count}")
    print(f"Note: Using serial execution in Flask (multiprocessing disabled)")
    print(f"{'='*80}\n")

    # ä¸²è¡Œæ‰§è¡Œ (Flaskç¯å¢ƒä¸­å¤šè¿›ç¨‹ä¸å¯ç”¨)
    start_time = time.time()
    results = []

    for idx, params in enumerate(param_combinations):
        print(f"Processing {idx+1}/{total_count}: {params}")
        result = execute_full_pipeline_internal_gpu(params)
        results.append((idx, params, result))

        # æ‰“å°è¿›åº¦
        if (idx + 1) % 10 == 0 or (idx + 1) == total_count:
            elapsed = time.time() - start_time
            progress = (idx + 1) / total_count * 100
            print(f"Progress: {idx+1}/{total_count} ({progress:.1f}%) - Elapsed: {elapsed:.1f}s")

    elapsed_time = time.time() - start_time

    # ç»Ÿè®¡ç»“æœ
    success_count = sum(1 for _, _, r in results if r.get('success') and not r.get('skipped'))
    skipped_count = sum(1 for _, _, r in results if r.get('skipped'))
    failed_count = sum(1 for _, _, r in results if not r.get('success'))

    stats = {
        'total': total_count,
        'success': success_count,
        'skipped': skipped_count,
        'failed': failed_count,
        'elapsed_time': elapsed_time,
        'avg_time_per_task': elapsed_time / total_count if total_count > 0 else 0
    }

    print(f"\n{'='*80}")
    print(f"GPU Parallel Batch Execution Completed")
    print(f"Time: {elapsed_time:.1f}s ({elapsed_time/3600:.2f}h)")
    print(f"Success: {success_count}, Skipped: {skipped_count}, Failed: {failed_count}")
    print(f"{'='*80}\n")

    return jsonify({
        'success': True,
        'stats': stats,
        'results': [{'index': i, 'params': p, 'result': r} for i, p, r in results[:100]]  # è¿”å›å‰100ä¸ª
    })